cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=400, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Epoch: 1 [0/5999 (0%)]	Loss: 54.673477
Epoch: 1 [2560/5999 (43%)]	Loss: 7.480249
saving model at:1,0.05378471928834915
Epoch: 1 [5120/5999 (85%)]	Loss: 5.308372
saving model at:1,0.039239246428012846
====> Epoch: 1 Average loss: 0.074697 
Epoch: 2 [0/5999 (0%)]	Loss: 4.808210
Epoch: 2 [2560/5999 (43%)]	Loss: 5.036375
saving model at:2,0.035748441338539126
Epoch: 2 [5120/5999 (85%)]	Loss: 4.883142
saving model at:2,0.033077047884464264
====> Epoch: 2 Average loss: 0.036869 
Epoch: 3 [0/5999 (0%)]	Loss: 4.019580
Epoch: 3 [2560/5999 (43%)]	Loss: 3.040941
saving model at:3,0.021962169110774993
Epoch: 3 [5120/5999 (85%)]	Loss: 1.708292
saving model at:3,0.014321900263428689
====> Epoch: 3 Average loss: 0.022518 
Epoch: 4 [0/5999 (0%)]	Loss: 1.837922
Epoch: 4 [2560/5999 (43%)]	Loss: 1.349042
saving model at:4,0.009824455916881561
Epoch: 4 [5120/5999 (85%)]	Loss: 1.022897
saving model at:4,0.007987676948308944
====> Epoch: 4 Average loss: 0.010297 
Epoch: 5 [0/5999 (0%)]	Loss: 1.053062
Epoch: 5 [2560/5999 (43%)]	Loss: 0.810166
saving model at:5,0.006182595234364271
Epoch: 5 [5120/5999 (85%)]	Loss: 0.693753
saving model at:5,0.005166645348072052
====> Epoch: 5 Average loss: 0.006420 
Epoch: 6 [0/5999 (0%)]	Loss: 0.629444
Epoch: 6 [2560/5999 (43%)]	Loss: 0.599002
saving model at:6,0.004243533555418253
Epoch: 6 [5120/5999 (85%)]	Loss: 0.600411
saving model at:6,0.004134166456758976
====> Epoch: 6 Average loss: 0.004917 
Epoch: 7 [0/5999 (0%)]	Loss: 0.490547
Epoch: 7 [2560/5999 (43%)]	Loss: 0.438101
saving model at:7,0.00320705534145236
Epoch: 7 [5120/5999 (85%)]	Loss: 0.435147
====> Epoch: 7 Average loss: 0.003583 
Epoch: 8 [0/5999 (0%)]	Loss: 0.419739
Epoch: 8 [2560/5999 (43%)]	Loss: 0.321509
saving model at:8,0.0025449308007955552
Epoch: 8 [5120/5999 (85%)]	Loss: 0.333150
saving model at:8,0.0024642289374023677
====> Epoch: 8 Average loss: 0.002779 
Epoch: 9 [0/5999 (0%)]	Loss: 0.346065
Epoch: 9 [2560/5999 (43%)]	Loss: 0.321022
saving model at:9,0.0020887583494186403
Epoch: 9 [5120/5999 (85%)]	Loss: 0.280940
saving model at:9,0.0019424378536641597
====> Epoch: 9 Average loss: 0.002406 
Epoch: 10 [0/5999 (0%)]	Loss: 0.273877
Epoch: 10 [2560/5999 (43%)]	Loss: 0.260844
saving model at:10,0.0018116248762235045
Epoch: 10 [5120/5999 (85%)]	Loss: 0.229553
====> Epoch: 10 Average loss: 0.002105 
Epoch: 11 [0/5999 (0%)]	Loss: 0.364862
Epoch: 11 [2560/5999 (43%)]	Loss: 0.280059
saving model at:11,0.001655367251485586
Epoch: 11 [5120/5999 (85%)]	Loss: 0.295899
====> Epoch: 11 Average loss: 0.002069 
Epoch: 12 [0/5999 (0%)]	Loss: 0.186150
Epoch: 12 [2560/5999 (43%)]	Loss: 0.294407
saving model at:12,0.0016038208194077016
Epoch: 12 [5120/5999 (85%)]	Loss: 0.249413
====> Epoch: 12 Average loss: 0.001769 
Epoch: 13 [0/5999 (0%)]	Loss: 0.251518
Epoch: 13 [2560/5999 (43%)]	Loss: 0.176390
saving model at:13,0.001420049519278109
Epoch: 13 [5120/5999 (85%)]	Loss: 0.176412
saving model at:13,0.0012202844200655818
====> Epoch: 13 Average loss: 0.001565 
Epoch: 14 [0/5999 (0%)]	Loss: 0.141747
Epoch: 14 [2560/5999 (43%)]	Loss: 0.183295
saving model at:14,0.0011393066924065352
Epoch: 14 [5120/5999 (85%)]	Loss: 0.151550
saving model at:14,0.0011380621269345284
====> Epoch: 14 Average loss: 0.001487 
Epoch: 15 [0/5999 (0%)]	Loss: 0.300656
Epoch: 15 [2560/5999 (43%)]	Loss: 0.219963
Epoch: 15 [5120/5999 (85%)]	Loss: 0.193071
====> Epoch: 15 Average loss: 0.001595 
Epoch: 16 [0/5999 (0%)]	Loss: 0.149136
Epoch: 16 [2560/5999 (43%)]	Loss: 0.168167
Epoch: 16 [5120/5999 (85%)]	Loss: 0.142569
saving model at:16,0.0009449917771853506
====> Epoch: 16 Average loss: 0.001510 
Epoch: 17 [0/5999 (0%)]	Loss: 0.183664
Epoch: 17 [2560/5999 (43%)]	Loss: 0.126278
saving model at:17,0.0009014307856559753
Epoch: 17 [5120/5999 (85%)]	Loss: 0.119269
====> Epoch: 17 Average loss: 0.001122 
Epoch: 18 [0/5999 (0%)]	Loss: 0.152053
Epoch: 18 [2560/5999 (43%)]	Loss: 0.132444
Epoch: 18 [5120/5999 (85%)]	Loss: 0.155809
====> Epoch: 18 Average loss: 0.001132 
Epoch: 19 [0/5999 (0%)]	Loss: 0.121371
Epoch: 19 [2560/5999 (43%)]	Loss: 0.268490
Epoch: 19 [5120/5999 (85%)]	Loss: 0.152340
====> Epoch: 19 Average loss: 0.001205 
Epoch: 20 [0/5999 (0%)]	Loss: 0.116778
Epoch: 20 [2560/5999 (43%)]	Loss: 0.099763
Epoch: 20 [5120/5999 (85%)]	Loss: 0.140997
====> Epoch: 20 Average loss: 0.001070 
Epoch: 21 [0/5999 (0%)]	Loss: 0.155874
Epoch: 21 [2560/5999 (43%)]	Loss: 0.190868
Epoch: 21 [5120/5999 (85%)]	Loss: 0.132655
====> Epoch: 21 Average loss: 0.001266 
Epoch: 22 [0/5999 (0%)]	Loss: 0.159145
Epoch: 22 [2560/5999 (43%)]	Loss: 0.168389
saving model at:22,0.0007020326745696366
Epoch: 22 [5120/5999 (85%)]	Loss: 0.309753
====> Epoch: 22 Average loss: 0.001045 
Epoch: 23 [0/5999 (0%)]	Loss: 0.092451
Epoch: 23 [2560/5999 (43%)]	Loss: 0.089293
saving model at:23,0.0006918276958167553
Epoch: 23 [5120/5999 (85%)]	Loss: 0.098420
saving model at:23,0.0005741010513156653
====> Epoch: 23 Average loss: 0.000815 
Epoch: 24 [0/5999 (0%)]	Loss: 0.123261
Epoch: 24 [2560/5999 (43%)]	Loss: 0.083354
Epoch: 24 [5120/5999 (85%)]	Loss: 0.094960
====> Epoch: 24 Average loss: 0.000803 
Epoch: 25 [0/5999 (0%)]	Loss: 0.139161
Epoch: 25 [2560/5999 (43%)]	Loss: 0.110751
Epoch: 25 [5120/5999 (85%)]	Loss: 0.089123
====> Epoch: 25 Average loss: 0.000892 
Epoch: 26 [0/5999 (0%)]	Loss: 0.109784
Epoch: 26 [2560/5999 (43%)]	Loss: 0.162771
Epoch: 26 [5120/5999 (85%)]	Loss: 0.102375
====> Epoch: 26 Average loss: 0.000960 
Epoch: 27 [0/5999 (0%)]	Loss: 0.089072
Epoch: 27 [2560/5999 (43%)]	Loss: 0.081838
Epoch: 27 [5120/5999 (85%)]	Loss: 0.386300
====> Epoch: 27 Average loss: 0.000942 
Epoch: 28 [0/5999 (0%)]	Loss: 0.110320
Epoch: 28 [2560/5999 (43%)]	Loss: 0.165192
Epoch: 28 [5120/5999 (85%)]	Loss: 0.198441
saving model at:28,0.0005723943887278438
====> Epoch: 28 Average loss: 0.001013 
Epoch: 29 [0/5999 (0%)]	Loss: 0.171923
Epoch: 29 [2560/5999 (43%)]	Loss: 0.095082
Epoch: 29 [5120/5999 (85%)]	Loss: 0.189396
====> Epoch: 29 Average loss: 0.000784 
Epoch: 30 [0/5999 (0%)]	Loss: 0.087047
Epoch: 30 [2560/5999 (43%)]	Loss: 0.077448
Epoch: 30 [5120/5999 (85%)]	Loss: 0.114286
saving model at:30,0.0005224332790821791
====> Epoch: 30 Average loss: 0.000795 
Epoch: 31 [0/5999 (0%)]	Loss: 0.146940
Epoch: 31 [2560/5999 (43%)]	Loss: 0.101775
Epoch: 31 [5120/5999 (85%)]	Loss: 0.068252
saving model at:31,0.0005193836409598589
====> Epoch: 31 Average loss: 0.000844 
Epoch: 32 [0/5999 (0%)]	Loss: 0.072862
Epoch: 32 [2560/5999 (43%)]	Loss: 0.073837
saving model at:32,0.0004262915556319058
Epoch: 32 [5120/5999 (85%)]	Loss: 0.066525
====> Epoch: 32 Average loss: 0.000679 
Epoch: 33 [0/5999 (0%)]	Loss: 0.055267
Epoch: 33 [2560/5999 (43%)]	Loss: 0.070858
saving model at:33,0.00041984087345190346
Epoch: 33 [5120/5999 (85%)]	Loss: 0.060030
====> Epoch: 33 Average loss: 0.000671 
Epoch: 34 [0/5999 (0%)]	Loss: 0.063931
Epoch: 34 [2560/5999 (43%)]	Loss: 0.174787
Epoch: 34 [5120/5999 (85%)]	Loss: 0.066105
====> Epoch: 34 Average loss: 0.000600 
Epoch: 35 [0/5999 (0%)]	Loss: 0.101016
Epoch: 35 [2560/5999 (43%)]	Loss: 0.057378
Epoch: 35 [5120/5999 (85%)]	Loss: 0.053775
====> Epoch: 35 Average loss: 0.000811 
Epoch: 36 [0/5999 (0%)]	Loss: 0.131243
Epoch: 36 [2560/5999 (43%)]	Loss: 0.065708
Epoch: 36 [5120/5999 (85%)]	Loss: 0.089958
====> Epoch: 36 Average loss: 0.000733 
Epoch: 37 [0/5999 (0%)]	Loss: 0.061399
Epoch: 37 [2560/5999 (43%)]	Loss: 0.053165
saving model at:37,0.00033191294828429816
Epoch: 37 [5120/5999 (85%)]	Loss: 0.145823
====> Epoch: 37 Average loss: 0.000672 
Epoch: 38 [0/5999 (0%)]	Loss: 0.107971
Epoch: 38 [2560/5999 (43%)]	Loss: 0.084183
Epoch: 38 [5120/5999 (85%)]	Loss: 0.112385
====> Epoch: 38 Average loss: 0.000729 
Epoch: 39 [0/5999 (0%)]	Loss: 0.071025
Epoch: 39 [2560/5999 (43%)]	Loss: 0.066612
Epoch: 39 [5120/5999 (85%)]	Loss: 0.136367
====> Epoch: 39 Average loss: 0.000549 
Epoch: 40 [0/5999 (0%)]	Loss: 0.168669
Epoch: 40 [2560/5999 (43%)]	Loss: 0.101666
Epoch: 40 [5120/5999 (85%)]	Loss: 0.051486
saving model at:40,0.00032071466138586404
====> Epoch: 40 Average loss: 0.000568 
Epoch: 41 [0/5999 (0%)]	Loss: 0.064236
Epoch: 41 [2560/5999 (43%)]	Loss: 0.044926
Epoch: 41 [5120/5999 (85%)]	Loss: 0.051717
====> Epoch: 41 Average loss: 0.000638 
Epoch: 42 [0/5999 (0%)]	Loss: 0.060797
Epoch: 42 [2560/5999 (43%)]	Loss: 0.050429
saving model at:42,0.0002905554864555597
Epoch: 42 [5120/5999 (85%)]	Loss: 0.036647
====> Epoch: 42 Average loss: 0.000550 
Epoch: 43 [0/5999 (0%)]	Loss: 0.047900
Epoch: 43 [2560/5999 (43%)]	Loss: 0.084020
Epoch: 43 [5120/5999 (85%)]	Loss: 0.083591
====> Epoch: 43 Average loss: 0.000623 
Epoch: 44 [0/5999 (0%)]	Loss: 0.047500
Epoch: 44 [2560/5999 (43%)]	Loss: 0.067897
Epoch: 44 [5120/5999 (85%)]	Loss: 0.088052
====> Epoch: 44 Average loss: 0.000667 
Epoch: 45 [0/5999 (0%)]	Loss: 0.071527
Epoch: 45 [2560/5999 (43%)]	Loss: 0.128952
Epoch: 45 [5120/5999 (85%)]	Loss: 0.061651
====> Epoch: 45 Average loss: 0.000610 
Epoch: 46 [0/5999 (0%)]	Loss: 0.054009
Epoch: 46 [2560/5999 (43%)]	Loss: 0.075844
Epoch: 46 [5120/5999 (85%)]	Loss: 0.072081
====> Epoch: 46 Average loss: 0.000607 
Epoch: 47 [0/5999 (0%)]	Loss: 0.090527
Epoch: 47 [2560/5999 (43%)]	Loss: 0.070982
saving model at:47,0.0002793277781456709
Epoch: 47 [5120/5999 (85%)]	Loss: 0.051136
====> Epoch: 47 Average loss: 0.000592 
Epoch: 48 [0/5999 (0%)]	Loss: 0.042997
Epoch: 48 [2560/5999 (43%)]	Loss: 0.115944
Epoch: 48 [5120/5999 (85%)]	Loss: 0.075775
====> Epoch: 48 Average loss: 0.000745 
Epoch: 49 [0/5999 (0%)]	Loss: 0.081467
Epoch: 49 [2560/5999 (43%)]	Loss: 0.200627
Epoch: 49 [5120/5999 (85%)]	Loss: 0.131150
====> Epoch: 49 Average loss: 0.000610 
Epoch: 50 [0/5999 (0%)]	Loss: 0.211086
Epoch: 50 [2560/5999 (43%)]	Loss: 0.083456
Epoch: 50 [5120/5999 (85%)]	Loss: 0.065734
====> Epoch: 50 Average loss: 0.000817 
Epoch: 51 [0/5999 (0%)]	Loss: 0.037988
Epoch: 51 [2560/5999 (43%)]	Loss: 0.056322
Epoch: 51 [5120/5999 (85%)]	Loss: 0.089929
====> Epoch: 51 Average loss: 0.000541 
Epoch: 52 [0/5999 (0%)]	Loss: 0.067500
Epoch: 52 [2560/5999 (43%)]	Loss: 0.039045
Epoch: 52 [5120/5999 (85%)]	Loss: 0.072950
====> Epoch: 52 Average loss: 0.000530 
Epoch: 53 [0/5999 (0%)]	Loss: 0.047018
Epoch: 53 [2560/5999 (43%)]	Loss: 0.082177
saving model at:53,0.00025627166056074204
Epoch: 53 [5120/5999 (85%)]	Loss: 0.043226
====> Epoch: 53 Average loss: 0.000699 
Epoch: 54 [0/5999 (0%)]	Loss: 0.050132
Epoch: 54 [2560/5999 (43%)]	Loss: 0.045069
Epoch: 54 [5120/5999 (85%)]	Loss: 0.084105
saving model at:54,0.00024514283752068877
====> Epoch: 54 Average loss: 0.000507 
Epoch: 55 [0/5999 (0%)]	Loss: 0.172763
Epoch: 55 [2560/5999 (43%)]	Loss: 0.260457
Epoch: 55 [5120/5999 (85%)]	Loss: 0.183513
====> Epoch: 55 Average loss: 0.000830 
Epoch: 56 [0/5999 (0%)]	Loss: 0.118341
Epoch: 56 [2560/5999 (43%)]	Loss: 0.060051
Epoch: 56 [5120/5999 (85%)]	Loss: 0.075126
====> Epoch: 56 Average loss: 0.000565 
Epoch: 57 [0/5999 (0%)]	Loss: 0.098056
Epoch: 57 [2560/5999 (43%)]	Loss: 0.097938
Epoch: 57 [5120/5999 (85%)]	Loss: 0.157742
====> Epoch: 57 Average loss: 0.000651 
Epoch: 58 [0/5999 (0%)]	Loss: 0.265642
Epoch: 58 [2560/5999 (43%)]	Loss: 0.070307
Epoch: 58 [5120/5999 (85%)]	Loss: 0.045888
====> Epoch: 58 Average loss: 0.000521 
Epoch: 59 [0/5999 (0%)]	Loss: 0.038851
Epoch: 59 [2560/5999 (43%)]	Loss: 0.032647
saving model at:59,0.00023164191283285617
Epoch: 59 [5120/5999 (85%)]	Loss: 0.108273
====> Epoch: 59 Average loss: 0.000535 
Epoch: 60 [0/5999 (0%)]	Loss: 0.047267
Epoch: 60 [2560/5999 (43%)]	Loss: 0.045988
Epoch: 60 [5120/5999 (85%)]	Loss: 0.114716
====> Epoch: 60 Average loss: 0.000532 
Epoch: 61 [0/5999 (0%)]	Loss: 0.101426
Epoch: 61 [2560/5999 (43%)]	Loss: 0.034855
Epoch: 61 [5120/5999 (85%)]	Loss: 0.045692
====> Epoch: 61 Average loss: 0.000497 
Epoch: 62 [0/5999 (0%)]	Loss: 0.054720
Epoch: 62 [2560/5999 (43%)]	Loss: 0.033872
Epoch: 62 [5120/5999 (85%)]	Loss: 0.087149
====> Epoch: 62 Average loss: 0.000457 
Epoch: 63 [0/5999 (0%)]	Loss: 0.054505
Epoch: 63 [2560/5999 (43%)]	Loss: 0.048688
Epoch: 63 [5120/5999 (85%)]	Loss: 0.049971
====> Epoch: 63 Average loss: 0.000499 
Epoch: 64 [0/5999 (0%)]	Loss: 0.037880
Epoch: 64 [2560/5999 (43%)]	Loss: 0.043484
Epoch: 64 [5120/5999 (85%)]	Loss: 0.094599
====> Epoch: 64 Average loss: 0.000441 
Epoch: 65 [0/5999 (0%)]	Loss: 0.215869
Epoch: 65 [2560/5999 (43%)]	Loss: 0.041537
Epoch: 65 [5120/5999 (85%)]	Loss: 0.037018
====> Epoch: 65 Average loss: 0.000600 
Epoch: 66 [0/5999 (0%)]	Loss: 0.044908
Epoch: 66 [2560/5999 (43%)]	Loss: 0.048334
Epoch: 66 [5120/5999 (85%)]	Loss: 0.084525
====> Epoch: 66 Average loss: 0.000599 
Epoch: 67 [0/5999 (0%)]	Loss: 0.070635
Epoch: 67 [2560/5999 (43%)]	Loss: 0.044652
Epoch: 67 [5120/5999 (85%)]	Loss: 0.076563
====> Epoch: 67 Average loss: 0.000516 
Epoch: 68 [0/5999 (0%)]	Loss: 0.036865
Epoch: 68 [2560/5999 (43%)]	Loss: 0.072170
Epoch: 68 [5120/5999 (85%)]	Loss: 0.041740
saving model at:68,0.00020820296602323653
====> Epoch: 68 Average loss: 0.000485 
Epoch: 69 [0/5999 (0%)]	Loss: 0.033742
Epoch: 69 [2560/5999 (43%)]	Loss: 0.057557
Epoch: 69 [5120/5999 (85%)]	Loss: 0.072835
====> Epoch: 69 Average loss: 0.000712 
Epoch: 70 [0/5999 (0%)]	Loss: 0.206449
Epoch: 70 [2560/5999 (43%)]	Loss: 0.038426
Epoch: 70 [5120/5999 (85%)]	Loss: 0.044909
====> Epoch: 70 Average loss: 0.000566 
Epoch: 71 [0/5999 (0%)]	Loss: 0.047233
Epoch: 71 [2560/5999 (43%)]	Loss: 0.047407
Epoch: 71 [5120/5999 (85%)]	Loss: 0.035923
saving model at:71,0.00019878084410447628
====> Epoch: 71 Average loss: 0.000357 
Epoch: 72 [0/5999 (0%)]	Loss: 0.034069
Epoch: 72 [2560/5999 (43%)]	Loss: 0.075811
Epoch: 72 [5120/5999 (85%)]	Loss: 0.069970
====> Epoch: 72 Average loss: 0.000531 
Epoch: 73 [0/5999 (0%)]	Loss: 0.087938
Epoch: 73 [2560/5999 (43%)]	Loss: 0.075918
Epoch: 73 [5120/5999 (85%)]	Loss: 0.036754
====> Epoch: 73 Average loss: 0.000484 
Epoch: 74 [0/5999 (0%)]	Loss: 0.046523
Epoch: 74 [2560/5999 (43%)]	Loss: 0.126679
Epoch: 74 [5120/5999 (85%)]	Loss: 0.054499
====> Epoch: 74 Average loss: 0.000641 
Epoch: 75 [0/5999 (0%)]	Loss: 0.036005
Epoch: 75 [2560/5999 (43%)]	Loss: 0.034122
saving model at:75,0.00019206268165726214
Epoch: 75 [5120/5999 (85%)]	Loss: 0.042271
====> Epoch: 75 Average loss: 0.000457 
Epoch: 76 [0/5999 (0%)]	Loss: 0.035949
Epoch: 76 [2560/5999 (43%)]	Loss: 0.068898
Epoch: 76 [5120/5999 (85%)]	Loss: 0.084692
====> Epoch: 76 Average loss: 0.000575 
Epoch: 77 [0/5999 (0%)]	Loss: 0.105537
Epoch: 77 [2560/5999 (43%)]	Loss: 0.059381
Epoch: 77 [5120/5999 (85%)]	Loss: 0.047149
====> Epoch: 77 Average loss: 0.000436 
Epoch: 78 [0/5999 (0%)]	Loss: 0.051854
Epoch: 78 [2560/5999 (43%)]	Loss: 0.082755
Epoch: 78 [5120/5999 (85%)]	Loss: 0.050494
====> Epoch: 78 Average loss: 0.000585 
Epoch: 79 [0/5999 (0%)]	Loss: 0.033366
Epoch: 79 [2560/5999 (43%)]	Loss: 0.033360
Epoch: 79 [5120/5999 (85%)]	Loss: 0.060730
====> Epoch: 79 Average loss: 0.000419 
Epoch: 80 [0/5999 (0%)]	Loss: 0.028818
Epoch: 80 [2560/5999 (43%)]	Loss: 0.042918
Epoch: 80 [5120/5999 (85%)]	Loss: 0.109710
====> Epoch: 80 Average loss: 0.000486 
Epoch: 81 [0/5999 (0%)]	Loss: 0.060704
Epoch: 81 [2560/5999 (43%)]	Loss: 0.025366
saving model at:81,0.00018917645642068238
Epoch: 81 [5120/5999 (85%)]	Loss: 0.169423
====> Epoch: 81 Average loss: 0.000433 
Epoch: 82 [0/5999 (0%)]	Loss: 0.048957
Epoch: 82 [2560/5999 (43%)]	Loss: 0.049829
Epoch: 82 [5120/5999 (85%)]	Loss: 0.064368
====> Epoch: 82 Average loss: 0.000447 
Epoch: 83 [0/5999 (0%)]	Loss: 0.052674
Epoch: 83 [2560/5999 (43%)]	Loss: 0.040563
saving model at:83,0.0001862140754237771
Epoch: 83 [5120/5999 (85%)]	Loss: 0.036740
====> Epoch: 83 Average loss: 0.000404 
Epoch: 84 [0/5999 (0%)]	Loss: 0.132717
Epoch: 84 [2560/5999 (43%)]	Loss: 0.035923
Epoch: 84 [5120/5999 (85%)]	Loss: 0.039200
====> Epoch: 84 Average loss: 0.000413 
Epoch: 85 [0/5999 (0%)]	Loss: 0.027673
Epoch: 85 [2560/5999 (43%)]	Loss: 0.043812
Epoch: 85 [5120/5999 (85%)]	Loss: 0.066899
saving model at:85,0.0001786299159284681
====> Epoch: 85 Average loss: 0.000377 
Epoch: 86 [0/5999 (0%)]	Loss: 0.033244
Epoch: 86 [2560/5999 (43%)]	Loss: 0.029555
Epoch: 86 [5120/5999 (85%)]	Loss: 0.056563
====> Epoch: 86 Average loss: 0.000305 
Epoch: 87 [0/5999 (0%)]	Loss: 0.089334
Epoch: 87 [2560/5999 (43%)]	Loss: 0.024129
Epoch: 87 [5120/5999 (85%)]	Loss: 0.039390
====> Epoch: 87 Average loss: 0.000497 
Epoch: 88 [0/5999 (0%)]	Loss: 0.214022
Epoch: 88 [2560/5999 (43%)]	Loss: 0.069287
Epoch: 88 [5120/5999 (85%)]	Loss: 0.044192
====> Epoch: 88 Average loss: 0.000612 
Epoch: 89 [0/5999 (0%)]	Loss: 0.042750
Epoch: 89 [2560/5999 (43%)]	Loss: 0.044864
Epoch: 89 [5120/5999 (85%)]	Loss: 0.050678
====> Epoch: 89 Average loss: 0.000378 
Epoch: 90 [0/5999 (0%)]	Loss: 0.079040
Epoch: 90 [2560/5999 (43%)]	Loss: 0.091670
Epoch: 90 [5120/5999 (85%)]	Loss: 0.207908
====> Epoch: 90 Average loss: 0.000487 
Epoch: 91 [0/5999 (0%)]	Loss: 0.082368
Epoch: 91 [2560/5999 (43%)]	Loss: 0.091068
Epoch: 91 [5120/5999 (85%)]	Loss: 0.058072
====> Epoch: 91 Average loss: 0.000405 
Epoch: 92 [0/5999 (0%)]	Loss: 0.146415
Epoch: 92 [2560/5999 (43%)]	Loss: 0.043717
Epoch: 92 [5120/5999 (85%)]	Loss: 0.046646
====> Epoch: 92 Average loss: 0.000338 
Epoch: 93 [0/5999 (0%)]	Loss: 0.046860
Epoch: 93 [2560/5999 (43%)]	Loss: 0.048220
Epoch: 93 [5120/5999 (85%)]	Loss: 0.033695
====> Epoch: 93 Average loss: 0.000413 
Epoch: 94 [0/5999 (0%)]	Loss: 0.075494
Epoch: 94 [2560/5999 (43%)]	Loss: 0.030448
Epoch: 94 [5120/5999 (85%)]	Loss: 0.036363
====> Epoch: 94 Average loss: 0.000511 
Epoch: 95 [0/5999 (0%)]	Loss: 0.023303
Epoch: 95 [2560/5999 (43%)]	Loss: 0.064443
Epoch: 95 [5120/5999 (85%)]	Loss: 0.027109
====> Epoch: 95 Average loss: 0.000380 
Epoch: 96 [0/5999 (0%)]	Loss: 0.046033
Epoch: 96 [2560/5999 (43%)]	Loss: 0.034610
Epoch: 96 [5120/5999 (85%)]	Loss: 0.160899
====> Epoch: 96 Average loss: 0.000567 
Epoch: 97 [0/5999 (0%)]	Loss: 0.081825
Epoch: 97 [2560/5999 (43%)]	Loss: 0.103191
Epoch: 97 [5120/5999 (85%)]	Loss: 0.080196
====> Epoch: 97 Average loss: 0.000630 
Epoch: 98 [0/5999 (0%)]	Loss: 0.053209
Epoch: 98 [2560/5999 (43%)]	Loss: 0.035595
Epoch: 98 [5120/5999 (85%)]	Loss: 0.084497
====> Epoch: 98 Average loss: 0.000409 
Epoch: 99 [0/5999 (0%)]	Loss: 0.101251
Epoch: 99 [2560/5999 (43%)]	Loss: 0.039962
Epoch: 99 [5120/5999 (85%)]	Loss: 0.034146
saving model at:99,0.00016746229550335557
====> Epoch: 99 Average loss: 0.000426 
Epoch: 100 [0/5999 (0%)]	Loss: 0.048716
Epoch: 100 [2560/5999 (43%)]	Loss: 0.039981
Epoch: 100 [5120/5999 (85%)]	Loss: 0.072710
====> Epoch: 100 Average loss: 0.000371 
Epoch: 101 [0/5999 (0%)]	Loss: 0.030812
Epoch: 101 [2560/5999 (43%)]	Loss: 0.029754
saving model at:101,0.00016559600143227726
Epoch: 101 [5120/5999 (85%)]	Loss: 0.169641
====> Epoch: 101 Average loss: 0.000593 
Epoch: 102 [0/5999 (0%)]	Loss: 0.056243
Epoch: 102 [2560/5999 (43%)]	Loss: 0.126272
Epoch: 102 [5120/5999 (85%)]	Loss: 0.061280
====> Epoch: 102 Average loss: 0.000569 
Epoch: 103 [0/5999 (0%)]	Loss: 0.062745
Epoch: 103 [2560/5999 (43%)]	Loss: 0.034556
Epoch: 103 [5120/5999 (85%)]	Loss: 0.074519
====> Epoch: 103 Average loss: 0.000530 
Epoch: 104 [0/5999 (0%)]	Loss: 0.096718
Epoch: 104 [2560/5999 (43%)]	Loss: 0.064235
Epoch: 104 [5120/5999 (85%)]	Loss: 0.220655
====> Epoch: 104 Average loss: 0.000425 
Epoch: 105 [0/5999 (0%)]	Loss: 0.053034
Epoch: 105 [2560/5999 (43%)]	Loss: 0.055408
Epoch: 105 [5120/5999 (85%)]	Loss: 0.047591
====> Epoch: 105 Average loss: 0.000590 
Epoch: 106 [0/5999 (0%)]	Loss: 0.116568
Epoch: 106 [2560/5999 (43%)]	Loss: 0.037430
Epoch: 106 [5120/5999 (85%)]	Loss: 0.060124
====> Epoch: 106 Average loss: 0.000445 
Epoch: 107 [0/5999 (0%)]	Loss: 0.033169
Epoch: 107 [2560/5999 (43%)]	Loss: 0.074201
Epoch: 107 [5120/5999 (85%)]	Loss: 0.044375
====> Epoch: 107 Average loss: 0.000422 
Epoch: 108 [0/5999 (0%)]	Loss: 0.026465
Epoch: 108 [2560/5999 (43%)]	Loss: 0.034879
Epoch: 108 [5120/5999 (85%)]	Loss: 0.023707
====> Epoch: 108 Average loss: 0.000300 
Epoch: 109 [0/5999 (0%)]	Loss: 0.118042
Epoch: 109 [2560/5999 (43%)]	Loss: 0.073675
Epoch: 109 [5120/5999 (85%)]	Loss: 0.046554
====> Epoch: 109 Average loss: 0.000564 
Epoch: 110 [0/5999 (0%)]	Loss: 0.038450
Epoch: 110 [2560/5999 (43%)]	Loss: 0.033019
saving model at:110,0.00016150965564884246
Epoch: 110 [5120/5999 (85%)]	Loss: 0.052434
====> Epoch: 110 Average loss: 0.000409 
Epoch: 111 [0/5999 (0%)]	Loss: 0.057627
Epoch: 111 [2560/5999 (43%)]	Loss: 0.042186
Epoch: 111 [5120/5999 (85%)]	Loss: 0.024246
====> Epoch: 111 Average loss: 0.000368 
Epoch: 112 [0/5999 (0%)]	Loss: 0.024696
Epoch: 112 [2560/5999 (43%)]	Loss: 0.058116
Epoch: 112 [5120/5999 (85%)]	Loss: 0.028993
====> Epoch: 112 Average loss: 0.000398 
Epoch: 113 [0/5999 (0%)]	Loss: 0.031289
Epoch: 113 [2560/5999 (43%)]	Loss: 0.069346
Epoch: 113 [5120/5999 (85%)]	Loss: 0.030143
====> Epoch: 113 Average loss: 0.000361 
Epoch: 114 [0/5999 (0%)]	Loss: 0.020486
Epoch: 114 [2560/5999 (43%)]	Loss: 0.038983
Epoch: 114 [5120/5999 (85%)]	Loss: 0.089585
====> Epoch: 114 Average loss: 0.000431 
Epoch: 115 [0/5999 (0%)]	Loss: 0.066270
Epoch: 115 [2560/5999 (43%)]	Loss: 0.062571
Epoch: 115 [5120/5999 (85%)]	Loss: 0.051905
====> Epoch: 115 Average loss: 0.000456 
Epoch: 116 [0/5999 (0%)]	Loss: 0.092527
Epoch: 116 [2560/5999 (43%)]	Loss: 0.040464
Epoch: 116 [5120/5999 (85%)]	Loss: 0.035362
====> Epoch: 116 Average loss: 0.000347 
Epoch: 117 [0/5999 (0%)]	Loss: 0.021227
Epoch: 117 [2560/5999 (43%)]	Loss: 0.061690
Epoch: 117 [5120/5999 (85%)]	Loss: 0.057673
====> Epoch: 117 Average loss: 0.000383 
Epoch: 118 [0/5999 (0%)]	Loss: 0.050960
Epoch: 118 [2560/5999 (43%)]	Loss: 0.047772
Epoch: 118 [5120/5999 (85%)]	Loss: 0.067415
====> Epoch: 118 Average loss: 0.000344 
Epoch: 119 [0/5999 (0%)]	Loss: 0.036822
Epoch: 119 [2560/5999 (43%)]	Loss: 0.097008
Epoch: 119 [5120/5999 (85%)]	Loss: 0.039605
====> Epoch: 119 Average loss: 0.000363 
Epoch: 120 [0/5999 (0%)]	Loss: 0.107887
Epoch: 120 [2560/5999 (43%)]	Loss: 0.035970
Epoch: 120 [5120/5999 (85%)]	Loss: 0.079642
====> Epoch: 120 Average loss: 0.000443 
Epoch: 121 [0/5999 (0%)]	Loss: 0.038558
Epoch: 121 [2560/5999 (43%)]	Loss: 0.038699
Epoch: 121 [5120/5999 (85%)]	Loss: 0.029857
====> Epoch: 121 Average loss: 0.000421 
Epoch: 122 [0/5999 (0%)]	Loss: 0.039462
Epoch: 122 [2560/5999 (43%)]	Loss: 0.048536
Epoch: 122 [5120/5999 (85%)]	Loss: 0.187926
====> Epoch: 122 Average loss: 0.000593 
Epoch: 123 [0/5999 (0%)]	Loss: 0.042484
Epoch: 123 [2560/5999 (43%)]	Loss: 0.033088
Epoch: 123 [5120/5999 (85%)]	Loss: 0.044335
====> Epoch: 123 Average loss: 0.000447 
Epoch: 124 [0/5999 (0%)]	Loss: 0.092492
Epoch: 124 [2560/5999 (43%)]	Loss: 0.046428
Epoch: 124 [5120/5999 (85%)]	Loss: 0.032547
====> Epoch: 124 Average loss: 0.000676 
Epoch: 125 [0/5999 (0%)]	Loss: 0.040176
Epoch: 125 [2560/5999 (43%)]	Loss: 0.055195
Epoch: 125 [5120/5999 (85%)]	Loss: 0.090002
====> Epoch: 125 Average loss: 0.000370 
Epoch: 126 [0/5999 (0%)]	Loss: 0.183546
Epoch: 126 [2560/5999 (43%)]	Loss: 0.041258
Epoch: 126 [5120/5999 (85%)]	Loss: 0.031392
====> Epoch: 126 Average loss: 0.000533 
Epoch: 127 [0/5999 (0%)]	Loss: 0.078429
Epoch: 127 [2560/5999 (43%)]	Loss: 0.026226
Epoch: 127 [5120/5999 (85%)]	Loss: 0.035210
====> Epoch: 127 Average loss: 0.000370 
Epoch: 128 [0/5999 (0%)]	Loss: 0.028245
Epoch: 128 [2560/5999 (43%)]	Loss: 0.027644
saving model at:128,0.0001531224475475028
Epoch: 128 [5120/5999 (85%)]	Loss: 0.029686
====> Epoch: 128 Average loss: 0.000293 
Epoch: 129 [0/5999 (0%)]	Loss: 0.034519
Epoch: 129 [2560/5999 (43%)]	Loss: 0.025894
Epoch: 129 [5120/5999 (85%)]	Loss: 0.042970
====> Epoch: 129 Average loss: 0.000404 
Epoch: 130 [0/5999 (0%)]	Loss: 0.038259
Epoch: 130 [2560/5999 (43%)]	Loss: 0.072204
Epoch: 130 [5120/5999 (85%)]	Loss: 0.071105
====> Epoch: 130 Average loss: 0.000423 
Epoch: 131 [0/5999 (0%)]	Loss: 0.045757
Epoch: 131 [2560/5999 (43%)]	Loss: 0.029547
Epoch: 131 [5120/5999 (85%)]	Loss: 0.049712
====> Epoch: 131 Average loss: 0.000333 
Epoch: 132 [0/5999 (0%)]	Loss: 0.038878
Epoch: 132 [2560/5999 (43%)]	Loss: 0.024887
Epoch: 132 [5120/5999 (85%)]	Loss: 0.081907
====> Epoch: 132 Average loss: 0.000393 
Epoch: 133 [0/5999 (0%)]	Loss: 0.020191
Epoch: 133 [2560/5999 (43%)]	Loss: 0.053894
Epoch: 133 [5120/5999 (85%)]	Loss: 0.077841
====> Epoch: 133 Average loss: 0.000406 
Epoch: 134 [0/5999 (0%)]	Loss: 0.078600
Epoch: 134 [2560/5999 (43%)]	Loss: 0.067140
Epoch: 134 [5120/5999 (85%)]	Loss: 0.075100
====> Epoch: 134 Average loss: 0.000356 
Epoch: 135 [0/5999 (0%)]	Loss: 0.025694
Epoch: 135 [2560/5999 (43%)]	Loss: 0.018193
Epoch: 135 [5120/5999 (85%)]	Loss: 0.029841
saving model at:135,0.00012000172946136445
====> Epoch: 135 Average loss: 0.000268 
Epoch: 136 [0/5999 (0%)]	Loss: 0.027647
Epoch: 136 [2560/5999 (43%)]	Loss: 0.054500
Epoch: 136 [5120/5999 (85%)]	Loss: 0.029858
====> Epoch: 136 Average loss: 0.000492 
Epoch: 137 [0/5999 (0%)]	Loss: 0.077871
Epoch: 137 [2560/5999 (43%)]	Loss: 0.084811
Epoch: 137 [5120/5999 (85%)]	Loss: 0.122283
====> Epoch: 137 Average loss: 0.000549 
Epoch: 138 [0/5999 (0%)]	Loss: 0.034294
Epoch: 138 [2560/5999 (43%)]	Loss: 0.047831
Epoch: 138 [5120/5999 (85%)]	Loss: 0.043079
====> Epoch: 138 Average loss: 0.000373 
Epoch: 139 [0/5999 (0%)]	Loss: 0.025757
Epoch: 139 [2560/5999 (43%)]	Loss: 0.051827
Epoch: 139 [5120/5999 (85%)]	Loss: 0.024071
====> Epoch: 139 Average loss: 0.000318 
Epoch: 140 [0/5999 (0%)]	Loss: 0.022370
Epoch: 140 [2560/5999 (43%)]	Loss: 0.022888
Epoch: 140 [5120/5999 (85%)]	Loss: 0.042716
====> Epoch: 140 Average loss: 0.000437 
Epoch: 141 [0/5999 (0%)]	Loss: 0.075940
Epoch: 141 [2560/5999 (43%)]	Loss: 0.030850
Epoch: 141 [5120/5999 (85%)]	Loss: 0.029118
====> Epoch: 141 Average loss: 0.000310 
Epoch: 142 [0/5999 (0%)]	Loss: 0.037887
Epoch: 142 [2560/5999 (43%)]	Loss: 0.036448
Epoch: 142 [5120/5999 (85%)]	Loss: 0.037258
====> Epoch: 142 Average loss: 0.000422 
Epoch: 143 [0/5999 (0%)]	Loss: 0.053911
Epoch: 143 [2560/5999 (43%)]	Loss: 0.051737
Epoch: 143 [5120/5999 (85%)]	Loss: 0.039028
====> Epoch: 143 Average loss: 0.000355 
Epoch: 144 [0/5999 (0%)]	Loss: 0.026800
Epoch: 144 [2560/5999 (43%)]	Loss: 0.030341
Epoch: 144 [5120/5999 (85%)]	Loss: 0.047666
====> Epoch: 144 Average loss: 0.000327 
Epoch: 145 [0/5999 (0%)]	Loss: 0.028761
Epoch: 145 [2560/5999 (43%)]	Loss: 0.080407
Epoch: 145 [5120/5999 (85%)]	Loss: 0.019952
saving model at:145,0.00011886387306731194
====> Epoch: 145 Average loss: 0.000277 
Epoch: 146 [0/5999 (0%)]	Loss: 0.023750
Epoch: 146 [2560/5999 (43%)]	Loss: 0.093624
Epoch: 146 [5120/5999 (85%)]	Loss: 0.030622
====> Epoch: 146 Average loss: 0.000601 
Epoch: 147 [0/5999 (0%)]	Loss: 0.044128
Epoch: 147 [2560/5999 (43%)]	Loss: 0.019564
Epoch: 147 [5120/5999 (85%)]	Loss: 0.035414
====> Epoch: 147 Average loss: 0.000249 
Epoch: 148 [0/5999 (0%)]	Loss: 0.024949
Epoch: 148 [2560/5999 (43%)]	Loss: 0.042625
Epoch: 148 [5120/5999 (85%)]	Loss: 0.093716
====> Epoch: 148 Average loss: 0.000564 
Epoch: 149 [0/5999 (0%)]	Loss: 0.046075
Epoch: 149 [2560/5999 (43%)]	Loss: 0.023726
Epoch: 149 [5120/5999 (85%)]	Loss: 0.039501
====> Epoch: 149 Average loss: 0.000448 
Epoch: 150 [0/5999 (0%)]	Loss: 0.055359
Epoch: 150 [2560/5999 (43%)]	Loss: 0.076730
Epoch: 150 [5120/5999 (85%)]	Loss: 0.050576
====> Epoch: 150 Average loss: 0.000357 
Epoch: 151 [0/5999 (0%)]	Loss: 0.024287
Epoch: 151 [2560/5999 (43%)]	Loss: 0.021126
Epoch: 151 [5120/5999 (85%)]	Loss: 0.037110
====> Epoch: 151 Average loss: 0.000346 
Epoch: 152 [0/5999 (0%)]	Loss: 0.070197
Epoch: 152 [2560/5999 (43%)]	Loss: 0.061642
Epoch: 152 [5120/5999 (85%)]	Loss: 0.029176
====> Epoch: 152 Average loss: 0.000291 
Epoch: 153 [0/5999 (0%)]	Loss: 0.028233
Epoch: 153 [2560/5999 (43%)]	Loss: 0.037925
Epoch: 153 [5120/5999 (85%)]	Loss: 0.029726
====> Epoch: 153 Average loss: 0.000321 
Epoch: 154 [0/5999 (0%)]	Loss: 0.033837
Epoch: 154 [2560/5999 (43%)]	Loss: 0.020021
Epoch: 154 [5120/5999 (85%)]	Loss: 0.071481
====> Epoch: 154 Average loss: 0.000411 
Epoch: 155 [0/5999 (0%)]	Loss: 0.091955
Epoch: 155 [2560/5999 (43%)]	Loss: 0.035060
Epoch: 155 [5120/5999 (85%)]	Loss: 0.038982
====> Epoch: 155 Average loss: 0.000368 
Epoch: 156 [0/5999 (0%)]	Loss: 0.019365
Epoch: 156 [2560/5999 (43%)]	Loss: 0.024081
Epoch: 156 [5120/5999 (85%)]	Loss: 0.039336
====> Epoch: 156 Average loss: 0.000404 
Epoch: 157 [0/5999 (0%)]	Loss: 0.054169
Epoch: 157 [2560/5999 (43%)]	Loss: 0.029371
Epoch: 157 [5120/5999 (85%)]	Loss: 0.014487
saving model at:157,0.0001045216703787446
====> Epoch: 157 Average loss: 0.000294 
Epoch: 158 [0/5999 (0%)]	Loss: 0.027313
Epoch: 158 [2560/5999 (43%)]	Loss: 0.020643
Epoch: 158 [5120/5999 (85%)]	Loss: 0.031932
====> Epoch: 158 Average loss: 0.000304 
Epoch: 159 [0/5999 (0%)]	Loss: 0.025597
Epoch: 159 [2560/5999 (43%)]	Loss: 0.028628
Epoch: 159 [5120/5999 (85%)]	Loss: 0.046039
====> Epoch: 159 Average loss: 0.000290 
Epoch: 160 [0/5999 (0%)]	Loss: 0.043177
Epoch: 160 [2560/5999 (43%)]	Loss: 0.029614
Epoch: 160 [5120/5999 (85%)]	Loss: 0.023452
====> Epoch: 160 Average loss: 0.000365 
Epoch: 161 [0/5999 (0%)]	Loss: 0.047766
Epoch: 161 [2560/5999 (43%)]	Loss: 0.020534
Epoch: 161 [5120/5999 (85%)]	Loss: 0.045544
====> Epoch: 161 Average loss: 0.000308 
Epoch: 162 [0/5999 (0%)]	Loss: 0.035684
Epoch: 162 [2560/5999 (43%)]	Loss: 0.044404
Epoch: 162 [5120/5999 (85%)]	Loss: 0.025780
====> Epoch: 162 Average loss: 0.000280 
Epoch: 163 [0/5999 (0%)]	Loss: 0.055588
Epoch: 163 [2560/5999 (43%)]	Loss: 0.058042
Epoch: 163 [5120/5999 (85%)]	Loss: 0.030878
====> Epoch: 163 Average loss: 0.000360 
Epoch: 164 [0/5999 (0%)]	Loss: 0.021778
Epoch: 164 [2560/5999 (43%)]	Loss: 0.048958
Epoch: 164 [5120/5999 (85%)]	Loss: 0.037085
====> Epoch: 164 Average loss: 0.000262 
Epoch: 165 [0/5999 (0%)]	Loss: 0.019389
Epoch: 165 [2560/5999 (43%)]	Loss: 0.020692
saving model at:165,0.00010356115427566693
Epoch: 165 [5120/5999 (85%)]	Loss: 0.025142
====> Epoch: 165 Average loss: 0.000287 
Epoch: 166 [0/5999 (0%)]	Loss: 0.031497
Epoch: 166 [2560/5999 (43%)]	Loss: 0.022858
Epoch: 166 [5120/5999 (85%)]	Loss: 0.032074
====> Epoch: 166 Average loss: 0.000325 
Epoch: 167 [0/5999 (0%)]	Loss: 0.021486
Epoch: 167 [2560/5999 (43%)]	Loss: 0.035086
Epoch: 167 [5120/5999 (85%)]	Loss: 0.026055
====> Epoch: 167 Average loss: 0.000368 
Epoch: 168 [0/5999 (0%)]	Loss: 0.042479
Epoch: 168 [2560/5999 (43%)]	Loss: 0.052083
Epoch: 168 [5120/5999 (85%)]	Loss: 0.055970
====> Epoch: 168 Average loss: 0.000327 
Epoch: 169 [0/5999 (0%)]	Loss: 0.016958
Epoch: 169 [2560/5999 (43%)]	Loss: 0.030424
Epoch: 169 [5120/5999 (85%)]	Loss: 0.016636
saving model at:169,9.51155707007274e-05
====> Epoch: 169 Average loss: 0.000312 
Epoch: 170 [0/5999 (0%)]	Loss: 0.068702
Epoch: 170 [2560/5999 (43%)]	Loss: 0.027907
Epoch: 170 [5120/5999 (85%)]	Loss: 0.019653
====> Epoch: 170 Average loss: 0.000292 
Epoch: 171 [0/5999 (0%)]	Loss: 0.124385
Epoch: 171 [2560/5999 (43%)]	Loss: 0.042327
Epoch: 171 [5120/5999 (85%)]	Loss: 0.046831
====> Epoch: 171 Average loss: 0.000383 
Epoch: 172 [0/5999 (0%)]	Loss: 0.040196
Epoch: 172 [2560/5999 (43%)]	Loss: 0.023125
Epoch: 172 [5120/5999 (85%)]	Loss: 0.034027
====> Epoch: 172 Average loss: 0.000265 
Epoch: 173 [0/5999 (0%)]	Loss: 0.033995
Epoch: 173 [2560/5999 (43%)]	Loss: 0.016724
Epoch: 173 [5120/5999 (85%)]	Loss: 0.042124
====> Epoch: 173 Average loss: 0.000310 
Epoch: 174 [0/5999 (0%)]	Loss: 0.060220
Epoch: 174 [2560/5999 (43%)]	Loss: 0.032321
Epoch: 174 [5120/5999 (85%)]	Loss: 0.132465
====> Epoch: 174 Average loss: 0.000370 
Epoch: 175 [0/5999 (0%)]	Loss: 0.026666
Epoch: 175 [2560/5999 (43%)]	Loss: 0.050842
Epoch: 175 [5120/5999 (85%)]	Loss: 0.042279
====> Epoch: 175 Average loss: 0.000249 
Epoch: 176 [0/5999 (0%)]	Loss: 0.025225
Epoch: 176 [2560/5999 (43%)]	Loss: 0.049782
Epoch: 176 [5120/5999 (85%)]	Loss: 0.086081
====> Epoch: 176 Average loss: 0.000337 
Epoch: 177 [0/5999 (0%)]	Loss: 0.029832
Epoch: 177 [2560/5999 (43%)]	Loss: 0.025746
Epoch: 177 [5120/5999 (85%)]	Loss: 0.020549
====> Epoch: 177 Average loss: 0.000268 
Epoch: 178 [0/5999 (0%)]	Loss: 0.044337
Epoch: 178 [2560/5999 (43%)]	Loss: 0.055169
Epoch: 178 [5120/5999 (85%)]	Loss: 0.023919
====> Epoch: 178 Average loss: 0.000241 
Epoch: 179 [0/5999 (0%)]	Loss: 0.019112
Epoch: 179 [2560/5999 (43%)]	Loss: 0.052476
Epoch: 179 [5120/5999 (85%)]	Loss: 0.123707
====> Epoch: 179 Average loss: 0.000318 
Epoch: 180 [0/5999 (0%)]	Loss: 0.032850
Epoch: 180 [2560/5999 (43%)]	Loss: 0.027687
Epoch: 180 [5120/5999 (85%)]	Loss: 0.035783
====> Epoch: 180 Average loss: 0.000275 
Epoch: 181 [0/5999 (0%)]	Loss: 0.021493
Epoch: 181 [2560/5999 (43%)]	Loss: 0.040584
Epoch: 181 [5120/5999 (85%)]	Loss: 0.021469
====> Epoch: 181 Average loss: 0.000231 
Epoch: 182 [0/5999 (0%)]	Loss: 0.014800
Epoch: 182 [2560/5999 (43%)]	Loss: 0.024795
Epoch: 182 [5120/5999 (85%)]	Loss: 0.130530
====> Epoch: 182 Average loss: 0.000335 
Epoch: 183 [0/5999 (0%)]	Loss: 0.033979
Epoch: 183 [2560/5999 (43%)]	Loss: 0.022529
Epoch: 183 [5120/5999 (85%)]	Loss: 0.020798
====> Epoch: 183 Average loss: 0.000388 
Epoch: 184 [0/5999 (0%)]	Loss: 0.025078
Epoch: 184 [2560/5999 (43%)]	Loss: 0.020456
Epoch: 184 [5120/5999 (85%)]	Loss: 0.053884
====> Epoch: 184 Average loss: 0.000277 
Epoch: 185 [0/5999 (0%)]	Loss: 0.017588
Epoch: 185 [2560/5999 (43%)]	Loss: 0.014313
saving model at:185,8.668768883217127e-05
Epoch: 185 [5120/5999 (85%)]	Loss: 0.102970
====> Epoch: 185 Average loss: 0.000334 
Epoch: 186 [0/5999 (0%)]	Loss: 0.041405
Epoch: 186 [2560/5999 (43%)]	Loss: 0.048668
Epoch: 186 [5120/5999 (85%)]	Loss: 0.023296
====> Epoch: 186 Average loss: 0.000353 
Epoch: 187 [0/5999 (0%)]	Loss: 0.058369
Epoch: 187 [2560/5999 (43%)]	Loss: 0.025477
Epoch: 187 [5120/5999 (85%)]	Loss: 0.027654
====> Epoch: 187 Average loss: 0.000308 
Epoch: 188 [0/5999 (0%)]	Loss: 0.049067
Epoch: 188 [2560/5999 (43%)]	Loss: 0.058996
Epoch: 188 [5120/5999 (85%)]	Loss: 0.079461
====> Epoch: 188 Average loss: 0.000232 
Epoch: 189 [0/5999 (0%)]	Loss: 0.041404
Epoch: 189 [2560/5999 (43%)]	Loss: 0.061479
Epoch: 189 [5120/5999 (85%)]	Loss: 0.043884
====> Epoch: 189 Average loss: 0.000321 
Epoch: 190 [0/5999 (0%)]	Loss: 0.023407
Epoch: 190 [2560/5999 (43%)]	Loss: 0.032888
Epoch: 190 [5120/5999 (85%)]	Loss: 0.017331
====> Epoch: 190 Average loss: 0.000272 
Epoch: 191 [0/5999 (0%)]	Loss: 0.046433
Epoch: 191 [2560/5999 (43%)]	Loss: 0.038808
Epoch: 191 [5120/5999 (85%)]	Loss: 0.032437
====> Epoch: 191 Average loss: 0.000272 
Epoch: 192 [0/5999 (0%)]	Loss: 0.037365
Epoch: 192 [2560/5999 (43%)]	Loss: 0.019762
Epoch: 192 [5120/5999 (85%)]	Loss: 0.014273
====> Epoch: 192 Average loss: 0.000246 
Epoch: 193 [0/5999 (0%)]	Loss: 0.098070
Epoch: 193 [2560/5999 (43%)]	Loss: 0.015883
Epoch: 193 [5120/5999 (85%)]	Loss: 0.045854
====> Epoch: 193 Average loss: 0.000295 
Epoch: 194 [0/5999 (0%)]	Loss: 0.027446
Epoch: 194 [2560/5999 (43%)]	Loss: 0.063782
Epoch: 194 [5120/5999 (85%)]	Loss: 0.044617
====> Epoch: 194 Average loss: 0.000345 
Epoch: 195 [0/5999 (0%)]	Loss: 0.051479
Epoch: 195 [2560/5999 (43%)]	Loss: 0.025073
Epoch: 195 [5120/5999 (85%)]	Loss: 0.014926
====> Epoch: 195 Average loss: 0.000294 
Epoch: 196 [0/5999 (0%)]	Loss: 0.055168
Epoch: 196 [2560/5999 (43%)]	Loss: 0.022537
saving model at:196,7.814796495949849e-05
Epoch: 196 [5120/5999 (85%)]	Loss: 0.017303
====> Epoch: 196 Average loss: 0.000272 
Epoch: 197 [0/5999 (0%)]	Loss: 0.062631
Epoch: 197 [2560/5999 (43%)]	Loss: 0.063667
Epoch: 197 [5120/5999 (85%)]	Loss: 0.056923
====> Epoch: 197 Average loss: 0.000313 
Epoch: 198 [0/5999 (0%)]	Loss: 0.062901
Epoch: 198 [2560/5999 (43%)]	Loss: 0.019568
Epoch: 198 [5120/5999 (85%)]	Loss: 0.063599
====> Epoch: 198 Average loss: 0.000234 
Epoch: 199 [0/5999 (0%)]	Loss: 0.033742
Epoch: 199 [2560/5999 (43%)]	Loss: 0.065949
Epoch: 199 [5120/5999 (85%)]	Loss: 0.018543
====> Epoch: 199 Average loss: 0.000356 
Epoch: 200 [0/5999 (0%)]	Loss: 0.081837
Epoch: 200 [2560/5999 (43%)]	Loss: 0.058532
Epoch: 200 [5120/5999 (85%)]	Loss: 0.013186
====> Epoch: 200 Average loss: 0.000300 
Epoch: 201 [0/5999 (0%)]	Loss: 0.037703
Epoch: 201 [2560/5999 (43%)]	Loss: 0.011495
saving model at:201,7.335823064204306e-05
Epoch: 201 [5120/5999 (85%)]	Loss: 0.026110
====> Epoch: 201 Average loss: 0.000256 
Epoch: 202 [0/5999 (0%)]	Loss: 0.027576
Epoch: 202 [2560/5999 (43%)]	Loss: 0.029449
Epoch: 202 [5120/5999 (85%)]	Loss: 0.019669
====> Epoch: 202 Average loss: 0.000296 
Epoch: 203 [0/5999 (0%)]	Loss: 0.025979
Epoch: 203 [2560/5999 (43%)]	Loss: 0.015672
Epoch: 203 [5120/5999 (85%)]	Loss: 0.032370
====> Epoch: 203 Average loss: 0.000220 
Epoch: 204 [0/5999 (0%)]	Loss: 0.045219
Epoch: 204 [2560/5999 (43%)]	Loss: 0.096776
Epoch: 204 [5120/5999 (85%)]	Loss: 0.018077
====> Epoch: 204 Average loss: 0.000182 
Epoch: 205 [0/5999 (0%)]	Loss: 0.012723
Epoch: 205 [2560/5999 (43%)]	Loss: 0.022797
Epoch: 205 [5120/5999 (85%)]	Loss: 0.018415
====> Epoch: 205 Average loss: 0.000270 
Epoch: 206 [0/5999 (0%)]	Loss: 0.062917
Epoch: 206 [2560/5999 (43%)]	Loss: 0.072370
Epoch: 206 [5120/5999 (85%)]	Loss: 0.051115
====> Epoch: 206 Average loss: 0.000304 
Epoch: 207 [0/5999 (0%)]	Loss: 0.020446
Epoch: 207 [2560/5999 (43%)]	Loss: 0.082394
Epoch: 207 [5120/5999 (85%)]	Loss: 0.019905
====> Epoch: 207 Average loss: 0.000303 
Epoch: 208 [0/5999 (0%)]	Loss: 0.016934
Epoch: 208 [2560/5999 (43%)]	Loss: 0.185274
Epoch: 208 [5120/5999 (85%)]	Loss: 0.017926
====> Epoch: 208 Average loss: 0.000290 
Epoch: 209 [0/5999 (0%)]	Loss: 0.028063
Epoch: 209 [2560/5999 (43%)]	Loss: 0.022745
Epoch: 209 [5120/5999 (85%)]	Loss: 0.045587
====> Epoch: 209 Average loss: 0.000265 
Epoch: 210 [0/5999 (0%)]	Loss: 0.013780
Epoch: 210 [2560/5999 (43%)]	Loss: 0.037666
Epoch: 210 [5120/5999 (85%)]	Loss: 0.024988
====> Epoch: 210 Average loss: 0.000267 
Epoch: 211 [0/5999 (0%)]	Loss: 0.053977
Epoch: 211 [2560/5999 (43%)]	Loss: 0.021770
Epoch: 211 [5120/5999 (85%)]	Loss: 0.044168
====> Epoch: 211 Average loss: 0.000260 
Epoch: 212 [0/5999 (0%)]	Loss: 0.022496
Epoch: 212 [2560/5999 (43%)]	Loss: 0.018711
Epoch: 212 [5120/5999 (85%)]	Loss: 0.034358
====> Epoch: 212 Average loss: 0.000188 
Epoch: 213 [0/5999 (0%)]	Loss: 0.017080
Epoch: 213 [2560/5999 (43%)]	Loss: 0.025422
Epoch: 213 [5120/5999 (85%)]	Loss: 0.050201
====> Epoch: 213 Average loss: 0.000272 
Epoch: 214 [0/5999 (0%)]	Loss: 0.021828
Epoch: 214 [2560/5999 (43%)]	Loss: 0.121809
Epoch: 214 [5120/5999 (85%)]	Loss: 0.047276
====> Epoch: 214 Average loss: 0.000398 
Epoch: 215 [0/5999 (0%)]	Loss: 0.019229
Epoch: 215 [2560/5999 (43%)]	Loss: 0.018361
Epoch: 215 [5120/5999 (85%)]	Loss: 0.027644
====> Epoch: 215 Average loss: 0.000363 
Epoch: 216 [0/5999 (0%)]	Loss: 0.068140
Epoch: 216 [2560/5999 (43%)]	Loss: 0.020429
Epoch: 216 [5120/5999 (85%)]	Loss: 0.066646
====> Epoch: 216 Average loss: 0.000278 
Epoch: 217 [0/5999 (0%)]	Loss: 0.058603
Epoch: 217 [2560/5999 (43%)]	Loss: 0.033907
Epoch: 217 [5120/5999 (85%)]	Loss: 0.015709
====> Epoch: 217 Average loss: 0.000224 
Epoch: 218 [0/5999 (0%)]	Loss: 0.020236
Epoch: 218 [2560/5999 (43%)]	Loss: 0.014378
Epoch: 218 [5120/5999 (85%)]	Loss: 0.058319
====> Epoch: 218 Average loss: 0.000325 
Epoch: 219 [0/5999 (0%)]	Loss: 0.077353
Epoch: 219 [2560/5999 (43%)]	Loss: 0.032946
Epoch: 219 [5120/5999 (85%)]	Loss: 0.023741
====> Epoch: 219 Average loss: 0.000379 
Epoch: 220 [0/5999 (0%)]	Loss: 0.019729
Epoch: 220 [2560/5999 (43%)]	Loss: 0.047922
Epoch: 220 [5120/5999 (85%)]	Loss: 0.030118
====> Epoch: 220 Average loss: 0.000278 
Epoch: 221 [0/5999 (0%)]	Loss: 0.025988
Epoch: 221 [2560/5999 (43%)]	Loss: 0.055344
Epoch: 221 [5120/5999 (85%)]	Loss: 0.067212
====> Epoch: 221 Average loss: 0.000390 
Epoch: 222 [0/5999 (0%)]	Loss: 0.039373
Epoch: 222 [2560/5999 (43%)]	Loss: 0.041628
Epoch: 222 [5120/5999 (85%)]	Loss: 0.028773
====> Epoch: 222 Average loss: 0.000268 
Epoch: 223 [0/5999 (0%)]	Loss: 0.012565
Epoch: 223 [2560/5999 (43%)]	Loss: 0.013919
saving model at:223,6.431879848241806e-05
Epoch: 223 [5120/5999 (85%)]	Loss: 0.020547
====> Epoch: 223 Average loss: 0.000221 
Epoch: 224 [0/5999 (0%)]	Loss: 0.017161
Epoch: 224 [2560/5999 (43%)]	Loss: 0.022724
Epoch: 224 [5120/5999 (85%)]	Loss: 0.015778
====> Epoch: 224 Average loss: 0.000250 
Epoch: 225 [0/5999 (0%)]	Loss: 0.031400
Epoch: 225 [2560/5999 (43%)]	Loss: 0.059489
Epoch: 225 [5120/5999 (85%)]	Loss: 0.020523
====> Epoch: 225 Average loss: 0.000290 
Epoch: 226 [0/5999 (0%)]	Loss: 0.034051
Epoch: 226 [2560/5999 (43%)]	Loss: 0.075847
Epoch: 226 [5120/5999 (85%)]	Loss: 0.020454
====> Epoch: 226 Average loss: 0.000278 
Epoch: 227 [0/5999 (0%)]	Loss: 0.015757
Epoch: 227 [2560/5999 (43%)]	Loss: 0.031227
Epoch: 227 [5120/5999 (85%)]	Loss: 0.044046
====> Epoch: 227 Average loss: 0.000213 
Epoch: 228 [0/5999 (0%)]	Loss: 0.017219
Epoch: 228 [2560/5999 (43%)]	Loss: 0.052979
Epoch: 228 [5120/5999 (85%)]	Loss: 0.045093
====> Epoch: 228 Average loss: 0.000246 
Epoch: 229 [0/5999 (0%)]	Loss: 0.020896
Epoch: 229 [2560/5999 (43%)]	Loss: 0.015235
Epoch: 229 [5120/5999 (85%)]	Loss: 0.014703
====> Epoch: 229 Average loss: 0.000223 
Epoch: 230 [0/5999 (0%)]	Loss: 0.042726
Epoch: 230 [2560/5999 (43%)]	Loss: 0.040447
Epoch: 230 [5120/5999 (85%)]	Loss: 0.059662
====> Epoch: 230 Average loss: 0.000330 
Epoch: 231 [0/5999 (0%)]	Loss: 0.048551
Epoch: 231 [2560/5999 (43%)]	Loss: 0.016250
Epoch: 231 [5120/5999 (85%)]	Loss: 0.048889
====> Epoch: 231 Average loss: 0.000327 
Epoch: 232 [0/5999 (0%)]	Loss: 0.052544
Epoch: 232 [2560/5999 (43%)]	Loss: 0.022945
Epoch: 232 [5120/5999 (85%)]	Loss: 0.129742
====> Epoch: 232 Average loss: 0.000309 
Epoch: 233 [0/5999 (0%)]	Loss: 0.025949
Epoch: 233 [2560/5999 (43%)]	Loss: 0.031021
Epoch: 233 [5120/5999 (85%)]	Loss: 0.032882
====> Epoch: 233 Average loss: 0.000226 
Epoch: 234 [0/5999 (0%)]	Loss: 0.011617
Epoch: 234 [2560/5999 (43%)]	Loss: 0.028386
Epoch: 234 [5120/5999 (85%)]	Loss: 0.047089
====> Epoch: 234 Average loss: 0.000267 
Epoch: 235 [0/5999 (0%)]	Loss: 0.038270
Epoch: 235 [2560/5999 (43%)]	Loss: 0.079685
Epoch: 235 [5120/5999 (85%)]	Loss: 0.032177
====> Epoch: 235 Average loss: 0.000409 
Epoch: 236 [0/5999 (0%)]	Loss: 0.018307
Epoch: 236 [2560/5999 (43%)]	Loss: 0.022130
Epoch: 236 [5120/5999 (85%)]	Loss: 0.029756
====> Epoch: 236 Average loss: 0.000229 
Epoch: 237 [0/5999 (0%)]	Loss: 0.046190
Epoch: 237 [2560/5999 (43%)]	Loss: 0.021886
Epoch: 237 [5120/5999 (85%)]	Loss: 0.029384
====> Epoch: 237 Average loss: 0.000261 
Epoch: 238 [0/5999 (0%)]	Loss: 0.059105
Epoch: 238 [2560/5999 (43%)]	Loss: 0.022992
Epoch: 238 [5120/5999 (85%)]	Loss: 0.168851
====> Epoch: 238 Average loss: 0.000298 
Epoch: 239 [0/5999 (0%)]	Loss: 0.025940
Epoch: 239 [2560/5999 (43%)]	Loss: 0.029128
Epoch: 239 [5120/5999 (85%)]	Loss: 0.020783
====> Epoch: 239 Average loss: 0.000203 
Epoch: 240 [0/5999 (0%)]	Loss: 0.019940
Epoch: 240 [2560/5999 (43%)]	Loss: 0.069178
Epoch: 240 [5120/5999 (85%)]	Loss: 0.025337
====> Epoch: 240 Average loss: 0.000400 
Epoch: 241 [0/5999 (0%)]	Loss: 0.072037
Epoch: 241 [2560/5999 (43%)]	Loss: 0.027799
Epoch: 241 [5120/5999 (85%)]	Loss: 0.034998
====> Epoch: 241 Average loss: 0.000195 
Epoch: 242 [0/5999 (0%)]	Loss: 0.037940
Epoch: 242 [2560/5999 (43%)]	Loss: 0.025380
Epoch: 242 [5120/5999 (85%)]	Loss: 0.019703
====> Epoch: 242 Average loss: 0.000208 
Epoch: 243 [0/5999 (0%)]	Loss: 0.053053
Epoch: 243 [2560/5999 (43%)]	Loss: 0.045021
Epoch: 243 [5120/5999 (85%)]	Loss: 0.074349
====> Epoch: 243 Average loss: 0.000259 
Epoch: 244 [0/5999 (0%)]	Loss: 0.068943
Epoch: 244 [2560/5999 (43%)]	Loss: 0.065730
Epoch: 244 [5120/5999 (85%)]	Loss: 0.118823
====> Epoch: 244 Average loss: 0.000352 
Epoch: 245 [0/5999 (0%)]	Loss: 0.062684
Epoch: 245 [2560/5999 (43%)]	Loss: 0.037385
Epoch: 245 [5120/5999 (85%)]	Loss: 0.018630
====> Epoch: 245 Average loss: 0.000298 
Epoch: 246 [0/5999 (0%)]	Loss: 0.036583
Epoch: 246 [2560/5999 (43%)]	Loss: 0.053501
Epoch: 246 [5120/5999 (85%)]	Loss: 0.016133
====> Epoch: 246 Average loss: 0.000250 
Epoch: 247 [0/5999 (0%)]	Loss: 0.017469
Epoch: 247 [2560/5999 (43%)]	Loss: 0.055367
Epoch: 247 [5120/5999 (85%)]	Loss: 0.013923
====> Epoch: 247 Average loss: 0.000205 
Epoch: 248 [0/5999 (0%)]	Loss: 0.032012
Epoch: 248 [2560/5999 (43%)]	Loss: 0.020671
Epoch: 248 [5120/5999 (85%)]	Loss: 0.040341
====> Epoch: 248 Average loss: 0.000234 
Epoch: 249 [0/5999 (0%)]	Loss: 0.047271
Epoch: 249 [2560/5999 (43%)]	Loss: 0.012582
Epoch: 249 [5120/5999 (85%)]	Loss: 0.011526
====> Epoch: 249 Average loss: 0.000223 
Epoch: 250 [0/5999 (0%)]	Loss: 0.013899
Epoch: 250 [2560/5999 (43%)]	Loss: 0.024354
Epoch: 250 [5120/5999 (85%)]	Loss: 0.014232
====> Epoch: 250 Average loss: 0.000188 
Epoch: 251 [0/5999 (0%)]	Loss: 0.024924
Epoch: 251 [2560/5999 (43%)]	Loss: 0.030052
Epoch: 251 [5120/5999 (85%)]	Loss: 0.011823
saving model at:251,6.366076227277517e-05
====> Epoch: 251 Average loss: 0.000203 
Epoch: 252 [0/5999 (0%)]	Loss: 0.014715
Epoch: 252 [2560/5999 (43%)]	Loss: 0.039697
Epoch: 252 [5120/5999 (85%)]	Loss: 0.015445
====> Epoch: 252 Average loss: 0.000242 
Epoch: 253 [0/5999 (0%)]	Loss: 0.044099
Epoch: 253 [2560/5999 (43%)]	Loss: 0.054165
Epoch: 253 [5120/5999 (85%)]	Loss: 0.025410
====> Epoch: 253 Average loss: 0.000225 
Epoch: 254 [0/5999 (0%)]	Loss: 0.020953
Epoch: 254 [2560/5999 (43%)]	Loss: 0.040924
Epoch: 254 [5120/5999 (85%)]	Loss: 0.016139
====> Epoch: 254 Average loss: 0.000261 
Epoch: 255 [0/5999 (0%)]	Loss: 0.022747
Epoch: 255 [2560/5999 (43%)]	Loss: 0.015819
Epoch: 255 [5120/5999 (85%)]	Loss: 0.036576
====> Epoch: 255 Average loss: 0.000219 
Epoch: 256 [0/5999 (0%)]	Loss: 0.016063
Epoch: 256 [2560/5999 (43%)]	Loss: 0.013150
Epoch: 256 [5120/5999 (85%)]	Loss: 0.042827
====> Epoch: 256 Average loss: 0.000217 
Epoch: 257 [0/5999 (0%)]	Loss: 0.069565
Epoch: 257 [2560/5999 (43%)]	Loss: 0.049719
Epoch: 257 [5120/5999 (85%)]	Loss: 0.042974
====> Epoch: 257 Average loss: 0.000297 
Epoch: 258 [0/5999 (0%)]	Loss: 0.075874
Epoch: 258 [2560/5999 (43%)]	Loss: 0.017455
Epoch: 258 [5120/5999 (85%)]	Loss: 0.038428
====> Epoch: 258 Average loss: 0.000271 
Epoch: 259 [0/5999 (0%)]	Loss: 0.027354
Epoch: 259 [2560/5999 (43%)]	Loss: 0.038277
Epoch: 259 [5120/5999 (85%)]	Loss: 0.029665
====> Epoch: 259 Average loss: 0.000260 
Epoch: 260 [0/5999 (0%)]	Loss: 0.009502
Epoch: 260 [2560/5999 (43%)]	Loss: 0.011089
Epoch: 260 [5120/5999 (85%)]	Loss: 0.019339
====> Epoch: 260 Average loss: 0.000240 
Epoch: 261 [0/5999 (0%)]	Loss: 0.033600
Epoch: 261 [2560/5999 (43%)]	Loss: 0.031952
Epoch: 261 [5120/5999 (85%)]	Loss: 0.027631
====> Epoch: 261 Average loss: 0.000175 
Epoch: 262 [0/5999 (0%)]	Loss: 0.011488
Epoch: 262 [2560/5999 (43%)]	Loss: 0.019043
Epoch: 262 [5120/5999 (85%)]	Loss: 0.020289
====> Epoch: 262 Average loss: 0.000190 
Epoch: 263 [0/5999 (0%)]	Loss: 0.022600
Epoch: 263 [2560/5999 (43%)]	Loss: 0.049665
Epoch: 263 [5120/5999 (85%)]	Loss: 0.029324
====> Epoch: 263 Average loss: 0.000187 
Epoch: 264 [0/5999 (0%)]	Loss: 0.019871
Epoch: 264 [2560/5999 (43%)]	Loss: 0.025706
Epoch: 264 [5120/5999 (85%)]	Loss: 0.014315
====> Epoch: 264 Average loss: 0.000190 
Epoch: 265 [0/5999 (0%)]	Loss: 0.049723
Epoch: 265 [2560/5999 (43%)]	Loss: 0.020029
Epoch: 265 [5120/5999 (85%)]	Loss: 0.039504
====> Epoch: 265 Average loss: 0.000280 
Epoch: 266 [0/5999 (0%)]	Loss: 0.016141
Epoch: 266 [2560/5999 (43%)]	Loss: 0.035686
Epoch: 266 [5120/5999 (85%)]	Loss: 0.019541
====> Epoch: 266 Average loss: 0.000236 
Epoch: 267 [0/5999 (0%)]	Loss: 0.027395
Epoch: 267 [2560/5999 (43%)]	Loss: 0.043054
Epoch: 267 [5120/5999 (85%)]	Loss: 0.010614
====> Epoch: 267 Average loss: 0.000245 
Epoch: 268 [0/5999 (0%)]	Loss: 0.011062
Epoch: 268 [2560/5999 (43%)]	Loss: 0.017787
Epoch: 268 [5120/5999 (85%)]	Loss: 0.057059
====> Epoch: 268 Average loss: 0.000166 
Epoch: 269 [0/5999 (0%)]	Loss: 0.021320
Epoch: 269 [2560/5999 (43%)]	Loss: 0.026377
Epoch: 269 [5120/5999 (85%)]	Loss: 0.036881
====> Epoch: 269 Average loss: 0.000215 
Epoch: 270 [0/5999 (0%)]	Loss: 0.015112
Epoch: 270 [2560/5999 (43%)]	Loss: 0.073018
Epoch: 270 [5120/5999 (85%)]	Loss: 0.044244
====> Epoch: 270 Average loss: 0.000314 
Epoch: 271 [0/5999 (0%)]	Loss: 0.031848
Epoch: 271 [2560/5999 (43%)]	Loss: 0.054723
Epoch: 271 [5120/5999 (85%)]	Loss: 0.025455
====> Epoch: 271 Average loss: 0.000205 
Epoch: 272 [0/5999 (0%)]	Loss: 0.040468
Epoch: 272 [2560/5999 (43%)]	Loss: 0.019220
Epoch: 272 [5120/5999 (85%)]	Loss: 0.022266
====> Epoch: 272 Average loss: 0.000199 
Epoch: 273 [0/5999 (0%)]	Loss: 0.026110
Epoch: 273 [2560/5999 (43%)]	Loss: 0.052098
Epoch: 273 [5120/5999 (85%)]	Loss: 0.010365
====> Epoch: 273 Average loss: 0.000201 
Epoch: 274 [0/5999 (0%)]	Loss: 0.033009
Epoch: 274 [2560/5999 (43%)]	Loss: 0.021048
Epoch: 274 [5120/5999 (85%)]	Loss: 0.016979
====> Epoch: 274 Average loss: 0.000202 
Epoch: 275 [0/5999 (0%)]	Loss: 0.025087
Epoch: 275 [2560/5999 (43%)]	Loss: 0.026689
Epoch: 275 [5120/5999 (85%)]	Loss: 0.031818
====> Epoch: 275 Average loss: 0.000226 
Epoch: 276 [0/5999 (0%)]	Loss: 0.040199
Epoch: 276 [2560/5999 (43%)]	Loss: 0.105964
Epoch: 276 [5120/5999 (85%)]	Loss: 0.039684
====> Epoch: 276 Average loss: 0.000245 
Epoch: 277 [0/5999 (0%)]	Loss: 0.024690
Epoch: 277 [2560/5999 (43%)]	Loss: 0.019704
Epoch: 277 [5120/5999 (85%)]	Loss: 0.014677
====> Epoch: 277 Average loss: 0.000234 
Epoch: 278 [0/5999 (0%)]	Loss: 0.024303
Epoch: 278 [2560/5999 (43%)]	Loss: 0.020780
Epoch: 278 [5120/5999 (85%)]	Loss: 0.069977
====> Epoch: 278 Average loss: 0.000235 
Epoch: 279 [0/5999 (0%)]	Loss: 0.020766
Epoch: 279 [2560/5999 (43%)]	Loss: 0.010836
Epoch: 279 [5120/5999 (85%)]	Loss: 0.012526
====> Epoch: 279 Average loss: 0.000189 
Epoch: 280 [0/5999 (0%)]	Loss: 0.024711
Epoch: 280 [2560/5999 (43%)]	Loss: 0.013882
saving model at:280,5.066492865444161e-05
Epoch: 280 [5120/5999 (85%)]	Loss: 0.049839
====> Epoch: 280 Average loss: 0.000191 
Epoch: 281 [0/5999 (0%)]	Loss: 0.036286
Epoch: 281 [2560/5999 (43%)]	Loss: 0.071570
Epoch: 281 [5120/5999 (85%)]	Loss: 0.050410
====> Epoch: 281 Average loss: 0.000372 
Epoch: 282 [0/5999 (0%)]	Loss: 0.019518
Epoch: 282 [2560/5999 (43%)]	Loss: 0.009966
Epoch: 282 [5120/5999 (85%)]	Loss: 0.095328
====> Epoch: 282 Average loss: 0.000234 
Epoch: 283 [0/5999 (0%)]	Loss: 0.025704
Epoch: 283 [2560/5999 (43%)]	Loss: 0.020981
Epoch: 283 [5120/5999 (85%)]	Loss: 0.036548
====> Epoch: 283 Average loss: 0.000227 
Epoch: 284 [0/5999 (0%)]	Loss: 0.022301
Epoch: 284 [2560/5999 (43%)]	Loss: 0.020453
Epoch: 284 [5120/5999 (85%)]	Loss: 0.037351
====> Epoch: 284 Average loss: 0.000285 
Epoch: 285 [0/5999 (0%)]	Loss: 0.047844
Epoch: 285 [2560/5999 (43%)]	Loss: 0.016988
Epoch: 285 [5120/5999 (85%)]	Loss: 0.048560
====> Epoch: 285 Average loss: 0.000256 
Epoch: 286 [0/5999 (0%)]	Loss: 0.011714
Epoch: 286 [2560/5999 (43%)]	Loss: 0.017979
Epoch: 286 [5120/5999 (85%)]	Loss: 0.019726
====> Epoch: 286 Average loss: 0.000251 
Epoch: 287 [0/5999 (0%)]	Loss: 0.051150
Epoch: 287 [2560/5999 (43%)]	Loss: 0.106553
Epoch: 287 [5120/5999 (85%)]	Loss: 0.027040
====> Epoch: 287 Average loss: 0.000313 
Epoch: 288 [0/5999 (0%)]	Loss: 0.031518
Epoch: 288 [2560/5999 (43%)]	Loss: 0.078765
Epoch: 288 [5120/5999 (85%)]	Loss: 0.014248
====> Epoch: 288 Average loss: 0.000272 
Epoch: 289 [0/5999 (0%)]	Loss: 0.015338
Epoch: 289 [2560/5999 (43%)]	Loss: 0.015860
Epoch: 289 [5120/5999 (85%)]	Loss: 0.066458
====> Epoch: 289 Average loss: 0.000214 
Epoch: 290 [0/5999 (0%)]	Loss: 0.032307
Epoch: 290 [2560/5999 (43%)]	Loss: 0.019892
Epoch: 290 [5120/5999 (85%)]	Loss: 0.031678
====> Epoch: 290 Average loss: 0.000284 
Epoch: 291 [0/5999 (0%)]	Loss: 0.015387
Epoch: 291 [2560/5999 (43%)]	Loss: 0.016784
Epoch: 291 [5120/5999 (85%)]	Loss: 0.029149
====> Epoch: 291 Average loss: 0.000277 
Epoch: 292 [0/5999 (0%)]	Loss: 0.014907
Epoch: 292 [2560/5999 (43%)]	Loss: 0.023823
Epoch: 292 [5120/5999 (85%)]	Loss: 0.046969
====> Epoch: 292 Average loss: 0.000225 
Epoch: 293 [0/5999 (0%)]	Loss: 0.025659
Epoch: 293 [2560/5999 (43%)]	Loss: 0.010628
Epoch: 293 [5120/5999 (85%)]	Loss: 0.050250
====> Epoch: 293 Average loss: 0.000224 
Epoch: 294 [0/5999 (0%)]	Loss: 0.048268
Epoch: 294 [2560/5999 (43%)]	Loss: 0.024512
Epoch: 294 [5120/5999 (85%)]	Loss: 0.035386
====> Epoch: 294 Average loss: 0.000248 
Epoch: 295 [0/5999 (0%)]	Loss: 0.017463
Epoch: 295 [2560/5999 (43%)]	Loss: 0.009989
Epoch: 295 [5120/5999 (85%)]	Loss: 0.013744
====> Epoch: 295 Average loss: 0.000211 
Epoch: 296 [0/5999 (0%)]	Loss: 0.017917
Epoch: 296 [2560/5999 (43%)]	Loss: 0.017059
Epoch: 296 [5120/5999 (85%)]	Loss: 0.020096
====> Epoch: 296 Average loss: 0.000219 
Epoch: 297 [0/5999 (0%)]	Loss: 0.088516
Epoch: 297 [2560/5999 (43%)]	Loss: 0.020845
Epoch: 297 [5120/5999 (85%)]	Loss: 0.011596
====> Epoch: 297 Average loss: 0.000191 
Epoch: 298 [0/5999 (0%)]	Loss: 0.010993
Epoch: 298 [2560/5999 (43%)]	Loss: 0.010577
Epoch: 298 [5120/5999 (85%)]	Loss: 0.016456
====> Epoch: 298 Average loss: 0.000151 
Epoch: 299 [0/5999 (0%)]	Loss: 0.023681
Epoch: 299 [2560/5999 (43%)]	Loss: 0.026874
Epoch: 299 [5120/5999 (85%)]	Loss: 0.014055
====> Epoch: 299 Average loss: 0.000184 
Epoch: 300 [0/5999 (0%)]	Loss: 0.027892
Epoch: 300 [2560/5999 (43%)]	Loss: 0.013720
Epoch: 300 [5120/5999 (85%)]	Loss: 0.030243
====> Epoch: 300 Average loss: 0.000279 
Epoch: 301 [0/5999 (0%)]	Loss: 0.021559
Epoch: 301 [2560/5999 (43%)]	Loss: 0.020856
Epoch: 301 [5120/5999 (85%)]	Loss: 0.022475
====> Epoch: 301 Average loss: 0.000200 
Epoch: 302 [0/5999 (0%)]	Loss: 0.017375
Epoch: 302 [2560/5999 (43%)]	Loss: 0.012320
Epoch: 302 [5120/5999 (85%)]	Loss: 0.015691
====> Epoch: 302 Average loss: 0.000196 
Epoch: 303 [0/5999 (0%)]	Loss: 0.019829
Epoch: 303 [2560/5999 (43%)]	Loss: 0.014692
Epoch: 303 [5120/5999 (85%)]	Loss: 0.014803
====> Epoch: 303 Average loss: 0.000186 
Epoch: 304 [0/5999 (0%)]	Loss: 0.029638
Epoch: 304 [2560/5999 (43%)]	Loss: 0.013284
Epoch: 304 [5120/5999 (85%)]	Loss: 0.024150
====> Epoch: 304 Average loss: 0.000184 
Epoch: 305 [0/5999 (0%)]	Loss: 0.019049
Epoch: 305 [2560/5999 (43%)]	Loss: 0.028805
Epoch: 305 [5120/5999 (85%)]	Loss: 0.014583
saving model at:305,5.038092710310593e-05
====> Epoch: 305 Average loss: 0.000178 
Epoch: 306 [0/5999 (0%)]	Loss: 0.011348
Epoch: 306 [2560/5999 (43%)]	Loss: 0.036062
Epoch: 306 [5120/5999 (85%)]	Loss: 0.012877
====> Epoch: 306 Average loss: 0.000149 
Epoch: 307 [0/5999 (0%)]	Loss: 0.016657
Epoch: 307 [2560/5999 (43%)]	Loss: 0.034059
Epoch: 307 [5120/5999 (85%)]	Loss: 0.018442
====> Epoch: 307 Average loss: 0.000186 
Epoch: 308 [0/5999 (0%)]	Loss: 0.052906
Epoch: 308 [2560/5999 (43%)]	Loss: 0.041668
Epoch: 308 [5120/5999 (85%)]	Loss: 0.017629
====> Epoch: 308 Average loss: 0.000194 
Epoch: 309 [0/5999 (0%)]	Loss: 0.007810
Epoch: 309 [2560/5999 (43%)]	Loss: 0.088587
Epoch: 309 [5120/5999 (85%)]	Loss: 0.041498
====> Epoch: 309 Average loss: 0.000231 
Epoch: 310 [0/5999 (0%)]	Loss: 0.014401
Epoch: 310 [2560/5999 (43%)]	Loss: 0.014284
Epoch: 310 [5120/5999 (85%)]	Loss: 0.032026
====> Epoch: 310 Average loss: 0.000177 
Epoch: 311 [0/5999 (0%)]	Loss: 0.014342
Epoch: 311 [2560/5999 (43%)]	Loss: 0.013011
Epoch: 311 [5120/5999 (85%)]	Loss: 0.016147
====> Epoch: 311 Average loss: 0.000160 
Epoch: 312 [0/5999 (0%)]	Loss: 0.033298
Epoch: 312 [2560/5999 (43%)]	Loss: 0.011287
Epoch: 312 [5120/5999 (85%)]	Loss: 0.018991
====> Epoch: 312 Average loss: 0.000209 
Epoch: 313 [0/5999 (0%)]	Loss: 0.012186
Epoch: 313 [2560/5999 (43%)]	Loss: 0.034248
Epoch: 313 [5120/5999 (85%)]	Loss: 0.016224
====> Epoch: 313 Average loss: 0.000144 
Epoch: 314 [0/5999 (0%)]	Loss: 0.017058
Epoch: 314 [2560/5999 (43%)]	Loss: 0.047160
Epoch: 314 [5120/5999 (85%)]	Loss: 0.018898
====> Epoch: 314 Average loss: 0.000175 
Epoch: 315 [0/5999 (0%)]	Loss: 0.017303
Epoch: 315 [2560/5999 (43%)]	Loss: 0.052443
Epoch: 315 [5120/5999 (85%)]	Loss: 0.019544
====> Epoch: 315 Average loss: 0.000180 
Epoch: 316 [0/5999 (0%)]	Loss: 0.022302
Epoch: 316 [2560/5999 (43%)]	Loss: 0.024416
Epoch: 316 [5120/5999 (85%)]	Loss: 0.022109
====> Epoch: 316 Average loss: 0.000228 
Epoch: 317 [0/5999 (0%)]	Loss: 0.016547
Epoch: 317 [2560/5999 (43%)]	Loss: 0.034737
Epoch: 317 [5120/5999 (85%)]	Loss: 0.026410
====> Epoch: 317 Average loss: 0.000196 
Epoch: 318 [0/5999 (0%)]	Loss: 0.019458
Epoch: 318 [2560/5999 (43%)]	Loss: 0.023173
Epoch: 318 [5120/5999 (85%)]	Loss: 0.014588
====> Epoch: 318 Average loss: 0.000186 
Epoch: 319 [0/5999 (0%)]	Loss: 0.011083
Epoch: 319 [2560/5999 (43%)]	Loss: 0.017233
Epoch: 319 [5120/5999 (85%)]	Loss: 0.017552
====> Epoch: 319 Average loss: 0.000242 
Epoch: 320 [0/5999 (0%)]	Loss: 0.030634
Epoch: 320 [2560/5999 (43%)]	Loss: 0.018255
Epoch: 320 [5120/5999 (85%)]	Loss: 0.012219
====> Epoch: 320 Average loss: 0.000173 
Epoch: 321 [0/5999 (0%)]	Loss: 0.049423
Epoch: 321 [2560/5999 (43%)]	Loss: 0.014010
Epoch: 321 [5120/5999 (85%)]	Loss: 0.024606
====> Epoch: 321 Average loss: 0.000173 
Epoch: 322 [0/5999 (0%)]	Loss: 0.022242
Epoch: 322 [2560/5999 (43%)]	Loss: 0.055885
Epoch: 322 [5120/5999 (85%)]	Loss: 0.018439
====> Epoch: 322 Average loss: 0.000218 
Epoch: 323 [0/5999 (0%)]	Loss: 0.044237
Epoch: 323 [2560/5999 (43%)]	Loss: 0.022586
Epoch: 323 [5120/5999 (85%)]	Loss: 0.017335
====> Epoch: 323 Average loss: 0.000221 
Epoch: 324 [0/5999 (0%)]	Loss: 0.026741
Epoch: 324 [2560/5999 (43%)]	Loss: 0.011818
Epoch: 324 [5120/5999 (85%)]	Loss: 0.030000
====> Epoch: 324 Average loss: 0.000170 
Epoch: 325 [0/5999 (0%)]	Loss: 0.049646
Epoch: 325 [2560/5999 (43%)]	Loss: 0.023598
Epoch: 325 [5120/5999 (85%)]	Loss: 0.041412
====> Epoch: 325 Average loss: 0.000217 
Epoch: 326 [0/5999 (0%)]	Loss: 0.025206
Epoch: 326 [2560/5999 (43%)]	Loss: 0.019134
Epoch: 326 [5120/5999 (85%)]	Loss: 0.016627
====> Epoch: 326 Average loss: 0.000197 
Epoch: 327 [0/5999 (0%)]	Loss: 0.014679
Epoch: 327 [2560/5999 (43%)]	Loss: 0.059861
Epoch: 327 [5120/5999 (85%)]	Loss: 0.039880
====> Epoch: 327 Average loss: 0.000292 
Epoch: 328 [0/5999 (0%)]	Loss: 0.095149
Epoch: 328 [2560/5999 (43%)]	Loss: 0.044548
Epoch: 328 [5120/5999 (85%)]	Loss: 0.045714
====> Epoch: 328 Average loss: 0.000239 
Epoch: 329 [0/5999 (0%)]	Loss: 0.062642
Epoch: 329 [2560/5999 (43%)]	Loss: 0.026211
Epoch: 329 [5120/5999 (85%)]	Loss: 0.068328
====> Epoch: 329 Average loss: 0.000191 
Epoch: 330 [0/5999 (0%)]	Loss: 0.009887
Epoch: 330 [2560/5999 (43%)]	Loss: 0.024826
Epoch: 330 [5120/5999 (85%)]	Loss: 0.033646
====> Epoch: 330 Average loss: 0.000207 
Epoch: 331 [0/5999 (0%)]	Loss: 0.066534
Epoch: 331 [2560/5999 (43%)]	Loss: 0.025957
Epoch: 331 [5120/5999 (85%)]	Loss: 0.027679
====> Epoch: 331 Average loss: 0.000205 
Epoch: 332 [0/5999 (0%)]	Loss: 0.026289
Epoch: 332 [2560/5999 (43%)]	Loss: 0.019482
Epoch: 332 [5120/5999 (85%)]	Loss: 0.021470
====> Epoch: 332 Average loss: 0.000210 
Epoch: 333 [0/5999 (0%)]	Loss: 0.009033
Epoch: 333 [2560/5999 (43%)]	Loss: 0.025795
Epoch: 333 [5120/5999 (85%)]	Loss: 0.013243
====> Epoch: 333 Average loss: 0.000192 
Epoch: 334 [0/5999 (0%)]	Loss: 0.033747
Epoch: 334 [2560/5999 (43%)]	Loss: 0.030403
Epoch: 334 [5120/5999 (85%)]	Loss: 0.019091
====> Epoch: 334 Average loss: 0.000186 
Epoch: 335 [0/5999 (0%)]	Loss: 0.032697
Epoch: 335 [2560/5999 (43%)]	Loss: 0.012790
Epoch: 335 [5120/5999 (85%)]	Loss: 0.035144
====> Epoch: 335 Average loss: 0.000164 
Epoch: 336 [0/5999 (0%)]	Loss: 0.015410
Epoch: 336 [2560/5999 (43%)]	Loss: 0.118372
Epoch: 336 [5120/5999 (85%)]	Loss: 0.080101
====> Epoch: 336 Average loss: 0.000266 
Epoch: 337 [0/5999 (0%)]	Loss: 0.017569
Epoch: 337 [2560/5999 (43%)]	Loss: 0.029685
Epoch: 337 [5120/5999 (85%)]	Loss: 0.021240
====> Epoch: 337 Average loss: 0.000243 
Epoch: 338 [0/5999 (0%)]	Loss: 0.035552
Epoch: 338 [2560/5999 (43%)]	Loss: 0.012941
Epoch: 338 [5120/5999 (85%)]	Loss: 0.036728
====> Epoch: 338 Average loss: 0.000230 
Epoch: 339 [0/5999 (0%)]	Loss: 0.021652
Epoch: 339 [2560/5999 (43%)]	Loss: 0.051765
Epoch: 339 [5120/5999 (85%)]	Loss: 0.006734
====> Epoch: 339 Average loss: 0.000174 
Epoch: 340 [0/5999 (0%)]	Loss: 0.019814
Epoch: 340 [2560/5999 (43%)]	Loss: 0.091750
Epoch: 340 [5120/5999 (85%)]	Loss: 0.023583
====> Epoch: 340 Average loss: 0.000208 
Epoch: 341 [0/5999 (0%)]	Loss: 0.016870
Epoch: 341 [2560/5999 (43%)]	Loss: 0.028493
Epoch: 341 [5120/5999 (85%)]	Loss: 0.013907
saving model at:341,4.752561869099736e-05
====> Epoch: 341 Average loss: 0.000197 
Epoch: 342 [0/5999 (0%)]	Loss: 0.045756
Epoch: 342 [2560/5999 (43%)]	Loss: 0.023724
Epoch: 342 [5120/5999 (85%)]	Loss: 0.008638
====> Epoch: 342 Average loss: 0.000183 
Epoch: 343 [0/5999 (0%)]	Loss: 0.016030
Epoch: 343 [2560/5999 (43%)]	Loss: 0.007240
saving model at:343,4.41471935773734e-05
Epoch: 343 [5120/5999 (85%)]	Loss: 0.068884
====> Epoch: 343 Average loss: 0.000183 
Epoch: 344 [0/5999 (0%)]	Loss: 0.020693
Epoch: 344 [2560/5999 (43%)]	Loss: 0.008149
Epoch: 344 [5120/5999 (85%)]	Loss: 0.014377
====> Epoch: 344 Average loss: 0.000151 
Epoch: 345 [0/5999 (0%)]	Loss: 0.013745
Epoch: 345 [2560/5999 (43%)]	Loss: 0.023112
Epoch: 345 [5120/5999 (85%)]	Loss: 0.017568
====> Epoch: 345 Average loss: 0.000189 
Epoch: 346 [0/5999 (0%)]	Loss: 0.023063
Epoch: 346 [2560/5999 (43%)]	Loss: 0.008235
Epoch: 346 [5120/5999 (85%)]	Loss: 0.023489
====> Epoch: 346 Average loss: 0.000133 
Epoch: 347 [0/5999 (0%)]	Loss: 0.025869
Epoch: 347 [2560/5999 (43%)]	Loss: 0.032276
Epoch: 347 [5120/5999 (85%)]	Loss: 0.007171
====> Epoch: 347 Average loss: 0.000160 
Epoch: 348 [0/5999 (0%)]	Loss: 0.009950
Epoch: 348 [2560/5999 (43%)]	Loss: 0.042372
Epoch: 348 [5120/5999 (85%)]	Loss: 0.013457
====> Epoch: 348 Average loss: 0.000154 
Epoch: 349 [0/5999 (0%)]	Loss: 0.024725
Epoch: 349 [2560/5999 (43%)]	Loss: 0.012152
Epoch: 349 [5120/5999 (85%)]	Loss: 0.022084
saving model at:349,4.128001385834068e-05
====> Epoch: 349 Average loss: 0.000177 
Epoch: 350 [0/5999 (0%)]	Loss: 0.079501
Epoch: 350 [2560/5999 (43%)]	Loss: 0.018239
Epoch: 350 [5120/5999 (85%)]	Loss: 0.045272
====> Epoch: 350 Average loss: 0.000286 
Epoch: 351 [0/5999 (0%)]	Loss: 0.046057
Epoch: 351 [2560/5999 (43%)]	Loss: 0.011410
Epoch: 351 [5120/5999 (85%)]	Loss: 0.009369
====> Epoch: 351 Average loss: 0.000140 
Epoch: 352 [0/5999 (0%)]	Loss: 0.026296
Epoch: 352 [2560/5999 (43%)]	Loss: 0.017307
Epoch: 352 [5120/5999 (85%)]	Loss: 0.021779
====> Epoch: 352 Average loss: 0.000190 
Epoch: 353 [0/5999 (0%)]	Loss: 0.022242
Epoch: 353 [2560/5999 (43%)]	Loss: 0.020939
Epoch: 353 [5120/5999 (85%)]	Loss: 0.013397
====> Epoch: 353 Average loss: 0.000192 
Epoch: 354 [0/5999 (0%)]	Loss: 0.032182
Epoch: 354 [2560/5999 (43%)]	Loss: 0.018155
Epoch: 354 [5120/5999 (85%)]	Loss: 0.016175
====> Epoch: 354 Average loss: 0.000158 
Epoch: 355 [0/5999 (0%)]	Loss: 0.040365
Epoch: 355 [2560/5999 (43%)]	Loss: 0.019490
Epoch: 355 [5120/5999 (85%)]	Loss: 0.016138
====> Epoch: 355 Average loss: 0.000144 
Epoch: 356 [0/5999 (0%)]	Loss: 0.034411
Epoch: 356 [2560/5999 (43%)]	Loss: 0.013310
Epoch: 356 [5120/5999 (85%)]	Loss: 0.016089
====> Epoch: 356 Average loss: 0.000146 
Epoch: 357 [0/5999 (0%)]	Loss: 0.015434
Epoch: 357 [2560/5999 (43%)]	Loss: 0.023871
Epoch: 357 [5120/5999 (85%)]	Loss: 0.017455
====> Epoch: 357 Average loss: 0.000170 
Epoch: 358 [0/5999 (0%)]	Loss: 0.009570
Epoch: 358 [2560/5999 (43%)]	Loss: 0.016592
Epoch: 358 [5120/5999 (85%)]	Loss: 0.046928
====> Epoch: 358 Average loss: 0.000192 
Epoch: 359 [0/5999 (0%)]	Loss: 0.052060
Epoch: 359 [2560/5999 (43%)]	Loss: 0.021516
Epoch: 359 [5120/5999 (85%)]	Loss: 0.014620
====> Epoch: 359 Average loss: 0.000185 
Epoch: 360 [0/5999 (0%)]	Loss: 0.017917
Epoch: 360 [2560/5999 (43%)]	Loss: 0.021208
Epoch: 360 [5120/5999 (85%)]	Loss: 0.037813
====> Epoch: 360 Average loss: 0.000222 
Epoch: 361 [0/5999 (0%)]	Loss: 0.014024
Epoch: 361 [2560/5999 (43%)]	Loss: 0.041514
Epoch: 361 [5120/5999 (85%)]	Loss: 0.027948
====> Epoch: 361 Average loss: 0.000301 
Epoch: 362 [0/5999 (0%)]	Loss: 0.011096
Epoch: 362 [2560/5999 (43%)]	Loss: 0.012369
Epoch: 362 [5120/5999 (85%)]	Loss: 0.054191
====> Epoch: 362 Average loss: 0.000216 
Epoch: 363 [0/5999 (0%)]	Loss: 0.020494
Epoch: 363 [2560/5999 (43%)]	Loss: 0.010455
Epoch: 363 [5120/5999 (85%)]	Loss: 0.024126
====> Epoch: 363 Average loss: 0.000199 
Epoch: 364 [0/5999 (0%)]	Loss: 0.012929
Epoch: 364 [2560/5999 (43%)]	Loss: 0.009426
Epoch: 364 [5120/5999 (85%)]	Loss: 0.022751
====> Epoch: 364 Average loss: 0.000176 
Epoch: 365 [0/5999 (0%)]	Loss: 0.013528
Epoch: 365 [2560/5999 (43%)]	Loss: 0.018312
Epoch: 365 [5120/5999 (85%)]	Loss: 0.021727
====> Epoch: 365 Average loss: 0.000206 
Epoch: 366 [0/5999 (0%)]	Loss: 0.029501
Epoch: 366 [2560/5999 (43%)]	Loss: 0.011558
Epoch: 366 [5120/5999 (85%)]	Loss: 0.012329
====> Epoch: 366 Average loss: 0.000172 
Epoch: 367 [0/5999 (0%)]	Loss: 0.010138
Epoch: 367 [2560/5999 (43%)]	Loss: 0.062353
Epoch: 367 [5120/5999 (85%)]	Loss: 0.025248
====> Epoch: 367 Average loss: 0.000172 
Epoch: 368 [0/5999 (0%)]	Loss: 0.026516
Epoch: 368 [2560/5999 (43%)]	Loss: 0.029303
Epoch: 368 [5120/5999 (85%)]	Loss: 0.025491
====> Epoch: 368 Average loss: 0.000174 
Epoch: 369 [0/5999 (0%)]	Loss: 0.014061
Epoch: 369 [2560/5999 (43%)]	Loss: 0.020194
Epoch: 369 [5120/5999 (85%)]	Loss: 0.024583
====> Epoch: 369 Average loss: 0.000157 
Epoch: 370 [0/5999 (0%)]	Loss: 0.018308
Epoch: 370 [2560/5999 (43%)]	Loss: 0.020562
Epoch: 370 [5120/5999 (85%)]	Loss: 0.011797
====> Epoch: 370 Average loss: 0.000225 
Epoch: 371 [0/5999 (0%)]	Loss: 0.012696
Epoch: 371 [2560/5999 (43%)]	Loss: 0.026396
Epoch: 371 [5120/5999 (85%)]	Loss: 0.016669
====> Epoch: 371 Average loss: 0.000153 
Epoch: 372 [0/5999 (0%)]	Loss: 0.012414
Epoch: 372 [2560/5999 (43%)]	Loss: 0.018327
Epoch: 372 [5120/5999 (85%)]	Loss: 0.010389
====> Epoch: 372 Average loss: 0.000158 
Epoch: 373 [0/5999 (0%)]	Loss: 0.020898
Epoch: 373 [2560/5999 (43%)]	Loss: 0.014374
Epoch: 373 [5120/5999 (85%)]	Loss: 0.023125
====> Epoch: 373 Average loss: 0.000151 
Epoch: 374 [0/5999 (0%)]	Loss: 0.015806
Epoch: 374 [2560/5999 (43%)]	Loss: 0.011899
Epoch: 374 [5120/5999 (85%)]	Loss: 0.008675
====> Epoch: 374 Average loss: 0.000151 
Epoch: 375 [0/5999 (0%)]	Loss: 0.044627
Epoch: 375 [2560/5999 (43%)]	Loss: 0.023814
Epoch: 375 [5120/5999 (85%)]	Loss: 0.037429
====> Epoch: 375 Average loss: 0.000184 
Epoch: 376 [0/5999 (0%)]	Loss: 0.070284
Epoch: 376 [2560/5999 (43%)]	Loss: 0.013565
Epoch: 376 [5120/5999 (85%)]	Loss: 0.011754
====> Epoch: 376 Average loss: 0.000170 
Epoch: 377 [0/5999 (0%)]	Loss: 0.011874
Epoch: 377 [2560/5999 (43%)]	Loss: 0.022945
Epoch: 377 [5120/5999 (85%)]	Loss: 0.011482
====> Epoch: 377 Average loss: 0.000137 
Epoch: 378 [0/5999 (0%)]	Loss: 0.018782
Epoch: 378 [2560/5999 (43%)]	Loss: 0.022352
Epoch: 378 [5120/5999 (85%)]	Loss: 0.038455
====> Epoch: 378 Average loss: 0.000229 
Epoch: 379 [0/5999 (0%)]	Loss: 0.014328
Epoch: 379 [2560/5999 (43%)]	Loss: 0.008705
Epoch: 379 [5120/5999 (85%)]	Loss: 0.019698
====> Epoch: 379 Average loss: 0.000145 
Epoch: 380 [0/5999 (0%)]	Loss: 0.013496
Epoch: 380 [2560/5999 (43%)]	Loss: 0.020162
Epoch: 380 [5120/5999 (85%)]	Loss: 0.012979
====> Epoch: 380 Average loss: 0.000194 
Epoch: 381 [0/5999 (0%)]	Loss: 0.019856
Epoch: 381 [2560/5999 (43%)]	Loss: 0.024354
Epoch: 381 [5120/5999 (85%)]	Loss: 0.019873
====> Epoch: 381 Average loss: 0.000182 
Epoch: 382 [0/5999 (0%)]	Loss: 0.019152
Epoch: 382 [2560/5999 (43%)]	Loss: 0.032860
Epoch: 382 [5120/5999 (85%)]	Loss: 0.026978
====> Epoch: 382 Average loss: 0.000189 
Epoch: 383 [0/5999 (0%)]	Loss: 0.011358
Epoch: 383 [2560/5999 (43%)]	Loss: 0.025384
Epoch: 383 [5120/5999 (85%)]	Loss: 0.017377
====> Epoch: 383 Average loss: 0.000181 
Epoch: 384 [0/5999 (0%)]	Loss: 0.012189
Epoch: 384 [2560/5999 (43%)]	Loss: 0.012784
Epoch: 384 [5120/5999 (85%)]	Loss: 0.011651
====> Epoch: 384 Average loss: 0.000151 
Epoch: 385 [0/5999 (0%)]	Loss: 0.025914
Epoch: 385 [2560/5999 (43%)]	Loss: 0.012136
Epoch: 385 [5120/5999 (85%)]	Loss: 0.024520
====> Epoch: 385 Average loss: 0.000153 
Epoch: 386 [0/5999 (0%)]	Loss: 0.012851
Epoch: 386 [2560/5999 (43%)]	Loss: 0.063202
Epoch: 386 [5120/5999 (85%)]	Loss: 0.022862
====> Epoch: 386 Average loss: 0.000155 
Epoch: 387 [0/5999 (0%)]	Loss: 0.026084
Epoch: 387 [2560/5999 (43%)]	Loss: 0.029757
Epoch: 387 [5120/5999 (85%)]	Loss: 0.014063
====> Epoch: 387 Average loss: 0.000186 
Epoch: 388 [0/5999 (0%)]	Loss: 0.035962
Epoch: 388 [2560/5999 (43%)]	Loss: 0.022904
Epoch: 388 [5120/5999 (85%)]	Loss: 0.026183
====> Epoch: 388 Average loss: 0.000227 
Epoch: 389 [0/5999 (0%)]	Loss: 0.026245
Epoch: 389 [2560/5999 (43%)]	Loss: 0.035477
Epoch: 389 [5120/5999 (85%)]	Loss: 0.029205
====> Epoch: 389 Average loss: 0.000202 
Epoch: 390 [0/5999 (0%)]	Loss: 0.020059
Epoch: 390 [2560/5999 (43%)]	Loss: 0.043643
Epoch: 390 [5120/5999 (85%)]	Loss: 0.017042
====> Epoch: 390 Average loss: 0.000158 
Epoch: 391 [0/5999 (0%)]	Loss: 0.010630
Epoch: 391 [2560/5999 (43%)]	Loss: 0.013451
Epoch: 391 [5120/5999 (85%)]	Loss: 0.015400
====> Epoch: 391 Average loss: 0.000177 
Epoch: 392 [0/5999 (0%)]	Loss: 0.021946
Epoch: 392 [2560/5999 (43%)]	Loss: 0.017078
Epoch: 392 [5120/5999 (85%)]	Loss: 0.048311
====> Epoch: 392 Average loss: 0.000215 
Epoch: 393 [0/5999 (0%)]	Loss: 0.027887
Epoch: 393 [2560/5999 (43%)]	Loss: 0.016899
Epoch: 393 [5120/5999 (85%)]	Loss: 0.014574
====> Epoch: 393 Average loss: 0.000210 
Epoch: 394 [0/5999 (0%)]	Loss: 0.022786
Epoch: 394 [2560/5999 (43%)]	Loss: 0.021867
Epoch: 394 [5120/5999 (85%)]	Loss: 0.009619
saving model at:394,4.0465823520207776e-05
====> Epoch: 394 Average loss: 0.000172 
Epoch: 395 [0/5999 (0%)]	Loss: 0.022936
Epoch: 395 [2560/5999 (43%)]	Loss: 0.034942
Epoch: 395 [5120/5999 (85%)]	Loss: 0.027471
====> Epoch: 395 Average loss: 0.000143 
Epoch: 396 [0/5999 (0%)]	Loss: 0.012583
Epoch: 396 [2560/5999 (43%)]	Loss: 0.072957
Epoch: 396 [5120/5999 (85%)]	Loss: 0.023400
====> Epoch: 396 Average loss: 0.000237 
Epoch: 397 [0/5999 (0%)]	Loss: 0.056556
Epoch: 397 [2560/5999 (43%)]	Loss: 0.023899
Epoch: 397 [5120/5999 (85%)]	Loss: 0.020238
====> Epoch: 397 Average loss: 0.000202 
Epoch: 398 [0/5999 (0%)]	Loss: 0.035667
Epoch: 398 [2560/5999 (43%)]	Loss: 0.011860
Epoch: 398 [5120/5999 (85%)]	Loss: 0.034131
====> Epoch: 398 Average loss: 0.000143 
Epoch: 399 [0/5999 (0%)]	Loss: 0.014530
Epoch: 399 [2560/5999 (43%)]	Loss: 0.011018
Epoch: 399 [5120/5999 (85%)]	Loss: 0.016712
====> Epoch: 399 Average loss: 0.000163 
Epoch: 400 [0/5999 (0%)]	Loss: 0.017820
Epoch: 400 [2560/5999 (43%)]	Loss: 0.038599
Epoch: 400 [5120/5999 (85%)]	Loss: 0.015556
====> Epoch: 400 Average loss: 0.000237 
Reconstruction Loss 4.046582500450313e-05
per_obj_mse: ['5.840469020768069e-05', '2.2526965040015057e-05']
Reconstruction Loss 3.9713952934578866e-05
per_obj_mse: ['5.9223493735771626e-05', '2.020441388594918e-05']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.00012392867734888568
per_obj_mse: ['8.432580943917856e-05', '0.0001635315566090867']
Reconstruction Loss 0.00011757200693169687
per_obj_mse: ['7.879789336584508e-05', '0.00015634614101145416']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.00012392867734888568
per_obj_mse: ['8.432580943917856e-05', '0.0001635315566090867']
Reconstruction Loss 0.00011757200693169687
per_obj_mse: ['7.879789336584508e-05', '0.00015634614101145416']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=20, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7997, 2, 11)
(1997, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0008422251418232918
per_obj_mse: ['0.0010739524150267243', '0.0006104980711825192']
Reconstruction Loss 0.0008632043253795492
per_obj_mse: ['0.0010520858922973275', '0.0006743228295817971']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=60, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7993, 2, 11)
(1993, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0028413358226459496
per_obj_mse: ['0.0038018005434423685', '0.0018808713648468256']
Reconstruction Loss 0.0028890650518466586
per_obj_mse: ['0.003737468970939517', '0.0020406614057719707']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=100, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7989, 2, 11)
(1989, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.004840548242068476
per_obj_mse: ['0.006536349654197693', '0.0031447468791157007']
Reconstruction Loss 0.004914500957093627
per_obj_mse: ['0.006422070320695639', '0.0034069321118295193']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=140, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7985, 2, 11)
(1985, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.006829282045494961
per_obj_mse: ['0.009244433604180813', '0.00441413139924407']
Reconstruction Loss 0.006942654917631252
per_obj_mse: ['0.009108462370932102', '0.0047768475487828255']
cpu
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=False, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/home/hw1415904/data3/codes/2020-10-09//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/home/hw1415904/data3/codes/2020-10-09/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0, inplace=False)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0, inplace=False)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0001239288207143545
per_obj_mse: ['8.432615868514404e-05', '0.00016353148384951055']
Reconstruction Loss 0.00011757215987967831
per_obj_mse: ['7.879826443968341e-05', '0.00015634603914804757']
