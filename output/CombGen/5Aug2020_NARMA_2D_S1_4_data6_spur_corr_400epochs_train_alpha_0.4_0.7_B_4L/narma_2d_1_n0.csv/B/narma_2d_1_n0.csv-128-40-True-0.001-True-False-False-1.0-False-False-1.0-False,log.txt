cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=400, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Epoch: 1 [0/5999 (0%)]	Loss: 80.732559
Epoch: 1 [2560/5999 (43%)]	Loss: 9.592461
saving model at:1,0.06843927299976349
Epoch: 1 [5120/5999 (85%)]	Loss: 6.591810
saving model at:1,0.04910533818602562
====> Epoch: 1 Average loss: 0.099641 
Epoch: 2 [0/5999 (0%)]	Loss: 6.205069
Epoch: 2 [2560/5999 (43%)]	Loss: 5.296832
saving model at:2,0.03764178645610809
Epoch: 2 [5120/5999 (85%)]	Loss: 4.661443
saving model at:2,0.03553112870454788
====> Epoch: 2 Average loss: 0.039537 
Epoch: 3 [0/5999 (0%)]	Loss: 4.553276
Epoch: 3 [2560/5999 (43%)]	Loss: 3.377624
saving model at:3,0.02493771779537201
Epoch: 3 [5120/5999 (85%)]	Loss: 2.366510
saving model at:3,0.017674203991889955
====> Epoch: 3 Average loss: 0.025490 
Epoch: 4 [0/5999 (0%)]	Loss: 2.238212
Epoch: 4 [2560/5999 (43%)]	Loss: 1.700070
saving model at:4,0.013570026569068432
Epoch: 4 [5120/5999 (85%)]	Loss: 1.648516
saving model at:4,0.012343864001333714
====> Epoch: 4 Average loss: 0.014132 
Epoch: 5 [0/5999 (0%)]	Loss: 1.846160
Epoch: 5 [2560/5999 (43%)]	Loss: 1.352010
saving model at:5,0.010804231218993664
Epoch: 5 [5120/5999 (85%)]	Loss: 1.513677
saving model at:5,0.01004744729399681
====> Epoch: 5 Average loss: 0.011316 
Epoch: 6 [0/5999 (0%)]	Loss: 1.158395
Epoch: 6 [2560/5999 (43%)]	Loss: 1.254318
saving model at:6,0.00887195959687233
Epoch: 6 [5120/5999 (85%)]	Loss: 1.068467
saving model at:6,0.00816841048747301
====> Epoch: 6 Average loss: 0.009281 
Epoch: 7 [0/5999 (0%)]	Loss: 1.066220
Epoch: 7 [2560/5999 (43%)]	Loss: 1.049441
saving model at:7,0.007588502325117588
Epoch: 7 [5120/5999 (85%)]	Loss: 0.979130
saving model at:7,0.007266040548682213
====> Epoch: 7 Average loss: 0.008131 
Epoch: 8 [0/5999 (0%)]	Loss: 0.898214
Epoch: 8 [2560/5999 (43%)]	Loss: 0.968832
saving model at:8,0.006903397943824529
Epoch: 8 [5120/5999 (85%)]	Loss: 1.045021
====> Epoch: 8 Average loss: 0.007295 
Epoch: 9 [0/5999 (0%)]	Loss: 1.052641
Epoch: 9 [2560/5999 (43%)]	Loss: 0.917381
saving model at:9,0.006617216620594263
Epoch: 9 [5120/5999 (85%)]	Loss: 0.918210
saving model at:9,0.006301833063364029
====> Epoch: 9 Average loss: 0.006809 
Epoch: 10 [0/5999 (0%)]	Loss: 0.812770
Epoch: 10 [2560/5999 (43%)]	Loss: 0.832416
saving model at:10,0.006063448190689087
Epoch: 10 [5120/5999 (85%)]	Loss: 0.798068
saving model at:10,0.005929877188056707
====> Epoch: 10 Average loss: 0.006375 
Epoch: 11 [0/5999 (0%)]	Loss: 0.753419
Epoch: 11 [2560/5999 (43%)]	Loss: 0.792138
saving model at:11,0.005785189811140299
Epoch: 11 [5120/5999 (85%)]	Loss: 0.885595
saving model at:11,0.005686276100575924
====> Epoch: 11 Average loss: 0.006110 
Epoch: 12 [0/5999 (0%)]	Loss: 0.797492
Epoch: 12 [2560/5999 (43%)]	Loss: 0.738230
saving model at:12,0.005417738385498524
Epoch: 12 [5120/5999 (85%)]	Loss: 0.830849
====> Epoch: 12 Average loss: 0.005736 
Epoch: 13 [0/5999 (0%)]	Loss: 0.841595
Epoch: 13 [2560/5999 (43%)]	Loss: 0.630196
saving model at:13,0.005162931002676487
Epoch: 13 [5120/5999 (85%)]	Loss: 0.613704
saving model at:13,0.005021037902683019
====> Epoch: 13 Average loss: 0.005505 
Epoch: 14 [0/5999 (0%)]	Loss: 0.699552
Epoch: 14 [2560/5999 (43%)]	Loss: 0.680741
saving model at:14,0.004863502729684115
Epoch: 14 [5120/5999 (85%)]	Loss: 0.614942
saving model at:14,0.004701760783791542
====> Epoch: 14 Average loss: 0.005017 
Epoch: 15 [0/5999 (0%)]	Loss: 0.625368
Epoch: 15 [2560/5999 (43%)]	Loss: 0.719484
saving model at:15,0.004662312567234039
Epoch: 15 [5120/5999 (85%)]	Loss: 0.569962
====> Epoch: 15 Average loss: 0.005284 
Epoch: 16 [0/5999 (0%)]	Loss: 0.618837
Epoch: 16 [2560/5999 (43%)]	Loss: 0.608049
saving model at:16,0.004622460424900055
Epoch: 16 [5120/5999 (85%)]	Loss: 0.557838
saving model at:16,0.00449480889737606
====> Epoch: 16 Average loss: 0.004997 
Epoch: 17 [0/5999 (0%)]	Loss: 0.570432
Epoch: 17 [2560/5999 (43%)]	Loss: 0.558041
saving model at:17,0.00418228204920888
Epoch: 17 [5120/5999 (85%)]	Loss: 0.561345
saving model at:17,0.004087707083672285
====> Epoch: 17 Average loss: 0.004429 
Epoch: 18 [0/5999 (0%)]	Loss: 0.572261
Epoch: 18 [2560/5999 (43%)]	Loss: 0.500273
Epoch: 18 [5120/5999 (85%)]	Loss: 0.536593
saving model at:18,0.004049232695251703
====> Epoch: 18 Average loss: 0.004323 
Epoch: 19 [0/5999 (0%)]	Loss: 0.649234
Epoch: 19 [2560/5999 (43%)]	Loss: 0.443405
saving model at:19,0.003924717262387275
Epoch: 19 [5120/5999 (85%)]	Loss: 0.497783
saving model at:19,0.003859727080911398
====> Epoch: 19 Average loss: 0.004124 
Epoch: 20 [0/5999 (0%)]	Loss: 0.490327
Epoch: 20 [2560/5999 (43%)]	Loss: 0.466770
Epoch: 20 [5120/5999 (85%)]	Loss: 0.509185
====> Epoch: 20 Average loss: 0.004106 
Epoch: 21 [0/5999 (0%)]	Loss: 0.515306
Epoch: 21 [2560/5999 (43%)]	Loss: 0.507710
saving model at:21,0.0036952636651694775
Epoch: 21 [5120/5999 (85%)]	Loss: 0.516666
====> Epoch: 21 Average loss: 0.004047 
Epoch: 22 [0/5999 (0%)]	Loss: 0.469913
Epoch: 22 [2560/5999 (43%)]	Loss: 0.525461
saving model at:22,0.0036078116483986376
Epoch: 22 [5120/5999 (85%)]	Loss: 0.492402
====> Epoch: 22 Average loss: 0.003997 
Epoch: 23 [0/5999 (0%)]	Loss: 0.497800
Epoch: 23 [2560/5999 (43%)]	Loss: 0.487581
saving model at:23,0.0035275137946009635
Epoch: 23 [5120/5999 (85%)]	Loss: 0.419748
====> Epoch: 23 Average loss: 0.003725 
Epoch: 24 [0/5999 (0%)]	Loss: 0.444040
Epoch: 24 [2560/5999 (43%)]	Loss: 0.490551
Epoch: 24 [5120/5999 (85%)]	Loss: 0.459814
saving model at:24,0.0033733881339430808
====> Epoch: 24 Average loss: 0.003627 
Epoch: 25 [0/5999 (0%)]	Loss: 0.490665
Epoch: 25 [2560/5999 (43%)]	Loss: 0.411197
saving model at:25,0.0033127723932266234
Epoch: 25 [5120/5999 (85%)]	Loss: 0.490582
====> Epoch: 25 Average loss: 0.003742 
Epoch: 26 [0/5999 (0%)]	Loss: 0.482138
Epoch: 26 [2560/5999 (43%)]	Loss: 0.564605
Epoch: 26 [5120/5999 (85%)]	Loss: 0.462819
====> Epoch: 26 Average loss: 0.003884 
Epoch: 27 [0/5999 (0%)]	Loss: 0.417043
Epoch: 27 [2560/5999 (43%)]	Loss: 0.415015
Epoch: 27 [5120/5999 (85%)]	Loss: 0.427872
saving model at:27,0.003282505186274648
====> Epoch: 27 Average loss: 0.003706 
Epoch: 28 [0/5999 (0%)]	Loss: 0.441303
Epoch: 28 [2560/5999 (43%)]	Loss: 0.426357
saving model at:28,0.003173318788409233
Epoch: 28 [5120/5999 (85%)]	Loss: 0.480741
====> Epoch: 28 Average loss: 0.003467 
Epoch: 29 [0/5999 (0%)]	Loss: 0.397971
Epoch: 29 [2560/5999 (43%)]	Loss: 0.395760
saving model at:29,0.003162613108754158
Epoch: 29 [5120/5999 (85%)]	Loss: 0.407532
saving model at:29,0.0030925256200134755
====> Epoch: 29 Average loss: 0.003433 
Epoch: 30 [0/5999 (0%)]	Loss: 0.402398
Epoch: 30 [2560/5999 (43%)]	Loss: 0.506341
Epoch: 30 [5120/5999 (85%)]	Loss: 0.447070
saving model at:30,0.0030372876860201357
====> Epoch: 30 Average loss: 0.003434 
Epoch: 31 [0/5999 (0%)]	Loss: 0.421041
Epoch: 31 [2560/5999 (43%)]	Loss: 0.404964
Epoch: 31 [5120/5999 (85%)]	Loss: 0.464708
====> Epoch: 31 Average loss: 0.003341 
Epoch: 32 [0/5999 (0%)]	Loss: 0.385753
Epoch: 32 [2560/5999 (43%)]	Loss: 0.457181
saving model at:32,0.003024312101304531
Epoch: 32 [5120/5999 (85%)]	Loss: 0.426908
====> Epoch: 32 Average loss: 0.003196 
Epoch: 33 [0/5999 (0%)]	Loss: 0.411532
Epoch: 33 [2560/5999 (43%)]	Loss: 0.433462
Epoch: 33 [5120/5999 (85%)]	Loss: 0.430241
====> Epoch: 33 Average loss: 0.003278 
Epoch: 34 [0/5999 (0%)]	Loss: 0.452885
Epoch: 34 [2560/5999 (43%)]	Loss: 0.390940
Epoch: 34 [5120/5999 (85%)]	Loss: 0.408093
====> Epoch: 34 Average loss: 0.003234 
Epoch: 35 [0/5999 (0%)]	Loss: 0.442308
Epoch: 35 [2560/5999 (43%)]	Loss: 0.388822
saving model at:35,0.002946621362119913
Epoch: 35 [5120/5999 (85%)]	Loss: 0.437591
====> Epoch: 35 Average loss: 0.003216 
Epoch: 36 [0/5999 (0%)]	Loss: 0.377703
Epoch: 36 [2560/5999 (43%)]	Loss: 0.419906
saving model at:36,0.0028949025552719833
Epoch: 36 [5120/5999 (85%)]	Loss: 0.395301
====> Epoch: 36 Average loss: 0.003234 
Epoch: 37 [0/5999 (0%)]	Loss: 0.385129
Epoch: 37 [2560/5999 (43%)]	Loss: 0.428429
saving model at:37,0.0028488105442374943
Epoch: 37 [5120/5999 (85%)]	Loss: 0.425199
====> Epoch: 37 Average loss: 0.003091 
Epoch: 38 [0/5999 (0%)]	Loss: 0.441610
Epoch: 38 [2560/5999 (43%)]	Loss: 0.397325
Epoch: 38 [5120/5999 (85%)]	Loss: 0.389728
====> Epoch: 38 Average loss: 0.003083 
Epoch: 39 [0/5999 (0%)]	Loss: 0.425821
Epoch: 39 [2560/5999 (43%)]	Loss: 0.405762
Epoch: 39 [5120/5999 (85%)]	Loss: 0.429713
====> Epoch: 39 Average loss: 0.003102 
Epoch: 40 [0/5999 (0%)]	Loss: 0.382274
Epoch: 40 [2560/5999 (43%)]	Loss: 0.395664
saving model at:40,0.002837363537400961
Epoch: 40 [5120/5999 (85%)]	Loss: 0.428399
saving model at:40,0.0028308186810463666
====> Epoch: 40 Average loss: 0.003160 
Epoch: 41 [0/5999 (0%)]	Loss: 0.414445
Epoch: 41 [2560/5999 (43%)]	Loss: 0.344806
saving model at:41,0.0028134581353515386
Epoch: 41 [5120/5999 (85%)]	Loss: 0.416735
====> Epoch: 41 Average loss: 0.003155 
Epoch: 42 [0/5999 (0%)]	Loss: 0.405723
Epoch: 42 [2560/5999 (43%)]	Loss: 0.397277
Epoch: 42 [5120/5999 (85%)]	Loss: 0.427389
====> Epoch: 42 Average loss: 0.003350 
Epoch: 43 [0/5999 (0%)]	Loss: 0.393577
Epoch: 43 [2560/5999 (43%)]	Loss: 0.393234
Epoch: 43 [5120/5999 (85%)]	Loss: 0.421367
====> Epoch: 43 Average loss: 0.003255 
Epoch: 44 [0/5999 (0%)]	Loss: 0.407989
Epoch: 44 [2560/5999 (43%)]	Loss: 0.393613
Epoch: 44 [5120/5999 (85%)]	Loss: 0.365218
====> Epoch: 44 Average loss: 0.003059 
Epoch: 45 [0/5999 (0%)]	Loss: 0.401089
Epoch: 45 [2560/5999 (43%)]	Loss: 0.381734
Epoch: 45 [5120/5999 (85%)]	Loss: 0.353255
saving model at:45,0.0027451534029096364
====> Epoch: 45 Average loss: 0.003142 
Epoch: 46 [0/5999 (0%)]	Loss: 0.344418
Epoch: 46 [2560/5999 (43%)]	Loss: 0.463607
Epoch: 46 [5120/5999 (85%)]	Loss: 0.356032
====> Epoch: 46 Average loss: 0.003043 
Epoch: 47 [0/5999 (0%)]	Loss: 0.367726
Epoch: 47 [2560/5999 (43%)]	Loss: 0.466799
Epoch: 47 [5120/5999 (85%)]	Loss: 0.339044
====> Epoch: 47 Average loss: 0.003091 
Epoch: 48 [0/5999 (0%)]	Loss: 0.334620
Epoch: 48 [2560/5999 (43%)]	Loss: 0.404314
Epoch: 48 [5120/5999 (85%)]	Loss: 0.408349
====> Epoch: 48 Average loss: 0.003037 
Epoch: 49 [0/5999 (0%)]	Loss: 0.386725
Epoch: 49 [2560/5999 (43%)]	Loss: 0.384813
Epoch: 49 [5120/5999 (85%)]	Loss: 0.389906
====> Epoch: 49 Average loss: 0.003172 
Epoch: 50 [0/5999 (0%)]	Loss: 0.394208
Epoch: 50 [2560/5999 (43%)]	Loss: 0.364062
Epoch: 50 [5120/5999 (85%)]	Loss: 0.370086
saving model at:50,0.002734867073595524
====> Epoch: 50 Average loss: 0.003076 
Epoch: 51 [0/5999 (0%)]	Loss: 0.447941
Epoch: 51 [2560/5999 (43%)]	Loss: 0.370436
Epoch: 51 [5120/5999 (85%)]	Loss: 0.416887
====> Epoch: 51 Average loss: 0.003105 
Epoch: 52 [0/5999 (0%)]	Loss: 0.423792
Epoch: 52 [2560/5999 (43%)]	Loss: 0.386940
Epoch: 52 [5120/5999 (85%)]	Loss: 0.402342
====> Epoch: 52 Average loss: 0.003080 
Epoch: 53 [0/5999 (0%)]	Loss: 0.407503
Epoch: 53 [2560/5999 (43%)]	Loss: 0.353864
Epoch: 53 [5120/5999 (85%)]	Loss: 0.356174
saving model at:53,0.0026623311527073385
====> Epoch: 53 Average loss: 0.003001 
Epoch: 54 [0/5999 (0%)]	Loss: 0.360446
Epoch: 54 [2560/5999 (43%)]	Loss: 0.373930
Epoch: 54 [5120/5999 (85%)]	Loss: 0.380319
====> Epoch: 54 Average loss: 0.003024 
Epoch: 55 [0/5999 (0%)]	Loss: 0.378829
Epoch: 55 [2560/5999 (43%)]	Loss: 0.455141
Epoch: 55 [5120/5999 (85%)]	Loss: 0.400231
====> Epoch: 55 Average loss: 0.003084 
Epoch: 56 [0/5999 (0%)]	Loss: 0.358011
Epoch: 56 [2560/5999 (43%)]	Loss: 0.385265
Epoch: 56 [5120/5999 (85%)]	Loss: 0.364649
====> Epoch: 56 Average loss: 0.002920 
Epoch: 57 [0/5999 (0%)]	Loss: 0.353294
Epoch: 57 [2560/5999 (43%)]	Loss: 0.365127
Epoch: 57 [5120/5999 (85%)]	Loss: 0.443746
====> Epoch: 57 Average loss: 0.003038 
Epoch: 58 [0/5999 (0%)]	Loss: 0.457943
Epoch: 58 [2560/5999 (43%)]	Loss: 0.405641
Epoch: 58 [5120/5999 (85%)]	Loss: 0.335442
saving model at:58,0.0026488840598613023
====> Epoch: 58 Average loss: 0.002976 
Epoch: 59 [0/5999 (0%)]	Loss: 0.355557
Epoch: 59 [2560/5999 (43%)]	Loss: 0.326516
Epoch: 59 [5120/5999 (85%)]	Loss: 0.631716
====> Epoch: 59 Average loss: 0.003245 
Epoch: 60 [0/5999 (0%)]	Loss: 0.537845
Epoch: 60 [2560/5999 (43%)]	Loss: 0.398124
Epoch: 60 [5120/5999 (85%)]	Loss: 0.345272
====> Epoch: 60 Average loss: 0.003090 
Epoch: 61 [0/5999 (0%)]	Loss: 0.394791
Epoch: 61 [2560/5999 (43%)]	Loss: 0.477562
Epoch: 61 [5120/5999 (85%)]	Loss: 0.403901
====> Epoch: 61 Average loss: 0.002981 
Epoch: 62 [0/5999 (0%)]	Loss: 0.428962
Epoch: 62 [2560/5999 (43%)]	Loss: 0.446626
Epoch: 62 [5120/5999 (85%)]	Loss: 0.360165
====> Epoch: 62 Average loss: 0.003108 
Epoch: 63 [0/5999 (0%)]	Loss: 0.388154
Epoch: 63 [2560/5999 (43%)]	Loss: 0.339585
Epoch: 63 [5120/5999 (85%)]	Loss: 0.369038
saving model at:63,0.0026279274933040143
====> Epoch: 63 Average loss: 0.002936 
Epoch: 64 [0/5999 (0%)]	Loss: 0.383611
Epoch: 64 [2560/5999 (43%)]	Loss: 0.319100
saving model at:64,0.0026089713014662264
Epoch: 64 [5120/5999 (85%)]	Loss: 0.377057
====> Epoch: 64 Average loss: 0.002885 
Epoch: 65 [0/5999 (0%)]	Loss: 0.381308
Epoch: 65 [2560/5999 (43%)]	Loss: 0.500624
Epoch: 65 [5120/5999 (85%)]	Loss: 0.378052
====> Epoch: 65 Average loss: 0.003053 
Epoch: 66 [0/5999 (0%)]	Loss: 0.344474
Epoch: 66 [2560/5999 (43%)]	Loss: 0.337592
Epoch: 66 [5120/5999 (85%)]	Loss: 0.324107
====> Epoch: 66 Average loss: 0.002932 
Epoch: 67 [0/5999 (0%)]	Loss: 0.355420
Epoch: 67 [2560/5999 (43%)]	Loss: 0.400967
Epoch: 67 [5120/5999 (85%)]	Loss: 0.323182
====> Epoch: 67 Average loss: 0.002916 
Epoch: 68 [0/5999 (0%)]	Loss: 0.365477
Epoch: 68 [2560/5999 (43%)]	Loss: 0.450849
Epoch: 68 [5120/5999 (85%)]	Loss: 0.389440
====> Epoch: 68 Average loss: 0.003075 
Epoch: 69 [0/5999 (0%)]	Loss: 0.353006
Epoch: 69 [2560/5999 (43%)]	Loss: 0.326599
Epoch: 69 [5120/5999 (85%)]	Loss: 0.370303
====> Epoch: 69 Average loss: 0.002974 
Epoch: 70 [0/5999 (0%)]	Loss: 0.358536
Epoch: 70 [2560/5999 (43%)]	Loss: 0.353240
Epoch: 70 [5120/5999 (85%)]	Loss: 0.389873
====> Epoch: 70 Average loss: 0.003109 
Epoch: 71 [0/5999 (0%)]	Loss: 0.364149
Epoch: 71 [2560/5999 (43%)]	Loss: 0.357313
Epoch: 71 [5120/5999 (85%)]	Loss: 0.348266
====> Epoch: 71 Average loss: 0.002873 
Epoch: 72 [0/5999 (0%)]	Loss: 0.313270
Epoch: 72 [2560/5999 (43%)]	Loss: 0.363425
Epoch: 72 [5120/5999 (85%)]	Loss: 0.343769
====> Epoch: 72 Average loss: 0.002856 
Epoch: 73 [0/5999 (0%)]	Loss: 0.349017
Epoch: 73 [2560/5999 (43%)]	Loss: 0.359821
Epoch: 73 [5120/5999 (85%)]	Loss: 0.414674
====> Epoch: 73 Average loss: 0.003143 
Epoch: 74 [0/5999 (0%)]	Loss: 0.382740
Epoch: 74 [2560/5999 (43%)]	Loss: 0.358106
saving model at:74,0.002595440777018666
Epoch: 74 [5120/5999 (85%)]	Loss: 0.391120
====> Epoch: 74 Average loss: 0.002788 
Epoch: 75 [0/5999 (0%)]	Loss: 0.380594
Epoch: 75 [2560/5999 (43%)]	Loss: 0.353755
Epoch: 75 [5120/5999 (85%)]	Loss: 0.370722
====> Epoch: 75 Average loss: 0.002890 
Epoch: 76 [0/5999 (0%)]	Loss: 0.489693
Epoch: 76 [2560/5999 (43%)]	Loss: 0.376245
Epoch: 76 [5120/5999 (85%)]	Loss: 0.352476
====> Epoch: 76 Average loss: 0.002878 
Epoch: 77 [0/5999 (0%)]	Loss: 0.344109
Epoch: 77 [2560/5999 (43%)]	Loss: 0.374578
Epoch: 77 [5120/5999 (85%)]	Loss: 0.382233
====> Epoch: 77 Average loss: 0.002868 
Epoch: 78 [0/5999 (0%)]	Loss: 0.340099
Epoch: 78 [2560/5999 (43%)]	Loss: 0.361387
Epoch: 78 [5120/5999 (85%)]	Loss: 0.407332
====> Epoch: 78 Average loss: 0.002963 
Epoch: 79 [0/5999 (0%)]	Loss: 0.474115
Epoch: 79 [2560/5999 (43%)]	Loss: 0.423258
Epoch: 79 [5120/5999 (85%)]	Loss: 0.358861
====> Epoch: 79 Average loss: 0.002842 
Epoch: 80 [0/5999 (0%)]	Loss: 0.362294
Epoch: 80 [2560/5999 (43%)]	Loss: 0.371129
Epoch: 80 [5120/5999 (85%)]	Loss: 0.363827
====> Epoch: 80 Average loss: 0.002826 
Epoch: 81 [0/5999 (0%)]	Loss: 0.415819
Epoch: 81 [2560/5999 (43%)]	Loss: 0.369010
Epoch: 81 [5120/5999 (85%)]	Loss: 0.398582
====> Epoch: 81 Average loss: 0.002863 
Epoch: 82 [0/5999 (0%)]	Loss: 0.358858
Epoch: 82 [2560/5999 (43%)]	Loss: 0.351640
saving model at:82,0.0025933569557964804
Epoch: 82 [5120/5999 (85%)]	Loss: 0.366459
====> Epoch: 82 Average loss: 0.002806 
Epoch: 83 [0/5999 (0%)]	Loss: 0.352516
Epoch: 83 [2560/5999 (43%)]	Loss: 0.354569
Epoch: 83 [5120/5999 (85%)]	Loss: 0.385851
saving model at:83,0.0025802983436733484
====> Epoch: 83 Average loss: 0.002831 
Epoch: 84 [0/5999 (0%)]	Loss: 0.358655
Epoch: 84 [2560/5999 (43%)]	Loss: 0.341880
Epoch: 84 [5120/5999 (85%)]	Loss: 0.419719
====> Epoch: 84 Average loss: 0.002967 
Epoch: 85 [0/5999 (0%)]	Loss: 0.472524
Epoch: 85 [2560/5999 (43%)]	Loss: 0.383023
Epoch: 85 [5120/5999 (85%)]	Loss: 0.324623
====> Epoch: 85 Average loss: 0.002880 
Epoch: 86 [0/5999 (0%)]	Loss: 0.368325
Epoch: 86 [2560/5999 (43%)]	Loss: 0.369103
Epoch: 86 [5120/5999 (85%)]	Loss: 0.411847
====> Epoch: 86 Average loss: 0.002989 
Epoch: 87 [0/5999 (0%)]	Loss: 0.321326
Epoch: 87 [2560/5999 (43%)]	Loss: 0.365311
Epoch: 87 [5120/5999 (85%)]	Loss: 0.366867
====> Epoch: 87 Average loss: 0.002774 
Epoch: 88 [0/5999 (0%)]	Loss: 0.352430
Epoch: 88 [2560/5999 (43%)]	Loss: 0.369137
Epoch: 88 [5120/5999 (85%)]	Loss: 0.388702
====> Epoch: 88 Average loss: 0.002859 
Epoch: 89 [0/5999 (0%)]	Loss: 0.364758
Epoch: 89 [2560/5999 (43%)]	Loss: 0.315424
saving model at:89,0.002569142876192927
Epoch: 89 [5120/5999 (85%)]	Loss: 0.474369
====> Epoch: 89 Average loss: 0.002985 
Epoch: 90 [0/5999 (0%)]	Loss: 0.417160
Epoch: 90 [2560/5999 (43%)]	Loss: 0.344948
Epoch: 90 [5120/5999 (85%)]	Loss: 0.400747
====> Epoch: 90 Average loss: 0.002857 
Epoch: 91 [0/5999 (0%)]	Loss: 0.386330
Epoch: 91 [2560/5999 (43%)]	Loss: 0.351437
Epoch: 91 [5120/5999 (85%)]	Loss: 0.378789
====> Epoch: 91 Average loss: 0.002849 
Epoch: 92 [0/5999 (0%)]	Loss: 0.404815
Epoch: 92 [2560/5999 (43%)]	Loss: 0.345210
Epoch: 92 [5120/5999 (85%)]	Loss: 0.320226
====> Epoch: 92 Average loss: 0.002861 
Epoch: 93 [0/5999 (0%)]	Loss: 0.348954
Epoch: 93 [2560/5999 (43%)]	Loss: 0.373807
Epoch: 93 [5120/5999 (85%)]	Loss: 0.433500
====> Epoch: 93 Average loss: 0.002909 
Epoch: 94 [0/5999 (0%)]	Loss: 0.463316
Epoch: 94 [2560/5999 (43%)]	Loss: 0.350368
Epoch: 94 [5120/5999 (85%)]	Loss: 0.349339
====> Epoch: 94 Average loss: 0.002892 
Epoch: 95 [0/5999 (0%)]	Loss: 0.344356
Epoch: 95 [2560/5999 (43%)]	Loss: 0.360203
Epoch: 95 [5120/5999 (85%)]	Loss: 0.341119
====> Epoch: 95 Average loss: 0.002832 
Epoch: 96 [0/5999 (0%)]	Loss: 0.423133
Epoch: 96 [2560/5999 (43%)]	Loss: 0.317131
Epoch: 96 [5120/5999 (85%)]	Loss: 0.362569
====> Epoch: 96 Average loss: 0.002769 
Epoch: 97 [0/5999 (0%)]	Loss: 0.315173
Epoch: 97 [2560/5999 (43%)]	Loss: 0.351673
Epoch: 97 [5120/5999 (85%)]	Loss: 0.302847
saving model at:97,0.0025513372384011745
====> Epoch: 97 Average loss: 0.002771 
Epoch: 98 [0/5999 (0%)]	Loss: 0.360888
Epoch: 98 [2560/5999 (43%)]	Loss: 0.526018
Epoch: 98 [5120/5999 (85%)]	Loss: 0.494157
====> Epoch: 98 Average loss: 0.003216 
Epoch: 99 [0/5999 (0%)]	Loss: 0.390444
Epoch: 99 [2560/5999 (43%)]	Loss: 0.333966
Epoch: 99 [5120/5999 (85%)]	Loss: 0.447058
====> Epoch: 99 Average loss: 0.002968 
Epoch: 100 [0/5999 (0%)]	Loss: 0.340807
Epoch: 100 [2560/5999 (43%)]	Loss: 0.351699
Epoch: 100 [5120/5999 (85%)]	Loss: 0.341687
====> Epoch: 100 Average loss: 0.002874 
Epoch: 101 [0/5999 (0%)]	Loss: 0.382092
Epoch: 101 [2560/5999 (43%)]	Loss: 0.342314
Epoch: 101 [5120/5999 (85%)]	Loss: 0.384687
====> Epoch: 101 Average loss: 0.002787 
Epoch: 102 [0/5999 (0%)]	Loss: 0.338854
Epoch: 102 [2560/5999 (43%)]	Loss: 0.378174
Epoch: 102 [5120/5999 (85%)]	Loss: 0.327612
====> Epoch: 102 Average loss: 0.002788 
Epoch: 103 [0/5999 (0%)]	Loss: 0.351975
Epoch: 103 [2560/5999 (43%)]	Loss: 0.377797
Epoch: 103 [5120/5999 (85%)]	Loss: 0.388910
====> Epoch: 103 Average loss: 0.002739 
Epoch: 104 [0/5999 (0%)]	Loss: 0.391083
Epoch: 104 [2560/5999 (43%)]	Loss: 0.341318
Epoch: 104 [5120/5999 (85%)]	Loss: 0.445260
====> Epoch: 104 Average loss: 0.002960 
Epoch: 105 [0/5999 (0%)]	Loss: 0.347039
Epoch: 105 [2560/5999 (43%)]	Loss: 0.372685
Epoch: 105 [5120/5999 (85%)]	Loss: 0.340786
====> Epoch: 105 Average loss: 0.002843 
Epoch: 106 [0/5999 (0%)]	Loss: 0.377152
Epoch: 106 [2560/5999 (43%)]	Loss: 0.376098
Epoch: 106 [5120/5999 (85%)]	Loss: 0.379069
====> Epoch: 106 Average loss: 0.002822 
Epoch: 107 [0/5999 (0%)]	Loss: 0.418797
Epoch: 107 [2560/5999 (43%)]	Loss: 0.338147
Epoch: 107 [5120/5999 (85%)]	Loss: 0.355933
====> Epoch: 107 Average loss: 0.003008 
Epoch: 108 [0/5999 (0%)]	Loss: 0.389754
Epoch: 108 [2560/5999 (43%)]	Loss: 0.342335
Epoch: 108 [5120/5999 (85%)]	Loss: 0.475891
====> Epoch: 108 Average loss: 0.002897 
Epoch: 109 [0/5999 (0%)]	Loss: 0.390438
Epoch: 109 [2560/5999 (43%)]	Loss: 0.362677
Epoch: 109 [5120/5999 (85%)]	Loss: 0.350166
====> Epoch: 109 Average loss: 0.002802 
Epoch: 110 [0/5999 (0%)]	Loss: 0.350330
Epoch: 110 [2560/5999 (43%)]	Loss: 0.377207
Epoch: 110 [5120/5999 (85%)]	Loss: 0.359364
====> Epoch: 110 Average loss: 0.002815 
Epoch: 111 [0/5999 (0%)]	Loss: 0.366657
Epoch: 111 [2560/5999 (43%)]	Loss: 0.359693
Epoch: 111 [5120/5999 (85%)]	Loss: 0.320036
====> Epoch: 111 Average loss: 0.002925 
Epoch: 112 [0/5999 (0%)]	Loss: 0.346930
Epoch: 112 [2560/5999 (43%)]	Loss: 0.349653
Epoch: 112 [5120/5999 (85%)]	Loss: 0.365800
====> Epoch: 112 Average loss: 0.002787 
Epoch: 113 [0/5999 (0%)]	Loss: 0.390735
Epoch: 113 [2560/5999 (43%)]	Loss: 0.388343
Epoch: 113 [5120/5999 (85%)]	Loss: 0.331085
====> Epoch: 113 Average loss: 0.002807 
Epoch: 114 [0/5999 (0%)]	Loss: 0.345940
Epoch: 114 [2560/5999 (43%)]	Loss: 0.333020
Epoch: 114 [5120/5999 (85%)]	Loss: 0.363422
====> Epoch: 114 Average loss: 0.002795 
Epoch: 115 [0/5999 (0%)]	Loss: 0.325646
Epoch: 115 [2560/5999 (43%)]	Loss: 0.391631
Epoch: 115 [5120/5999 (85%)]	Loss: 0.387205
====> Epoch: 115 Average loss: 0.002802 
Epoch: 116 [0/5999 (0%)]	Loss: 0.367946
Epoch: 116 [2560/5999 (43%)]	Loss: 0.368788
Epoch: 116 [5120/5999 (85%)]	Loss: 0.337264
====> Epoch: 116 Average loss: 0.002745 
Epoch: 117 [0/5999 (0%)]	Loss: 0.318069
Epoch: 117 [2560/5999 (43%)]	Loss: 0.303522
Epoch: 117 [5120/5999 (85%)]	Loss: 0.358558
====> Epoch: 117 Average loss: 0.002794 
Epoch: 118 [0/5999 (0%)]	Loss: 0.324438
Epoch: 118 [2560/5999 (43%)]	Loss: 0.362019
Epoch: 118 [5120/5999 (85%)]	Loss: 0.302700
====> Epoch: 118 Average loss: 0.002853 
Epoch: 119 [0/5999 (0%)]	Loss: 0.378042
Epoch: 119 [2560/5999 (43%)]	Loss: 0.377470
Epoch: 119 [5120/5999 (85%)]	Loss: 0.346875
====> Epoch: 119 Average loss: 0.002753 
Epoch: 120 [0/5999 (0%)]	Loss: 0.358417
Epoch: 120 [2560/5999 (43%)]	Loss: 0.368613
Epoch: 120 [5120/5999 (85%)]	Loss: 0.452326
====> Epoch: 120 Average loss: 0.002870 
Epoch: 121 [0/5999 (0%)]	Loss: 0.368935
Epoch: 121 [2560/5999 (43%)]	Loss: 0.391786
Epoch: 121 [5120/5999 (85%)]	Loss: 0.346608
====> Epoch: 121 Average loss: 0.002805 
Epoch: 122 [0/5999 (0%)]	Loss: 0.420825
Epoch: 122 [2560/5999 (43%)]	Loss: 0.359304
Epoch: 122 [5120/5999 (85%)]	Loss: 0.349223
====> Epoch: 122 Average loss: 0.002825 
Epoch: 123 [0/5999 (0%)]	Loss: 0.361324
Epoch: 123 [2560/5999 (43%)]	Loss: 0.385904
Epoch: 123 [5120/5999 (85%)]	Loss: 0.341868
====> Epoch: 123 Average loss: 0.002890 
Epoch: 124 [0/5999 (0%)]	Loss: 0.358233
Epoch: 124 [2560/5999 (43%)]	Loss: 0.334671
saving model at:124,0.0025510382745414974
Epoch: 124 [5120/5999 (85%)]	Loss: 0.333748
====> Epoch: 124 Average loss: 0.002728 
Epoch: 125 [0/5999 (0%)]	Loss: 0.354818
Epoch: 125 [2560/5999 (43%)]	Loss: 0.309345
Epoch: 125 [5120/5999 (85%)]	Loss: 0.330270
====> Epoch: 125 Average loss: 0.002713 
Epoch: 126 [0/5999 (0%)]	Loss: 0.329448
Epoch: 126 [2560/5999 (43%)]	Loss: 0.391732
Epoch: 126 [5120/5999 (85%)]	Loss: 0.386212
====> Epoch: 126 Average loss: 0.002741 
Epoch: 127 [0/5999 (0%)]	Loss: 0.321810
Epoch: 127 [2560/5999 (43%)]	Loss: 0.315755
Epoch: 127 [5120/5999 (85%)]	Loss: 0.344211
====> Epoch: 127 Average loss: 0.002769 
Epoch: 128 [0/5999 (0%)]	Loss: 0.373536
Epoch: 128 [2560/5999 (43%)]	Loss: 0.346368
Epoch: 128 [5120/5999 (85%)]	Loss: 0.366623
====> Epoch: 128 Average loss: 0.002774 
Epoch: 129 [0/5999 (0%)]	Loss: 0.393381
Epoch: 129 [2560/5999 (43%)]	Loss: 0.306263
Epoch: 129 [5120/5999 (85%)]	Loss: 0.328205
====> Epoch: 129 Average loss: 0.002706 
Epoch: 130 [0/5999 (0%)]	Loss: 0.344932
Epoch: 130 [2560/5999 (43%)]	Loss: 0.324711
Epoch: 130 [5120/5999 (85%)]	Loss: 0.333041
====> Epoch: 130 Average loss: 0.002705 
Epoch: 131 [0/5999 (0%)]	Loss: 0.340223
Epoch: 131 [2560/5999 (43%)]	Loss: 0.359765
Epoch: 131 [5120/5999 (85%)]	Loss: 0.336193
====> Epoch: 131 Average loss: 0.002771 
Epoch: 132 [0/5999 (0%)]	Loss: 0.411157
Epoch: 132 [2560/5999 (43%)]	Loss: 0.344853
Epoch: 132 [5120/5999 (85%)]	Loss: 0.429776
====> Epoch: 132 Average loss: 0.002774 
Epoch: 133 [0/5999 (0%)]	Loss: 0.336528
Epoch: 133 [2560/5999 (43%)]	Loss: 0.362751
Epoch: 133 [5120/5999 (85%)]	Loss: 0.348051
====> Epoch: 133 Average loss: 0.002833 
Epoch: 134 [0/5999 (0%)]	Loss: 0.384326
Epoch: 134 [2560/5999 (43%)]	Loss: 0.324928
Epoch: 134 [5120/5999 (85%)]	Loss: 0.383131
====> Epoch: 134 Average loss: 0.002740 
Epoch: 135 [0/5999 (0%)]	Loss: 0.307769
Epoch: 135 [2560/5999 (43%)]	Loss: 0.392145
Epoch: 135 [5120/5999 (85%)]	Loss: 0.373406
====> Epoch: 135 Average loss: 0.002833 
Epoch: 136 [0/5999 (0%)]	Loss: 0.332479
Epoch: 136 [2560/5999 (43%)]	Loss: 0.305511
saving model at:136,0.0025287408102303744
Epoch: 136 [5120/5999 (85%)]	Loss: 0.344303
====> Epoch: 136 Average loss: 0.002656 
Epoch: 137 [0/5999 (0%)]	Loss: 0.322630
Epoch: 137 [2560/5999 (43%)]	Loss: 0.343748
Epoch: 137 [5120/5999 (85%)]	Loss: 0.384437
====> Epoch: 137 Average loss: 0.002701 
Epoch: 138 [0/5999 (0%)]	Loss: 0.359454
Epoch: 138 [2560/5999 (43%)]	Loss: 0.302945
Epoch: 138 [5120/5999 (85%)]	Loss: 0.310481
saving model at:138,0.0024896829426288604
====> Epoch: 138 Average loss: 0.002692 
Epoch: 139 [0/5999 (0%)]	Loss: 0.307261
Epoch: 139 [2560/5999 (43%)]	Loss: 0.315305
saving model at:139,0.002438413178548217
Epoch: 139 [5120/5999 (85%)]	Loss: 0.313769
saving model at:139,0.002413108976557851
====> Epoch: 139 Average loss: 0.002556 
Epoch: 140 [0/5999 (0%)]	Loss: 0.423839
Epoch: 140 [2560/5999 (43%)]	Loss: 0.337693
Epoch: 140 [5120/5999 (85%)]	Loss: 0.276702
saving model at:140,0.002033713258802891
====> Epoch: 140 Average loss: 0.002515 
Epoch: 141 [0/5999 (0%)]	Loss: 0.261589
Epoch: 141 [2560/5999 (43%)]	Loss: 0.275788
saving model at:141,0.0016289947088807821
Epoch: 141 [5120/5999 (85%)]	Loss: 0.189665
saving model at:141,0.0016205152841284871
====> Epoch: 141 Average loss: 0.001784 
Epoch: 142 [0/5999 (0%)]	Loss: 0.192734
Epoch: 142 [2560/5999 (43%)]	Loss: 0.137491
saving model at:142,0.0010916220461949706
Epoch: 142 [5120/5999 (85%)]	Loss: 0.134146
saving model at:142,0.0009896683669649064
====> Epoch: 142 Average loss: 0.001238 
Epoch: 143 [0/5999 (0%)]	Loss: 0.158977
Epoch: 143 [2560/5999 (43%)]	Loss: 0.163833
Epoch: 143 [5120/5999 (85%)]	Loss: 0.108560
saving model at:143,0.0008891227571293711
====> Epoch: 143 Average loss: 0.001085 
Epoch: 144 [0/5999 (0%)]	Loss: 0.106761
Epoch: 144 [2560/5999 (43%)]	Loss: 0.219504
Epoch: 144 [5120/5999 (85%)]	Loss: 0.082443
saving model at:144,0.0007851832173764706
====> Epoch: 144 Average loss: 0.001069 
Epoch: 145 [0/5999 (0%)]	Loss: 0.095994
Epoch: 145 [2560/5999 (43%)]	Loss: 0.090509
saving model at:145,0.0007467273594811559
Epoch: 145 [5120/5999 (85%)]	Loss: 0.077796
saving model at:145,0.0006651432076469064
====> Epoch: 145 Average loss: 0.000808 
Epoch: 146 [0/5999 (0%)]	Loss: 0.086503
Epoch: 146 [2560/5999 (43%)]	Loss: 0.121332
Epoch: 146 [5120/5999 (85%)]	Loss: 0.155038
====> Epoch: 146 Average loss: 0.000840 
Epoch: 147 [0/5999 (0%)]	Loss: 0.135058
Epoch: 147 [2560/5999 (43%)]	Loss: 0.081251
Epoch: 147 [5120/5999 (85%)]	Loss: 0.069622
====> Epoch: 147 Average loss: 0.000877 
Epoch: 148 [0/5999 (0%)]	Loss: 0.075842
Epoch: 148 [2560/5999 (43%)]	Loss: 0.077306
saving model at:148,0.0005815064636990428
Epoch: 148 [5120/5999 (85%)]	Loss: 0.081471
saving model at:148,0.0005090118823572993
====> Epoch: 148 Average loss: 0.000669 
Epoch: 149 [0/5999 (0%)]	Loss: 0.060995
Epoch: 149 [2560/5999 (43%)]	Loss: 0.064438
saving model at:149,0.0004954591430723667
Epoch: 149 [5120/5999 (85%)]	Loss: 0.102461
====> Epoch: 149 Average loss: 0.000656 
Epoch: 150 [0/5999 (0%)]	Loss: 0.065636
Epoch: 150 [2560/5999 (43%)]	Loss: 0.064064
Epoch: 150 [5120/5999 (85%)]	Loss: 0.096449
saving model at:150,0.0004849880840629339
====> Epoch: 150 Average loss: 0.000716 
Epoch: 151 [0/5999 (0%)]	Loss: 0.059638
Epoch: 151 [2560/5999 (43%)]	Loss: 0.055714
Epoch: 151 [5120/5999 (85%)]	Loss: 0.084720
====> Epoch: 151 Average loss: 0.000643 
Epoch: 152 [0/5999 (0%)]	Loss: 0.105138
Epoch: 152 [2560/5999 (43%)]	Loss: 0.060649
saving model at:152,0.0004551846818067133
Epoch: 152 [5120/5999 (85%)]	Loss: 0.056371
====> Epoch: 152 Average loss: 0.000688 
Epoch: 153 [0/5999 (0%)]	Loss: 0.048010
Epoch: 153 [2560/5999 (43%)]	Loss: 0.060583
saving model at:153,0.0004205188557971269
Epoch: 153 [5120/5999 (85%)]	Loss: 0.069119
saving model at:153,0.0004143472290597856
====> Epoch: 153 Average loss: 0.000539 
Epoch: 154 [0/5999 (0%)]	Loss: 0.060895
Epoch: 154 [2560/5999 (43%)]	Loss: 0.117009
Epoch: 154 [5120/5999 (85%)]	Loss: 0.092511
====> Epoch: 154 Average loss: 0.000718 
Epoch: 155 [0/5999 (0%)]	Loss: 0.054037
Epoch: 155 [2560/5999 (43%)]	Loss: 0.051693
Epoch: 155 [5120/5999 (85%)]	Loss: 0.058082
saving model at:155,0.0003827396479900926
====> Epoch: 155 Average loss: 0.000522 
Epoch: 156 [0/5999 (0%)]	Loss: 0.082999
Epoch: 156 [2560/5999 (43%)]	Loss: 0.053130
saving model at:156,0.0003725807138253003
Epoch: 156 [5120/5999 (85%)]	Loss: 0.120731
====> Epoch: 156 Average loss: 0.000566 
Epoch: 157 [0/5999 (0%)]	Loss: 0.067102
Epoch: 157 [2560/5999 (43%)]	Loss: 0.051374
saving model at:157,0.0003635141379199922
Epoch: 157 [5120/5999 (85%)]	Loss: 0.078492
====> Epoch: 157 Average loss: 0.000537 
Epoch: 158 [0/5999 (0%)]	Loss: 0.058027
Epoch: 158 [2560/5999 (43%)]	Loss: 0.141316
Epoch: 158 [5120/5999 (85%)]	Loss: 0.061591
====> Epoch: 158 Average loss: 0.000634 
Epoch: 159 [0/5999 (0%)]	Loss: 0.079550
Epoch: 159 [2560/5999 (43%)]	Loss: 0.062532
Epoch: 159 [5120/5999 (85%)]	Loss: 0.101254
====> Epoch: 159 Average loss: 0.000663 
Epoch: 160 [0/5999 (0%)]	Loss: 0.050006
Epoch: 160 [2560/5999 (43%)]	Loss: 0.050874
saving model at:160,0.00035305911605246367
Epoch: 160 [5120/5999 (85%)]	Loss: 0.034722
saving model at:160,0.0003062373078428209
====> Epoch: 160 Average loss: 0.000473 
Epoch: 161 [0/5999 (0%)]	Loss: 0.037566
Epoch: 161 [2560/5999 (43%)]	Loss: 0.060072
Epoch: 161 [5120/5999 (85%)]	Loss: 0.087899
====> Epoch: 161 Average loss: 0.000429 
Epoch: 162 [0/5999 (0%)]	Loss: 0.046741
Epoch: 162 [2560/5999 (43%)]	Loss: 0.034612
Epoch: 162 [5120/5999 (85%)]	Loss: 0.056398
saving model at:162,0.00028791147051379087
====> Epoch: 162 Average loss: 0.000444 
Epoch: 163 [0/5999 (0%)]	Loss: 0.058917
Epoch: 163 [2560/5999 (43%)]	Loss: 0.045288
Epoch: 163 [5120/5999 (85%)]	Loss: 0.040965
====> Epoch: 163 Average loss: 0.000421 
Epoch: 164 [0/5999 (0%)]	Loss: 0.152666
Epoch: 164 [2560/5999 (43%)]	Loss: 0.041750
Epoch: 164 [5120/5999 (85%)]	Loss: 0.056581
====> Epoch: 164 Average loss: 0.000446 
Epoch: 165 [0/5999 (0%)]	Loss: 0.043051
Epoch: 165 [2560/5999 (43%)]	Loss: 0.040191
saving model at:165,0.00027195641584694386
Epoch: 165 [5120/5999 (85%)]	Loss: 0.086555
====> Epoch: 165 Average loss: 0.000623 
Epoch: 166 [0/5999 (0%)]	Loss: 0.100502
Epoch: 166 [2560/5999 (43%)]	Loss: 0.096615
Epoch: 166 [5120/5999 (85%)]	Loss: 0.051016
====> Epoch: 166 Average loss: 0.000619 
Epoch: 167 [0/5999 (0%)]	Loss: 0.067982
Epoch: 167 [2560/5999 (43%)]	Loss: 0.045933
Epoch: 167 [5120/5999 (85%)]	Loss: 0.120495
====> Epoch: 167 Average loss: 0.000614 
Epoch: 168 [0/5999 (0%)]	Loss: 0.073823
Epoch: 168 [2560/5999 (43%)]	Loss: 0.092883
Epoch: 168 [5120/5999 (85%)]	Loss: 0.054679
====> Epoch: 168 Average loss: 0.000562 
Epoch: 169 [0/5999 (0%)]	Loss: 0.064656
Epoch: 169 [2560/5999 (43%)]	Loss: 0.038244
Epoch: 169 [5120/5999 (85%)]	Loss: 0.050647
====> Epoch: 169 Average loss: 0.000493 
Epoch: 170 [0/5999 (0%)]	Loss: 0.045626
Epoch: 170 [2560/5999 (43%)]	Loss: 0.061292
saving model at:170,0.00023815372330136597
Epoch: 170 [5120/5999 (85%)]	Loss: 0.074990
====> Epoch: 170 Average loss: 0.000504 
Epoch: 171 [0/5999 (0%)]	Loss: 0.042437
Epoch: 171 [2560/5999 (43%)]	Loss: 0.145574
Epoch: 171 [5120/5999 (85%)]	Loss: 0.037721
====> Epoch: 171 Average loss: 0.000481 
Epoch: 172 [0/5999 (0%)]	Loss: 0.045882
Epoch: 172 [2560/5999 (43%)]	Loss: 0.060734
Epoch: 172 [5120/5999 (85%)]	Loss: 0.044548
====> Epoch: 172 Average loss: 0.000406 
Epoch: 173 [0/5999 (0%)]	Loss: 0.043794
Epoch: 173 [2560/5999 (43%)]	Loss: 0.049822
Epoch: 173 [5120/5999 (85%)]	Loss: 0.109232
====> Epoch: 173 Average loss: 0.000626 
Epoch: 174 [0/5999 (0%)]	Loss: 0.037800
Epoch: 174 [2560/5999 (43%)]	Loss: 0.042580
Epoch: 174 [5120/5999 (85%)]	Loss: 0.047169
saving model at:174,0.00023181984666734935
====> Epoch: 174 Average loss: 0.000366 
Epoch: 175 [0/5999 (0%)]	Loss: 0.046823
Epoch: 175 [2560/5999 (43%)]	Loss: 0.090858
Epoch: 175 [5120/5999 (85%)]	Loss: 0.056329
====> Epoch: 175 Average loss: 0.000475 
Epoch: 176 [0/5999 (0%)]	Loss: 0.044796
Epoch: 176 [2560/5999 (43%)]	Loss: 0.039895
Epoch: 176 [5120/5999 (85%)]	Loss: 0.030302
====> Epoch: 176 Average loss: 0.000537 
Epoch: 177 [0/5999 (0%)]	Loss: 0.088109
Epoch: 177 [2560/5999 (43%)]	Loss: 0.039825
Epoch: 177 [5120/5999 (85%)]	Loss: 0.055531
saving model at:177,0.00019925269507803023
====> Epoch: 177 Average loss: 0.000411 
Epoch: 178 [0/5999 (0%)]	Loss: 0.039669
Epoch: 178 [2560/5999 (43%)]	Loss: 0.068685
Epoch: 178 [5120/5999 (85%)]	Loss: 0.064463
====> Epoch: 178 Average loss: 0.000489 
Epoch: 179 [0/5999 (0%)]	Loss: 0.050841
Epoch: 179 [2560/5999 (43%)]	Loss: 0.062640
Epoch: 179 [5120/5999 (85%)]	Loss: 0.081008
====> Epoch: 179 Average loss: 0.000474 
Epoch: 180 [0/5999 (0%)]	Loss: 0.041670
Epoch: 180 [2560/5999 (43%)]	Loss: 0.029125
Epoch: 180 [5120/5999 (85%)]	Loss: 0.055892
====> Epoch: 180 Average loss: 0.000443 
Epoch: 181 [0/5999 (0%)]	Loss: 0.094554
Epoch: 181 [2560/5999 (43%)]	Loss: 0.048175
Epoch: 181 [5120/5999 (85%)]	Loss: 0.061099
====> Epoch: 181 Average loss: 0.000406 
Epoch: 182 [0/5999 (0%)]	Loss: 0.051224
Epoch: 182 [2560/5999 (43%)]	Loss: 0.027648
Epoch: 182 [5120/5999 (85%)]	Loss: 0.055160
====> Epoch: 182 Average loss: 0.000389 
Epoch: 183 [0/5999 (0%)]	Loss: 0.038090
Epoch: 183 [2560/5999 (43%)]	Loss: 0.025081
saving model at:183,0.00017825917399022728
Epoch: 183 [5120/5999 (85%)]	Loss: 0.031130
====> Epoch: 183 Average loss: 0.000296 
Epoch: 184 [0/5999 (0%)]	Loss: 0.035819
Epoch: 184 [2560/5999 (43%)]	Loss: 0.057557
Epoch: 184 [5120/5999 (85%)]	Loss: 0.029602
====> Epoch: 184 Average loss: 0.000352 
Epoch: 185 [0/5999 (0%)]	Loss: 0.023926
Epoch: 185 [2560/5999 (43%)]	Loss: 0.071827
Epoch: 185 [5120/5999 (85%)]	Loss: 0.051171
====> Epoch: 185 Average loss: 0.000463 
Epoch: 186 [0/5999 (0%)]	Loss: 0.032147
Epoch: 186 [2560/5999 (43%)]	Loss: 0.069540
Epoch: 186 [5120/5999 (85%)]	Loss: 0.037749
====> Epoch: 186 Average loss: 0.000351 
Epoch: 187 [0/5999 (0%)]	Loss: 0.056151
Epoch: 187 [2560/5999 (43%)]	Loss: 0.049516
Epoch: 187 [5120/5999 (85%)]	Loss: 0.062689
====> Epoch: 187 Average loss: 0.000506 
Epoch: 188 [0/5999 (0%)]	Loss: 0.060608
Epoch: 188 [2560/5999 (43%)]	Loss: 0.042950
Epoch: 188 [5120/5999 (85%)]	Loss: 0.106977
====> Epoch: 188 Average loss: 0.000492 
Epoch: 189 [0/5999 (0%)]	Loss: 0.120106
Epoch: 189 [2560/5999 (43%)]	Loss: 0.032363
Epoch: 189 [5120/5999 (85%)]	Loss: 0.049327
====> Epoch: 189 Average loss: 0.000475 
Epoch: 190 [0/5999 (0%)]	Loss: 0.049979
Epoch: 190 [2560/5999 (43%)]	Loss: 0.039231
Epoch: 190 [5120/5999 (85%)]	Loss: 0.037407
====> Epoch: 190 Average loss: 0.000404 
Epoch: 191 [0/5999 (0%)]	Loss: 0.044570
Epoch: 191 [2560/5999 (43%)]	Loss: 0.094158
Epoch: 191 [5120/5999 (85%)]	Loss: 0.052616
====> Epoch: 191 Average loss: 0.000495 
Epoch: 192 [0/5999 (0%)]	Loss: 0.030096
Epoch: 192 [2560/5999 (43%)]	Loss: 0.047084
Epoch: 192 [5120/5999 (85%)]	Loss: 0.035067
====> Epoch: 192 Average loss: 0.000372 
Epoch: 193 [0/5999 (0%)]	Loss: 0.028126
Epoch: 193 [2560/5999 (43%)]	Loss: 0.056884
Epoch: 193 [5120/5999 (85%)]	Loss: 0.035522
====> Epoch: 193 Average loss: 0.000355 
Epoch: 194 [0/5999 (0%)]	Loss: 0.031343
Epoch: 194 [2560/5999 (43%)]	Loss: 0.041734
Epoch: 194 [5120/5999 (85%)]	Loss: 0.071357
====> Epoch: 194 Average loss: 0.000409 
Epoch: 195 [0/5999 (0%)]	Loss: 0.033565
Epoch: 195 [2560/5999 (43%)]	Loss: 0.028103
Epoch: 195 [5120/5999 (85%)]	Loss: 0.048540
====> Epoch: 195 Average loss: 0.000349 
Epoch: 196 [0/5999 (0%)]	Loss: 0.024870
Epoch: 196 [2560/5999 (43%)]	Loss: 0.031852
saving model at:196,0.0001627028415678069
Epoch: 196 [5120/5999 (85%)]	Loss: 0.037672
====> Epoch: 196 Average loss: 0.000273 
Epoch: 197 [0/5999 (0%)]	Loss: 0.029056
Epoch: 197 [2560/5999 (43%)]	Loss: 0.108480
Epoch: 197 [5120/5999 (85%)]	Loss: 0.031336
====> Epoch: 197 Average loss: 0.000326 
Epoch: 198 [0/5999 (0%)]	Loss: 0.031053
Epoch: 198 [2560/5999 (43%)]	Loss: 0.043561
Epoch: 198 [5120/5999 (85%)]	Loss: 0.052305
====> Epoch: 198 Average loss: 0.000482 
Epoch: 199 [0/5999 (0%)]	Loss: 0.053555
Epoch: 199 [2560/5999 (43%)]	Loss: 0.025872
Epoch: 199 [5120/5999 (85%)]	Loss: 0.029789
====> Epoch: 199 Average loss: 0.000318 
Epoch: 200 [0/5999 (0%)]	Loss: 0.025726
Epoch: 200 [2560/5999 (43%)]	Loss: 0.033198
Epoch: 200 [5120/5999 (85%)]	Loss: 0.047936
====> Epoch: 200 Average loss: 0.000370 
Epoch: 201 [0/5999 (0%)]	Loss: 0.030431
Epoch: 201 [2560/5999 (43%)]	Loss: 0.051374
Epoch: 201 [5120/5999 (85%)]	Loss: 0.029133
====> Epoch: 201 Average loss: 0.000306 
Epoch: 202 [0/5999 (0%)]	Loss: 0.027940
Epoch: 202 [2560/5999 (43%)]	Loss: 0.110301
Epoch: 202 [5120/5999 (85%)]	Loss: 0.036272
====> Epoch: 202 Average loss: 0.000380 
Epoch: 203 [0/5999 (0%)]	Loss: 0.029939
Epoch: 203 [2560/5999 (43%)]	Loss: 0.047682
Epoch: 203 [5120/5999 (85%)]	Loss: 0.029873
====> Epoch: 203 Average loss: 0.000533 
Epoch: 204 [0/5999 (0%)]	Loss: 0.037957
Epoch: 204 [2560/5999 (43%)]	Loss: 0.022766
Epoch: 204 [5120/5999 (85%)]	Loss: 0.072257
====> Epoch: 204 Average loss: 0.000302 
Epoch: 205 [0/5999 (0%)]	Loss: 0.020030
Epoch: 205 [2560/5999 (43%)]	Loss: 0.023731
Epoch: 205 [5120/5999 (85%)]	Loss: 0.026731
saving model at:205,0.00016126951726619154
====> Epoch: 205 Average loss: 0.000280 
Epoch: 206 [0/5999 (0%)]	Loss: 0.046097
Epoch: 206 [2560/5999 (43%)]	Loss: 0.025971
saving model at:206,0.00016024198941886426
Epoch: 206 [5120/5999 (85%)]	Loss: 0.052440
====> Epoch: 206 Average loss: 0.000358 
Epoch: 207 [0/5999 (0%)]	Loss: 0.132352
Epoch: 207 [2560/5999 (43%)]	Loss: 0.034047
Epoch: 207 [5120/5999 (85%)]	Loss: 0.024920
====> Epoch: 207 Average loss: 0.000289 
Epoch: 208 [0/5999 (0%)]	Loss: 0.049162
Epoch: 208 [2560/5999 (43%)]	Loss: 0.064101
Epoch: 208 [5120/5999 (85%)]	Loss: 0.079900
====> Epoch: 208 Average loss: 0.000446 
Epoch: 209 [0/5999 (0%)]	Loss: 0.032213
Epoch: 209 [2560/5999 (43%)]	Loss: 0.050597
Epoch: 209 [5120/5999 (85%)]	Loss: 0.032618
====> Epoch: 209 Average loss: 0.000317 
Epoch: 210 [0/5999 (0%)]	Loss: 0.055789
Epoch: 210 [2560/5999 (43%)]	Loss: 0.052508
Epoch: 210 [5120/5999 (85%)]	Loss: 0.023949
====> Epoch: 210 Average loss: 0.000336 
Epoch: 211 [0/5999 (0%)]	Loss: 0.124492
Epoch: 211 [2560/5999 (43%)]	Loss: 0.025950
Epoch: 211 [5120/5999 (85%)]	Loss: 0.048089
====> Epoch: 211 Average loss: 0.000399 
Epoch: 212 [0/5999 (0%)]	Loss: 0.084319
Epoch: 212 [2560/5999 (43%)]	Loss: 0.041652
Epoch: 212 [5120/5999 (85%)]	Loss: 0.023110
saving model at:212,0.00014351707650348545
====> Epoch: 212 Average loss: 0.000344 
Epoch: 213 [0/5999 (0%)]	Loss: 0.021621
Epoch: 213 [2560/5999 (43%)]	Loss: 0.027922
Epoch: 213 [5120/5999 (85%)]	Loss: 0.022029
====> Epoch: 213 Average loss: 0.000373 
Epoch: 214 [0/5999 (0%)]	Loss: 0.054825
Epoch: 214 [2560/5999 (43%)]	Loss: 0.037632
Epoch: 214 [5120/5999 (85%)]	Loss: 0.057119
====> Epoch: 214 Average loss: 0.000348 
Epoch: 215 [0/5999 (0%)]	Loss: 0.060847
Epoch: 215 [2560/5999 (43%)]	Loss: 0.068253
Epoch: 215 [5120/5999 (85%)]	Loss: 0.032889
====> Epoch: 215 Average loss: 0.000513 
Epoch: 216 [0/5999 (0%)]	Loss: 0.038900
Epoch: 216 [2560/5999 (43%)]	Loss: 0.028979
Epoch: 216 [5120/5999 (85%)]	Loss: 0.042827
====> Epoch: 216 Average loss: 0.000235 
Epoch: 217 [0/5999 (0%)]	Loss: 0.025318
Epoch: 217 [2560/5999 (43%)]	Loss: 0.064259
Epoch: 217 [5120/5999 (85%)]	Loss: 0.024102
====> Epoch: 217 Average loss: 0.000283 
Epoch: 218 [0/5999 (0%)]	Loss: 0.084002
Epoch: 218 [2560/5999 (43%)]	Loss: 0.065758
Epoch: 218 [5120/5999 (85%)]	Loss: 0.034611
====> Epoch: 218 Average loss: 0.000424 
Epoch: 219 [0/5999 (0%)]	Loss: 0.032396
Epoch: 219 [2560/5999 (43%)]	Loss: 0.025153
Epoch: 219 [5120/5999 (85%)]	Loss: 0.021185
saving model at:219,0.0001343456640606746
====> Epoch: 219 Average loss: 0.000249 
Epoch: 220 [0/5999 (0%)]	Loss: 0.051423
Epoch: 220 [2560/5999 (43%)]	Loss: 0.020074
Epoch: 220 [5120/5999 (85%)]	Loss: 0.022310
====> Epoch: 220 Average loss: 0.000283 
Epoch: 221 [0/5999 (0%)]	Loss: 0.023838
Epoch: 221 [2560/5999 (43%)]	Loss: 0.035312
Epoch: 221 [5120/5999 (85%)]	Loss: 0.021239
====> Epoch: 221 Average loss: 0.000297 
Epoch: 222 [0/5999 (0%)]	Loss: 0.056863
Epoch: 222 [2560/5999 (43%)]	Loss: 0.031361
Epoch: 222 [5120/5999 (85%)]	Loss: 0.052851
====> Epoch: 222 Average loss: 0.000322 
Epoch: 223 [0/5999 (0%)]	Loss: 0.048383
Epoch: 223 [2560/5999 (43%)]	Loss: 0.084250
Epoch: 223 [5120/5999 (85%)]	Loss: 0.035757
====> Epoch: 223 Average loss: 0.000336 
Epoch: 224 [0/5999 (0%)]	Loss: 0.034865
Epoch: 224 [2560/5999 (43%)]	Loss: 0.037685
Epoch: 224 [5120/5999 (85%)]	Loss: 0.023299
saving model at:224,0.00013207529962528498
====> Epoch: 224 Average loss: 0.000232 
Epoch: 225 [0/5999 (0%)]	Loss: 0.038330
Epoch: 225 [2560/5999 (43%)]	Loss: 0.063319
Epoch: 225 [5120/5999 (85%)]	Loss: 0.053573
====> Epoch: 225 Average loss: 0.000435 
Epoch: 226 [0/5999 (0%)]	Loss: 0.046624
Epoch: 226 [2560/5999 (43%)]	Loss: 0.031408
Epoch: 226 [5120/5999 (85%)]	Loss: 0.058430
====> Epoch: 226 Average loss: 0.000300 
Epoch: 227 [0/5999 (0%)]	Loss: 0.118487
Epoch: 227 [2560/5999 (43%)]	Loss: 0.026198
Epoch: 227 [5120/5999 (85%)]	Loss: 0.026886
====> Epoch: 227 Average loss: 0.000374 
Epoch: 228 [0/5999 (0%)]	Loss: 0.062421
Epoch: 228 [2560/5999 (43%)]	Loss: 0.074532
Epoch: 228 [5120/5999 (85%)]	Loss: 0.027706
====> Epoch: 228 Average loss: 0.000315 
Epoch: 229 [0/5999 (0%)]	Loss: 0.058898
Epoch: 229 [2560/5999 (43%)]	Loss: 0.041155
Epoch: 229 [5120/5999 (85%)]	Loss: 0.040124
====> Epoch: 229 Average loss: 0.000285 
Epoch: 230 [0/5999 (0%)]	Loss: 0.031014
Epoch: 230 [2560/5999 (43%)]	Loss: 0.019881
Epoch: 230 [5120/5999 (85%)]	Loss: 0.064360
====> Epoch: 230 Average loss: 0.000266 
Epoch: 231 [0/5999 (0%)]	Loss: 0.033495
Epoch: 231 [2560/5999 (43%)]	Loss: 0.027806
Epoch: 231 [5120/5999 (85%)]	Loss: 0.032129
====> Epoch: 231 Average loss: 0.000408 
Epoch: 232 [0/5999 (0%)]	Loss: 0.076914
Epoch: 232 [2560/5999 (43%)]	Loss: 0.026572
Epoch: 232 [5120/5999 (85%)]	Loss: 0.020851
====> Epoch: 232 Average loss: 0.000284 
Epoch: 233 [0/5999 (0%)]	Loss: 0.020897
Epoch: 233 [2560/5999 (43%)]	Loss: 0.052487
Epoch: 233 [5120/5999 (85%)]	Loss: 0.031106
====> Epoch: 233 Average loss: 0.000281 
Epoch: 234 [0/5999 (0%)]	Loss: 0.019352
Epoch: 234 [2560/5999 (43%)]	Loss: 0.036432
Epoch: 234 [5120/5999 (85%)]	Loss: 0.028624
====> Epoch: 234 Average loss: 0.000346 
Epoch: 235 [0/5999 (0%)]	Loss: 0.051967
Epoch: 235 [2560/5999 (43%)]	Loss: 0.028715
saving model at:235,0.00012000103516038507
Epoch: 235 [5120/5999 (85%)]	Loss: 0.091633
====> Epoch: 235 Average loss: 0.000297 
Epoch: 236 [0/5999 (0%)]	Loss: 0.026386
Epoch: 236 [2560/5999 (43%)]	Loss: 0.022561
Epoch: 236 [5120/5999 (85%)]	Loss: 0.057189
====> Epoch: 236 Average loss: 0.000233 
Epoch: 237 [0/5999 (0%)]	Loss: 0.037704
Epoch: 237 [2560/5999 (43%)]	Loss: 0.023381
Epoch: 237 [5120/5999 (85%)]	Loss: 0.031937
====> Epoch: 237 Average loss: 0.000229 
Epoch: 238 [0/5999 (0%)]	Loss: 0.022440
Epoch: 238 [2560/5999 (43%)]	Loss: 0.023668
Epoch: 238 [5120/5999 (85%)]	Loss: 0.029034
====> Epoch: 238 Average loss: 0.000315 
Epoch: 239 [0/5999 (0%)]	Loss: 0.039116
Epoch: 239 [2560/5999 (43%)]	Loss: 0.043078
Epoch: 239 [5120/5999 (85%)]	Loss: 0.022151
====> Epoch: 239 Average loss: 0.000357 
Epoch: 240 [0/5999 (0%)]	Loss: 0.039241
Epoch: 240 [2560/5999 (43%)]	Loss: 0.025522
Epoch: 240 [5120/5999 (85%)]	Loss: 0.034954
====> Epoch: 240 Average loss: 0.000381 
Epoch: 241 [0/5999 (0%)]	Loss: 0.038659
Epoch: 241 [2560/5999 (43%)]	Loss: 0.023814
Epoch: 241 [5120/5999 (85%)]	Loss: 0.025702
====> Epoch: 241 Average loss: 0.000287 
Epoch: 242 [0/5999 (0%)]	Loss: 0.019804
Epoch: 242 [2560/5999 (43%)]	Loss: 0.018719
saving model at:242,0.00011582953023025765
Epoch: 242 [5120/5999 (85%)]	Loss: 0.041338
====> Epoch: 242 Average loss: 0.000262 
Epoch: 243 [0/5999 (0%)]	Loss: 0.021729
Epoch: 243 [2560/5999 (43%)]	Loss: 0.019088
Epoch: 243 [5120/5999 (85%)]	Loss: 0.025516
====> Epoch: 243 Average loss: 0.000307 
Epoch: 244 [0/5999 (0%)]	Loss: 0.026717
Epoch: 244 [2560/5999 (43%)]	Loss: 0.019132
Epoch: 244 [5120/5999 (85%)]	Loss: 0.030618
====> Epoch: 244 Average loss: 0.000266 
Epoch: 245 [0/5999 (0%)]	Loss: 0.018012
Epoch: 245 [2560/5999 (43%)]	Loss: 0.046565
Epoch: 245 [5120/5999 (85%)]	Loss: 0.036194
====> Epoch: 245 Average loss: 0.000270 
Epoch: 246 [0/5999 (0%)]	Loss: 0.019447
Epoch: 246 [2560/5999 (43%)]	Loss: 0.022149
Epoch: 246 [5120/5999 (85%)]	Loss: 0.042197
====> Epoch: 246 Average loss: 0.000259 
Epoch: 247 [0/5999 (0%)]	Loss: 0.037914
Epoch: 247 [2560/5999 (43%)]	Loss: 0.017124
Epoch: 247 [5120/5999 (85%)]	Loss: 0.034547
====> Epoch: 247 Average loss: 0.000271 
Epoch: 248 [0/5999 (0%)]	Loss: 0.042798
Epoch: 248 [2560/5999 (43%)]	Loss: 0.092340
Epoch: 248 [5120/5999 (85%)]	Loss: 0.041306
====> Epoch: 248 Average loss: 0.000311 
Epoch: 249 [0/5999 (0%)]	Loss: 0.028453
Epoch: 249 [2560/5999 (43%)]	Loss: 0.020361
saving model at:249,0.00010489683190826327
Epoch: 249 [5120/5999 (85%)]	Loss: 0.045982
====> Epoch: 249 Average loss: 0.000259 
Epoch: 250 [0/5999 (0%)]	Loss: 0.026838
Epoch: 250 [2560/5999 (43%)]	Loss: 0.024659
Epoch: 250 [5120/5999 (85%)]	Loss: 0.033215
====> Epoch: 250 Average loss: 0.000288 
Epoch: 251 [0/5999 (0%)]	Loss: 0.023681
Epoch: 251 [2560/5999 (43%)]	Loss: 0.018493
Epoch: 251 [5120/5999 (85%)]	Loss: 0.029546
====> Epoch: 251 Average loss: 0.000302 
Epoch: 252 [0/5999 (0%)]	Loss: 0.027979
Epoch: 252 [2560/5999 (43%)]	Loss: 0.040002
Epoch: 252 [5120/5999 (85%)]	Loss: 0.070060
====> Epoch: 252 Average loss: 0.000369 
Epoch: 253 [0/5999 (0%)]	Loss: 0.040614
Epoch: 253 [2560/5999 (43%)]	Loss: 0.030538
Epoch: 253 [5120/5999 (85%)]	Loss: 0.047604
====> Epoch: 253 Average loss: 0.000307 
Epoch: 254 [0/5999 (0%)]	Loss: 0.040321
Epoch: 254 [2560/5999 (43%)]	Loss: 0.031903
Epoch: 254 [5120/5999 (85%)]	Loss: 0.057356
====> Epoch: 254 Average loss: 0.000290 
Epoch: 255 [0/5999 (0%)]	Loss: 0.054507
Epoch: 255 [2560/5999 (43%)]	Loss: 0.025150
Epoch: 255 [5120/5999 (85%)]	Loss: 0.037661
====> Epoch: 255 Average loss: 0.000257 
Epoch: 256 [0/5999 (0%)]	Loss: 0.013680
Epoch: 256 [2560/5999 (43%)]	Loss: 0.029112
Epoch: 256 [5120/5999 (85%)]	Loss: 0.015355
saving model at:256,9.171632101060823e-05
====> Epoch: 256 Average loss: 0.000217 
Epoch: 257 [0/5999 (0%)]	Loss: 0.018750
Epoch: 257 [2560/5999 (43%)]	Loss: 0.018498
Epoch: 257 [5120/5999 (85%)]	Loss: 0.027358
====> Epoch: 257 Average loss: 0.000216 
Epoch: 258 [0/5999 (0%)]	Loss: 0.015424
Epoch: 258 [2560/5999 (43%)]	Loss: 0.023727
Epoch: 258 [5120/5999 (85%)]	Loss: 0.025525
====> Epoch: 258 Average loss: 0.000297 
Epoch: 259 [0/5999 (0%)]	Loss: 0.035364
Epoch: 259 [2560/5999 (43%)]	Loss: 0.014486
Epoch: 259 [5120/5999 (85%)]	Loss: 0.027153
====> Epoch: 259 Average loss: 0.000267 
Epoch: 260 [0/5999 (0%)]	Loss: 0.033701
Epoch: 260 [2560/5999 (43%)]	Loss: 0.020119
Epoch: 260 [5120/5999 (85%)]	Loss: 0.124880
====> Epoch: 260 Average loss: 0.000321 
Epoch: 261 [0/5999 (0%)]	Loss: 0.038709
Epoch: 261 [2560/5999 (43%)]	Loss: 0.019784
Epoch: 261 [5120/5999 (85%)]	Loss: 0.093873
====> Epoch: 261 Average loss: 0.000325 
Epoch: 262 [0/5999 (0%)]	Loss: 0.048949
Epoch: 262 [2560/5999 (43%)]	Loss: 0.036991
Epoch: 262 [5120/5999 (85%)]	Loss: 0.057458
====> Epoch: 262 Average loss: 0.000256 
Epoch: 263 [0/5999 (0%)]	Loss: 0.021069
Epoch: 263 [2560/5999 (43%)]	Loss: 0.020169
Epoch: 263 [5120/5999 (85%)]	Loss: 0.020266
====> Epoch: 263 Average loss: 0.000268 
Epoch: 264 [0/5999 (0%)]	Loss: 0.025230
Epoch: 264 [2560/5999 (43%)]	Loss: 0.033232
Epoch: 264 [5120/5999 (85%)]	Loss: 0.027918
====> Epoch: 264 Average loss: 0.000261 
Epoch: 265 [0/5999 (0%)]	Loss: 0.039178
Epoch: 265 [2560/5999 (43%)]	Loss: 0.024520
Epoch: 265 [5120/5999 (85%)]	Loss: 0.022127
====> Epoch: 265 Average loss: 0.000232 
Epoch: 266 [0/5999 (0%)]	Loss: 0.019441
Epoch: 266 [2560/5999 (43%)]	Loss: 0.048313
Epoch: 266 [5120/5999 (85%)]	Loss: 0.016982
====> Epoch: 266 Average loss: 0.000235 
Epoch: 267 [0/5999 (0%)]	Loss: 0.051196
Epoch: 267 [2560/5999 (43%)]	Loss: 0.015665
Epoch: 267 [5120/5999 (85%)]	Loss: 0.034176
====> Epoch: 267 Average loss: 0.000375 
Epoch: 268 [0/5999 (0%)]	Loss: 0.030679
Epoch: 268 [2560/5999 (43%)]	Loss: 0.036543
Epoch: 268 [5120/5999 (85%)]	Loss: 0.032016
====> Epoch: 268 Average loss: 0.000274 
Epoch: 269 [0/5999 (0%)]	Loss: 0.024569
Epoch: 269 [2560/5999 (43%)]	Loss: 0.018632
Epoch: 269 [5120/5999 (85%)]	Loss: 0.018656
====> Epoch: 269 Average loss: 0.000196 
Epoch: 270 [0/5999 (0%)]	Loss: 0.017503
Epoch: 270 [2560/5999 (43%)]	Loss: 0.033347
Epoch: 270 [5120/5999 (85%)]	Loss: 0.032397
====> Epoch: 270 Average loss: 0.000195 
Epoch: 271 [0/5999 (0%)]	Loss: 0.016376
Epoch: 271 [2560/5999 (43%)]	Loss: 0.034919
Epoch: 271 [5120/5999 (85%)]	Loss: 0.029981
====> Epoch: 271 Average loss: 0.000230 
Epoch: 272 [0/5999 (0%)]	Loss: 0.028473
Epoch: 272 [2560/5999 (43%)]	Loss: 0.038922
Epoch: 272 [5120/5999 (85%)]	Loss: 0.021613
====> Epoch: 272 Average loss: 0.000246 
Epoch: 273 [0/5999 (0%)]	Loss: 0.025242
Epoch: 273 [2560/5999 (43%)]	Loss: 0.013455
Epoch: 273 [5120/5999 (85%)]	Loss: 0.040891
====> Epoch: 273 Average loss: 0.000338 
Epoch: 274 [0/5999 (0%)]	Loss: 0.035124
Epoch: 274 [2560/5999 (43%)]	Loss: 0.044510
Epoch: 274 [5120/5999 (85%)]	Loss: 0.026350
====> Epoch: 274 Average loss: 0.000356 
Epoch: 275 [0/5999 (0%)]	Loss: 0.051076
Epoch: 275 [2560/5999 (43%)]	Loss: 0.017035
Epoch: 275 [5120/5999 (85%)]	Loss: 0.017070
====> Epoch: 275 Average loss: 0.000237 
Epoch: 276 [0/5999 (0%)]	Loss: 0.027825
Epoch: 276 [2560/5999 (43%)]	Loss: 0.023031
Epoch: 276 [5120/5999 (85%)]	Loss: 0.014735
====> Epoch: 276 Average loss: 0.000288 
Epoch: 277 [0/5999 (0%)]	Loss: 0.033008
Epoch: 277 [2560/5999 (43%)]	Loss: 0.019486
Epoch: 277 [5120/5999 (85%)]	Loss: 0.050228
====> Epoch: 277 Average loss: 0.000283 
Epoch: 278 [0/5999 (0%)]	Loss: 0.134352
Epoch: 278 [2560/5999 (43%)]	Loss: 0.145365
Epoch: 278 [5120/5999 (85%)]	Loss: 0.026849
====> Epoch: 278 Average loss: 0.000345 
Epoch: 279 [0/5999 (0%)]	Loss: 0.032034
Epoch: 279 [2560/5999 (43%)]	Loss: 0.022366
Epoch: 279 [5120/5999 (85%)]	Loss: 0.020855
====> Epoch: 279 Average loss: 0.000212 
Epoch: 280 [0/5999 (0%)]	Loss: 0.035364
Epoch: 280 [2560/5999 (43%)]	Loss: 0.027457
Epoch: 280 [5120/5999 (85%)]	Loss: 0.028551
====> Epoch: 280 Average loss: 0.000312 
Epoch: 281 [0/5999 (0%)]	Loss: 0.023903
Epoch: 281 [2560/5999 (43%)]	Loss: 0.022416
Epoch: 281 [5120/5999 (85%)]	Loss: 0.012637
====> Epoch: 281 Average loss: 0.000273 
Epoch: 282 [0/5999 (0%)]	Loss: 0.064326
Epoch: 282 [2560/5999 (43%)]	Loss: 0.046508
Epoch: 282 [5120/5999 (85%)]	Loss: 0.034422
====> Epoch: 282 Average loss: 0.000373 
Epoch: 283 [0/5999 (0%)]	Loss: 0.058243
Epoch: 283 [2560/5999 (43%)]	Loss: 0.023807
Epoch: 283 [5120/5999 (85%)]	Loss: 0.048211
====> Epoch: 283 Average loss: 0.000296 
Epoch: 284 [0/5999 (0%)]	Loss: 0.014732
Epoch: 284 [2560/5999 (43%)]	Loss: 0.048052
Epoch: 284 [5120/5999 (85%)]	Loss: 0.024782
====> Epoch: 284 Average loss: 0.000204 
Epoch: 285 [0/5999 (0%)]	Loss: 0.032009
Epoch: 285 [2560/5999 (43%)]	Loss: 0.025835
Epoch: 285 [5120/5999 (85%)]	Loss: 0.019563
====> Epoch: 285 Average loss: 0.000309 
Epoch: 286 [0/5999 (0%)]	Loss: 0.021582
Epoch: 286 [2560/5999 (43%)]	Loss: 0.065927
Epoch: 286 [5120/5999 (85%)]	Loss: 0.019871
saving model at:286,8.338222798192874e-05
====> Epoch: 286 Average loss: 0.000250 
Epoch: 287 [0/5999 (0%)]	Loss: 0.026800
Epoch: 287 [2560/5999 (43%)]	Loss: 0.014662
Epoch: 287 [5120/5999 (85%)]	Loss: 0.118262
====> Epoch: 287 Average loss: 0.000270 
Epoch: 288 [0/5999 (0%)]	Loss: 0.050976
Epoch: 288 [2560/5999 (43%)]	Loss: 0.039574
Epoch: 288 [5120/5999 (85%)]	Loss: 0.014945
====> Epoch: 288 Average loss: 0.000217 
Epoch: 289 [0/5999 (0%)]	Loss: 0.025041
Epoch: 289 [2560/5999 (43%)]	Loss: 0.014867
Epoch: 289 [5120/5999 (85%)]	Loss: 0.062709
====> Epoch: 289 Average loss: 0.000213 
Epoch: 290 [0/5999 (0%)]	Loss: 0.027371
Epoch: 290 [2560/5999 (43%)]	Loss: 0.050486
Epoch: 290 [5120/5999 (85%)]	Loss: 0.020753
====> Epoch: 290 Average loss: 0.000334 
Epoch: 291 [0/5999 (0%)]	Loss: 0.023587
Epoch: 291 [2560/5999 (43%)]	Loss: 0.062707
Epoch: 291 [5120/5999 (85%)]	Loss: 0.047248
====> Epoch: 291 Average loss: 0.000337 
Epoch: 292 [0/5999 (0%)]	Loss: 0.022349
Epoch: 292 [2560/5999 (43%)]	Loss: 0.024657
Epoch: 292 [5120/5999 (85%)]	Loss: 0.015266
====> Epoch: 292 Average loss: 0.000273 
Epoch: 293 [0/5999 (0%)]	Loss: 0.031759
Epoch: 293 [2560/5999 (43%)]	Loss: 0.048628
Epoch: 293 [5120/5999 (85%)]	Loss: 0.012122
====> Epoch: 293 Average loss: 0.000293 
Epoch: 294 [0/5999 (0%)]	Loss: 0.044315
Epoch: 294 [2560/5999 (43%)]	Loss: 0.024984
Epoch: 294 [5120/5999 (85%)]	Loss: 0.052904
====> Epoch: 294 Average loss: 0.000232 
Epoch: 295 [0/5999 (0%)]	Loss: 0.028911
Epoch: 295 [2560/5999 (43%)]	Loss: 0.030021
Epoch: 295 [5120/5999 (85%)]	Loss: 0.060463
====> Epoch: 295 Average loss: 0.000411 
Epoch: 296 [0/5999 (0%)]	Loss: 0.104031
Epoch: 296 [2560/5999 (43%)]	Loss: 0.018792
Epoch: 296 [5120/5999 (85%)]	Loss: 0.022121
====> Epoch: 296 Average loss: 0.000286 
Epoch: 297 [0/5999 (0%)]	Loss: 0.016070
Epoch: 297 [2560/5999 (43%)]	Loss: 0.012567
Epoch: 297 [5120/5999 (85%)]	Loss: 0.050675
====> Epoch: 297 Average loss: 0.000193 
Epoch: 298 [0/5999 (0%)]	Loss: 0.031201
Epoch: 298 [2560/5999 (43%)]	Loss: 0.019016
Epoch: 298 [5120/5999 (85%)]	Loss: 0.019182
====> Epoch: 298 Average loss: 0.000249 
Epoch: 299 [0/5999 (0%)]	Loss: 0.039315
Epoch: 299 [2560/5999 (43%)]	Loss: 0.025417
Epoch: 299 [5120/5999 (85%)]	Loss: 0.073486
====> Epoch: 299 Average loss: 0.000252 
Epoch: 300 [0/5999 (0%)]	Loss: 0.017773
Epoch: 300 [2560/5999 (43%)]	Loss: 0.015112
Epoch: 300 [5120/5999 (85%)]	Loss: 0.030767
====> Epoch: 300 Average loss: 0.000236 
Epoch: 301 [0/5999 (0%)]	Loss: 0.037160
Epoch: 301 [2560/5999 (43%)]	Loss: 0.056237
Epoch: 301 [5120/5999 (85%)]	Loss: 0.030105
====> Epoch: 301 Average loss: 0.000328 
Epoch: 302 [0/5999 (0%)]	Loss: 0.054079
Epoch: 302 [2560/5999 (43%)]	Loss: 0.030097
Epoch: 302 [5120/5999 (85%)]	Loss: 0.023863
====> Epoch: 302 Average loss: 0.000302 
Epoch: 303 [0/5999 (0%)]	Loss: 0.025876
Epoch: 303 [2560/5999 (43%)]	Loss: 0.015310
Epoch: 303 [5120/5999 (85%)]	Loss: 0.024477
====> Epoch: 303 Average loss: 0.000282 
Epoch: 304 [0/5999 (0%)]	Loss: 0.028800
Epoch: 304 [2560/5999 (43%)]	Loss: 0.010840
Epoch: 304 [5120/5999 (85%)]	Loss: 0.040531
====> Epoch: 304 Average loss: 0.000239 
Epoch: 305 [0/5999 (0%)]	Loss: 0.017108
Epoch: 305 [2560/5999 (43%)]	Loss: 0.016071
Epoch: 305 [5120/5999 (85%)]	Loss: 0.019658
====> Epoch: 305 Average loss: 0.000204 
Epoch: 306 [0/5999 (0%)]	Loss: 0.022587
Epoch: 306 [2560/5999 (43%)]	Loss: 0.030093
Epoch: 306 [5120/5999 (85%)]	Loss: 0.033867
====> Epoch: 306 Average loss: 0.000214 
Epoch: 307 [0/5999 (0%)]	Loss: 0.023274
Epoch: 307 [2560/5999 (43%)]	Loss: 0.072849
Epoch: 307 [5120/5999 (85%)]	Loss: 0.019475
====> Epoch: 307 Average loss: 0.000248 
Epoch: 308 [0/5999 (0%)]	Loss: 0.026507
Epoch: 308 [2560/5999 (43%)]	Loss: 0.015560
saving model at:308,7.692421914543957e-05
Epoch: 308 [5120/5999 (85%)]	Loss: 0.029256
====> Epoch: 308 Average loss: 0.000257 
Epoch: 309 [0/5999 (0%)]	Loss: 0.017466
Epoch: 309 [2560/5999 (43%)]	Loss: 0.039654
Epoch: 309 [5120/5999 (85%)]	Loss: 0.051344
====> Epoch: 309 Average loss: 0.000201 
Epoch: 310 [0/5999 (0%)]	Loss: 0.017066
Epoch: 310 [2560/5999 (43%)]	Loss: 0.017033
Epoch: 310 [5120/5999 (85%)]	Loss: 0.020379
saving model at:310,7.154325098963454e-05
====> Epoch: 310 Average loss: 0.000213 
Epoch: 311 [0/5999 (0%)]	Loss: 0.099562
Epoch: 311 [2560/5999 (43%)]	Loss: 0.049324
Epoch: 311 [5120/5999 (85%)]	Loss: 0.032120
====> Epoch: 311 Average loss: 0.000297 
Epoch: 312 [0/5999 (0%)]	Loss: 0.015467
Epoch: 312 [2560/5999 (43%)]	Loss: 0.013345
Epoch: 312 [5120/5999 (85%)]	Loss: 0.015139
====> Epoch: 312 Average loss: 0.000192 
Epoch: 313 [0/5999 (0%)]	Loss: 0.015801
Epoch: 313 [2560/5999 (43%)]	Loss: 0.031742
Epoch: 313 [5120/5999 (85%)]	Loss: 0.045847
====> Epoch: 313 Average loss: 0.000219 
Epoch: 314 [0/5999 (0%)]	Loss: 0.013072
Epoch: 314 [2560/5999 (43%)]	Loss: 0.025253
Epoch: 314 [5120/5999 (85%)]	Loss: 0.037387
====> Epoch: 314 Average loss: 0.000196 
Epoch: 315 [0/5999 (0%)]	Loss: 0.036084
Epoch: 315 [2560/5999 (43%)]	Loss: 0.013900
Epoch: 315 [5120/5999 (85%)]	Loss: 0.039777
====> Epoch: 315 Average loss: 0.000196 
Epoch: 316 [0/5999 (0%)]	Loss: 0.020763
Epoch: 316 [2560/5999 (43%)]	Loss: 0.013270
Epoch: 316 [5120/5999 (85%)]	Loss: 0.016527
====> Epoch: 316 Average loss: 0.000181 
Epoch: 317 [0/5999 (0%)]	Loss: 0.035639
Epoch: 317 [2560/5999 (43%)]	Loss: 0.029517
Epoch: 317 [5120/5999 (85%)]	Loss: 0.058977
====> Epoch: 317 Average loss: 0.000318 
Epoch: 318 [0/5999 (0%)]	Loss: 0.042463
Epoch: 318 [2560/5999 (43%)]	Loss: 0.011359
Epoch: 318 [5120/5999 (85%)]	Loss: 0.018084
====> Epoch: 318 Average loss: 0.000258 
Epoch: 319 [0/5999 (0%)]	Loss: 0.033642
Epoch: 319 [2560/5999 (43%)]	Loss: 0.019548
Epoch: 319 [5120/5999 (85%)]	Loss: 0.035666
====> Epoch: 319 Average loss: 0.000202 
Epoch: 320 [0/5999 (0%)]	Loss: 0.014192
Epoch: 320 [2560/5999 (43%)]	Loss: 0.013204
Epoch: 320 [5120/5999 (85%)]	Loss: 0.032743
====> Epoch: 320 Average loss: 0.000220 
Epoch: 321 [0/5999 (0%)]	Loss: 0.023051
Epoch: 321 [2560/5999 (43%)]	Loss: 0.022992
Epoch: 321 [5120/5999 (85%)]	Loss: 0.015622
====> Epoch: 321 Average loss: 0.000223 
Epoch: 322 [0/5999 (0%)]	Loss: 0.016497
Epoch: 322 [2560/5999 (43%)]	Loss: 0.020356
Epoch: 322 [5120/5999 (85%)]	Loss: 0.014736
====> Epoch: 322 Average loss: 0.000251 
Epoch: 323 [0/5999 (0%)]	Loss: 0.025927
Epoch: 323 [2560/5999 (43%)]	Loss: 0.013180
Epoch: 323 [5120/5999 (85%)]	Loss: 0.036814
====> Epoch: 323 Average loss: 0.000198 
Epoch: 324 [0/5999 (0%)]	Loss: 0.009308
Epoch: 324 [2560/5999 (43%)]	Loss: 0.021538
Epoch: 324 [5120/5999 (85%)]	Loss: 0.013541
====> Epoch: 324 Average loss: 0.000220 
Epoch: 325 [0/5999 (0%)]	Loss: 0.022502
Epoch: 325 [2560/5999 (43%)]	Loss: 0.019959
Epoch: 325 [5120/5999 (85%)]	Loss: 0.021974
====> Epoch: 325 Average loss: 0.000265 
Epoch: 326 [0/5999 (0%)]	Loss: 0.026866
Epoch: 326 [2560/5999 (43%)]	Loss: 0.055624
Epoch: 326 [5120/5999 (85%)]	Loss: 0.052724
====> Epoch: 326 Average loss: 0.000340 
Epoch: 327 [0/5999 (0%)]	Loss: 0.053237
Epoch: 327 [2560/5999 (43%)]	Loss: 0.025328
Epoch: 327 [5120/5999 (85%)]	Loss: 0.015697
====> Epoch: 327 Average loss: 0.000231 
Epoch: 328 [0/5999 (0%)]	Loss: 0.012453
Epoch: 328 [2560/5999 (43%)]	Loss: 0.016136
Epoch: 328 [5120/5999 (85%)]	Loss: 0.036270
====> Epoch: 328 Average loss: 0.000236 
Epoch: 329 [0/5999 (0%)]	Loss: 0.076102
Epoch: 329 [2560/5999 (43%)]	Loss: 0.044448
Epoch: 329 [5120/5999 (85%)]	Loss: 0.017046
====> Epoch: 329 Average loss: 0.000350 
Epoch: 330 [0/5999 (0%)]	Loss: 0.021449
Epoch: 330 [2560/5999 (43%)]	Loss: 0.013701
Epoch: 330 [5120/5999 (85%)]	Loss: 0.066142
====> Epoch: 330 Average loss: 0.000234 
Epoch: 331 [0/5999 (0%)]	Loss: 0.015171
Epoch: 331 [2560/5999 (43%)]	Loss: 0.021718
Epoch: 331 [5120/5999 (85%)]	Loss: 0.055348
====> Epoch: 331 Average loss: 0.000175 
Epoch: 332 [0/5999 (0%)]	Loss: 0.015554
Epoch: 332 [2560/5999 (43%)]	Loss: 0.012689
Epoch: 332 [5120/5999 (85%)]	Loss: 0.040618
====> Epoch: 332 Average loss: 0.000279 
Epoch: 333 [0/5999 (0%)]	Loss: 0.065288
Epoch: 333 [2560/5999 (43%)]	Loss: 0.019812
Epoch: 333 [5120/5999 (85%)]	Loss: 0.047147
====> Epoch: 333 Average loss: 0.000239 
Epoch: 334 [0/5999 (0%)]	Loss: 0.072154
Epoch: 334 [2560/5999 (43%)]	Loss: 0.011554
Epoch: 334 [5120/5999 (85%)]	Loss: 0.047847
====> Epoch: 334 Average loss: 0.000223 
Epoch: 335 [0/5999 (0%)]	Loss: 0.069707
Epoch: 335 [2560/5999 (43%)]	Loss: 0.058656
Epoch: 335 [5120/5999 (85%)]	Loss: 0.041401
====> Epoch: 335 Average loss: 0.000423 
Epoch: 336 [0/5999 (0%)]	Loss: 0.028108
Epoch: 336 [2560/5999 (43%)]	Loss: 0.019713
Epoch: 336 [5120/5999 (85%)]	Loss: 0.014007
====> Epoch: 336 Average loss: 0.000253 
Epoch: 337 [0/5999 (0%)]	Loss: 0.026217
Epoch: 337 [2560/5999 (43%)]	Loss: 0.019683
Epoch: 337 [5120/5999 (85%)]	Loss: 0.036467
====> Epoch: 337 Average loss: 0.000210 
Epoch: 338 [0/5999 (0%)]	Loss: 0.024866
Epoch: 338 [2560/5999 (43%)]	Loss: 0.032963
Epoch: 338 [5120/5999 (85%)]	Loss: 0.038379
====> Epoch: 338 Average loss: 0.000243 
Epoch: 339 [0/5999 (0%)]	Loss: 0.044764
Epoch: 339 [2560/5999 (43%)]	Loss: 0.014259
Epoch: 339 [5120/5999 (85%)]	Loss: 0.022459
====> Epoch: 339 Average loss: 0.000270 
Epoch: 340 [0/5999 (0%)]	Loss: 0.031254
Epoch: 340 [2560/5999 (43%)]	Loss: 0.032162
Epoch: 340 [5120/5999 (85%)]	Loss: 0.023168
====> Epoch: 340 Average loss: 0.000222 
Epoch: 341 [0/5999 (0%)]	Loss: 0.041286
Epoch: 341 [2560/5999 (43%)]	Loss: 0.032674
Epoch: 341 [5120/5999 (85%)]	Loss: 0.020653
====> Epoch: 341 Average loss: 0.000190 
Epoch: 342 [0/5999 (0%)]	Loss: 0.063515
Epoch: 342 [2560/5999 (43%)]	Loss: 0.015143
Epoch: 342 [5120/5999 (85%)]	Loss: 0.012843
====> Epoch: 342 Average loss: 0.000174 
Epoch: 343 [0/5999 (0%)]	Loss: 0.043593
Epoch: 343 [2560/5999 (43%)]	Loss: 0.034724
Epoch: 343 [5120/5999 (85%)]	Loss: 0.022850
====> Epoch: 343 Average loss: 0.000346 
Epoch: 344 [0/5999 (0%)]	Loss: 0.047029
Epoch: 344 [2560/5999 (43%)]	Loss: 0.028111
Epoch: 344 [5120/5999 (85%)]	Loss: 0.017679
====> Epoch: 344 Average loss: 0.000217 
Epoch: 345 [0/5999 (0%)]	Loss: 0.015463
Epoch: 345 [2560/5999 (43%)]	Loss: 0.013361
Epoch: 345 [5120/5999 (85%)]	Loss: 0.044495
====> Epoch: 345 Average loss: 0.000195 
Epoch: 346 [0/5999 (0%)]	Loss: 0.073420
Epoch: 346 [2560/5999 (43%)]	Loss: 0.054455
Epoch: 346 [5120/5999 (85%)]	Loss: 0.053230
====> Epoch: 346 Average loss: 0.000220 
Epoch: 347 [0/5999 (0%)]	Loss: 0.026410
Epoch: 347 [2560/5999 (43%)]	Loss: 0.101595
Epoch: 347 [5120/5999 (85%)]	Loss: 0.035504
====> Epoch: 347 Average loss: 0.000269 
Epoch: 348 [0/5999 (0%)]	Loss: 0.015113
Epoch: 348 [2560/5999 (43%)]	Loss: 0.016514
Epoch: 348 [5120/5999 (85%)]	Loss: 0.027816
====> Epoch: 348 Average loss: 0.000248 
Epoch: 349 [0/5999 (0%)]	Loss: 0.020129
Epoch: 349 [2560/5999 (43%)]	Loss: 0.031731
Epoch: 349 [5120/5999 (85%)]	Loss: 0.020180
====> Epoch: 349 Average loss: 0.000210 
Epoch: 350 [0/5999 (0%)]	Loss: 0.016592
Epoch: 350 [2560/5999 (43%)]	Loss: 0.016964
Epoch: 350 [5120/5999 (85%)]	Loss: 0.028190
====> Epoch: 350 Average loss: 0.000245 
Epoch: 351 [0/5999 (0%)]	Loss: 0.051651
Epoch: 351 [2560/5999 (43%)]	Loss: 0.011430
saving model at:351,6.743721407838166e-05
Epoch: 351 [5120/5999 (85%)]	Loss: 0.022720
====> Epoch: 351 Average loss: 0.000182 
Epoch: 352 [0/5999 (0%)]	Loss: 0.017835
Epoch: 352 [2560/5999 (43%)]	Loss: 0.016614
Epoch: 352 [5120/5999 (85%)]	Loss: 0.035447
====> Epoch: 352 Average loss: 0.000212 
Epoch: 353 [0/5999 (0%)]	Loss: 0.013954
Epoch: 353 [2560/5999 (43%)]	Loss: 0.010274
saving model at:353,6.45451968885027e-05
Epoch: 353 [5120/5999 (85%)]	Loss: 0.026818
====> Epoch: 353 Average loss: 0.000195 
Epoch: 354 [0/5999 (0%)]	Loss: 0.039565
Epoch: 354 [2560/5999 (43%)]	Loss: 0.018746
Epoch: 354 [5120/5999 (85%)]	Loss: 0.010644
====> Epoch: 354 Average loss: 0.000202 
Epoch: 355 [0/5999 (0%)]	Loss: 0.045431
Epoch: 355 [2560/5999 (43%)]	Loss: 0.041136
Epoch: 355 [5120/5999 (85%)]	Loss: 0.072012
====> Epoch: 355 Average loss: 0.000236 
Epoch: 356 [0/5999 (0%)]	Loss: 0.022073
Epoch: 356 [2560/5999 (43%)]	Loss: 0.025757
Epoch: 356 [5120/5999 (85%)]	Loss: 0.019990
====> Epoch: 356 Average loss: 0.000234 
Epoch: 357 [0/5999 (0%)]	Loss: 0.018916
Epoch: 357 [2560/5999 (43%)]	Loss: 0.016866
Epoch: 357 [5120/5999 (85%)]	Loss: 0.051160
====> Epoch: 357 Average loss: 0.000229 
Epoch: 358 [0/5999 (0%)]	Loss: 0.016745
Epoch: 358 [2560/5999 (43%)]	Loss: 0.023470
Epoch: 358 [5120/5999 (85%)]	Loss: 0.017038
====> Epoch: 358 Average loss: 0.000168 
Epoch: 359 [0/5999 (0%)]	Loss: 0.017396
Epoch: 359 [2560/5999 (43%)]	Loss: 0.014726
Epoch: 359 [5120/5999 (85%)]	Loss: 0.009447
====> Epoch: 359 Average loss: 0.000151 
Epoch: 360 [0/5999 (0%)]	Loss: 0.039433
Epoch: 360 [2560/5999 (43%)]	Loss: 0.055496
Epoch: 360 [5120/5999 (85%)]	Loss: 0.034306
====> Epoch: 360 Average loss: 0.000276 
Epoch: 361 [0/5999 (0%)]	Loss: 0.026226
Epoch: 361 [2560/5999 (43%)]	Loss: 0.031626
Epoch: 361 [5120/5999 (85%)]	Loss: 0.052509
====> Epoch: 361 Average loss: 0.000219 
Epoch: 362 [0/5999 (0%)]	Loss: 0.086280
Epoch: 362 [2560/5999 (43%)]	Loss: 0.017767
Epoch: 362 [5120/5999 (85%)]	Loss: 0.060325
====> Epoch: 362 Average loss: 0.000282 
Epoch: 363 [0/5999 (0%)]	Loss: 0.033610
Epoch: 363 [2560/5999 (43%)]	Loss: 0.026549
Epoch: 363 [5120/5999 (85%)]	Loss: 0.017579
====> Epoch: 363 Average loss: 0.000242 
Epoch: 364 [0/5999 (0%)]	Loss: 0.016037
Epoch: 364 [2560/5999 (43%)]	Loss: 0.013631
Epoch: 364 [5120/5999 (85%)]	Loss: 0.018033
====> Epoch: 364 Average loss: 0.000200 
Epoch: 365 [0/5999 (0%)]	Loss: 0.053944
Epoch: 365 [2560/5999 (43%)]	Loss: 0.016257
Epoch: 365 [5120/5999 (85%)]	Loss: 0.017366
====> Epoch: 365 Average loss: 0.000211 
Epoch: 366 [0/5999 (0%)]	Loss: 0.017427
Epoch: 366 [2560/5999 (43%)]	Loss: 0.020133
Epoch: 366 [5120/5999 (85%)]	Loss: 0.013267
saving model at:366,6.122775829862803e-05
====> Epoch: 366 Average loss: 0.000181 
Epoch: 367 [0/5999 (0%)]	Loss: 0.027248
Epoch: 367 [2560/5999 (43%)]	Loss: 0.024348
Epoch: 367 [5120/5999 (85%)]	Loss: 0.033356
====> Epoch: 367 Average loss: 0.000242 
Epoch: 368 [0/5999 (0%)]	Loss: 0.121476
Epoch: 368 [2560/5999 (43%)]	Loss: 0.044734
Epoch: 368 [5120/5999 (85%)]	Loss: 0.066392
====> Epoch: 368 Average loss: 0.000356 
Epoch: 369 [0/5999 (0%)]	Loss: 0.023792
Epoch: 369 [2560/5999 (43%)]	Loss: 0.018263
Epoch: 369 [5120/5999 (85%)]	Loss: 0.012646
====> Epoch: 369 Average loss: 0.000219 
Epoch: 370 [0/5999 (0%)]	Loss: 0.023851
Epoch: 370 [2560/5999 (43%)]	Loss: 0.034128
Epoch: 370 [5120/5999 (85%)]	Loss: 0.037148
====> Epoch: 370 Average loss: 0.000254 
Epoch: 371 [0/5999 (0%)]	Loss: 0.046632
Epoch: 371 [2560/5999 (43%)]	Loss: 0.054158
Epoch: 371 [5120/5999 (85%)]	Loss: 0.068329
====> Epoch: 371 Average loss: 0.000387 
Epoch: 372 [0/5999 (0%)]	Loss: 0.050538
Epoch: 372 [2560/5999 (43%)]	Loss: 0.018582
Epoch: 372 [5120/5999 (85%)]	Loss: 0.010591
====> Epoch: 372 Average loss: 0.000216 
Epoch: 373 [0/5999 (0%)]	Loss: 0.021308
Epoch: 373 [2560/5999 (43%)]	Loss: 0.020888
Epoch: 373 [5120/5999 (85%)]	Loss: 0.018614
====> Epoch: 373 Average loss: 0.000203 
Epoch: 374 [0/5999 (0%)]	Loss: 0.009559
Epoch: 374 [2560/5999 (43%)]	Loss: 0.016695
Epoch: 374 [5120/5999 (85%)]	Loss: 0.020654
====> Epoch: 374 Average loss: 0.000276 
Epoch: 375 [0/5999 (0%)]	Loss: 0.072921
Epoch: 375 [2560/5999 (43%)]	Loss: 0.018257
Epoch: 375 [5120/5999 (85%)]	Loss: 0.011068
====> Epoch: 375 Average loss: 0.000211 
Epoch: 376 [0/5999 (0%)]	Loss: 0.018510
Epoch: 376 [2560/5999 (43%)]	Loss: 0.014919
Epoch: 376 [5120/5999 (85%)]	Loss: 0.078267
====> Epoch: 376 Average loss: 0.000168 
Epoch: 377 [0/5999 (0%)]	Loss: 0.013371
Epoch: 377 [2560/5999 (43%)]	Loss: 0.021682
Epoch: 377 [5120/5999 (85%)]	Loss: 0.018398
====> Epoch: 377 Average loss: 0.000194 
Epoch: 378 [0/5999 (0%)]	Loss: 0.060996
Epoch: 378 [2560/5999 (43%)]	Loss: 0.013104
Epoch: 378 [5120/5999 (85%)]	Loss: 0.035355
====> Epoch: 378 Average loss: 0.000205 
Epoch: 379 [0/5999 (0%)]	Loss: 0.036088
Epoch: 379 [2560/5999 (43%)]	Loss: 0.016826
Epoch: 379 [5120/5999 (85%)]	Loss: 0.034810
====> Epoch: 379 Average loss: 0.000205 
Epoch: 380 [0/5999 (0%)]	Loss: 0.027862
Epoch: 380 [2560/5999 (43%)]	Loss: 0.023217
Epoch: 380 [5120/5999 (85%)]	Loss: 0.038648
====> Epoch: 380 Average loss: 0.000243 
Epoch: 381 [0/5999 (0%)]	Loss: 0.058463
Epoch: 381 [2560/5999 (43%)]	Loss: 0.074274
Epoch: 381 [5120/5999 (85%)]	Loss: 0.016327
====> Epoch: 381 Average loss: 0.000242 
Epoch: 382 [0/5999 (0%)]	Loss: 0.021374
Epoch: 382 [2560/5999 (43%)]	Loss: 0.022110
Epoch: 382 [5120/5999 (85%)]	Loss: 0.020348
====> Epoch: 382 Average loss: 0.000206 
Epoch: 383 [0/5999 (0%)]	Loss: 0.019159
Epoch: 383 [2560/5999 (43%)]	Loss: 0.126294
Epoch: 383 [5120/5999 (85%)]	Loss: 0.021149
====> Epoch: 383 Average loss: 0.000220 
Epoch: 384 [0/5999 (0%)]	Loss: 0.036459
Epoch: 384 [2560/5999 (43%)]	Loss: 0.024155
Epoch: 384 [5120/5999 (85%)]	Loss: 0.045898
====> Epoch: 384 Average loss: 0.000255 
Epoch: 385 [0/5999 (0%)]	Loss: 0.013229
Epoch: 385 [2560/5999 (43%)]	Loss: 0.020764
Epoch: 385 [5120/5999 (85%)]	Loss: 0.034871
====> Epoch: 385 Average loss: 0.000206 
Epoch: 386 [0/5999 (0%)]	Loss: 0.018023
Epoch: 386 [2560/5999 (43%)]	Loss: 0.030310
Epoch: 386 [5120/5999 (85%)]	Loss: 0.014327
====> Epoch: 386 Average loss: 0.000223 
Epoch: 387 [0/5999 (0%)]	Loss: 0.019509
Epoch: 387 [2560/5999 (43%)]	Loss: 0.023882
Epoch: 387 [5120/5999 (85%)]	Loss: 0.024445
====> Epoch: 387 Average loss: 0.000302 
Epoch: 388 [0/5999 (0%)]	Loss: 0.041614
Epoch: 388 [2560/5999 (43%)]	Loss: 0.015758
Epoch: 388 [5120/5999 (85%)]	Loss: 0.024712
====> Epoch: 388 Average loss: 0.000223 
Epoch: 389 [0/5999 (0%)]	Loss: 0.047257
Epoch: 389 [2560/5999 (43%)]	Loss: 0.017484
Epoch: 389 [5120/5999 (85%)]	Loss: 0.013432
====> Epoch: 389 Average loss: 0.000210 
Epoch: 390 [0/5999 (0%)]	Loss: 0.011839
Epoch: 390 [2560/5999 (43%)]	Loss: 0.056601
Epoch: 390 [5120/5999 (85%)]	Loss: 0.019323
====> Epoch: 390 Average loss: 0.000196 
Epoch: 391 [0/5999 (0%)]	Loss: 0.019769
Epoch: 391 [2560/5999 (43%)]	Loss: 0.018144
Epoch: 391 [5120/5999 (85%)]	Loss: 0.027874
====> Epoch: 391 Average loss: 0.000203 
Epoch: 392 [0/5999 (0%)]	Loss: 0.042918
Epoch: 392 [2560/5999 (43%)]	Loss: 0.017485
Epoch: 392 [5120/5999 (85%)]	Loss: 0.023300
====> Epoch: 392 Average loss: 0.000183 
Epoch: 393 [0/5999 (0%)]	Loss: 0.021064
Epoch: 393 [2560/5999 (43%)]	Loss: 0.007731
Epoch: 393 [5120/5999 (85%)]	Loss: 0.060491
====> Epoch: 393 Average loss: 0.000188 
Epoch: 394 [0/5999 (0%)]	Loss: 0.019565
Epoch: 394 [2560/5999 (43%)]	Loss: 0.015731
Epoch: 394 [5120/5999 (85%)]	Loss: 0.014464
====> Epoch: 394 Average loss: 0.000197 
Epoch: 395 [0/5999 (0%)]	Loss: 0.021991
Epoch: 395 [2560/5999 (43%)]	Loss: 0.036733
Epoch: 395 [5120/5999 (85%)]	Loss: 0.014738
saving model at:395,5.915224921773188e-05
====> Epoch: 395 Average loss: 0.000171 
Epoch: 396 [0/5999 (0%)]	Loss: 0.033235
Epoch: 396 [2560/5999 (43%)]	Loss: 0.017291
Epoch: 396 [5120/5999 (85%)]	Loss: 0.013653
====> Epoch: 396 Average loss: 0.000187 
Epoch: 397 [0/5999 (0%)]	Loss: 0.010672
Epoch: 397 [2560/5999 (43%)]	Loss: 0.035882
Epoch: 397 [5120/5999 (85%)]	Loss: 0.022578
====> Epoch: 397 Average loss: 0.000208 
Epoch: 398 [0/5999 (0%)]	Loss: 0.033352
Epoch: 398 [2560/5999 (43%)]	Loss: 0.014264
Epoch: 398 [5120/5999 (85%)]	Loss: 0.009858
====> Epoch: 398 Average loss: 0.000198 
Epoch: 399 [0/5999 (0%)]	Loss: 0.017641
Epoch: 399 [2560/5999 (43%)]	Loss: 0.027315
Epoch: 399 [5120/5999 (85%)]	Loss: 0.021246
====> Epoch: 399 Average loss: 0.000214 
Epoch: 400 [0/5999 (0%)]	Loss: 0.040518
Epoch: 400 [2560/5999 (43%)]	Loss: 0.012825
Epoch: 400 [5120/5999 (85%)]	Loss: 0.024397
====> Epoch: 400 Average loss: 0.000180 
Reconstruction Loss 5.9152250556508076e-05
per_obj_mse: ['7.125753472791985e-05', '4.7046974941622466e-05']
Reconstruction Loss 5.631027220399643e-05
per_obj_mse: ['6.867961201351136e-05', '4.39409413957037e-05']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.001501194490119815
per_obj_mse: ['0.0003660997317638248', '0.002636289456859231']
Reconstruction Loss 0.0015274856271936916
per_obj_mse: ['0.0003814907686319202', '0.002673480659723282']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=20, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=20)
(7998, 2, 20)
(1998, 2, 20)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(20, 40)
  (control_LSTM): LSTM(20, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=20, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=10, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=20, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=20)
(7997, 2, 20)
(1998, 2, 20)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(20, 40)
  (control_LSTM): LSTM(20, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=20, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=20, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=20, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=20)
(7996, 2, 20)
(1997, 2, 20)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(20, 40)
  (control_LSTM): LSTM(20, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=20, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=40, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=20, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=20)
(7994, 2, 20)
(1995, 2, 20)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(20, 40)
  (control_LSTM): LSTM(20, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=20, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=60, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=20, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=20)
(7992, 2, 20)
(1993, 2, 20)
Data loaded!
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 5.915224994532764e-05
per_obj_mse: ['7.125753472791985e-05', '4.704696766566485e-05']
Reconstruction Loss 5.631027268444986e-05
per_obj_mse: ['6.867960473755375e-05', '4.3940937757724896e-05']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=10, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7998, 2, 11)
(1998, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.00019561435468494893
per_obj_mse: ['0.00029528498998843133', '9.594373113941401e-05']
Reconstruction Loss 0.00019118033956178838
per_obj_mse: ['0.0002900742692872882', '9.228645649272949e-05']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=20, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7997, 2, 11)
(1997, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0004674404789693654
per_obj_mse: ['0.0007450158009305596', '0.00018986521172337234']
Reconstruction Loss 0.00046964179782516804
per_obj_mse: ['0.0007538485806435347', '0.00018543502665124834']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=40, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7995, 2, 11)
(1995, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0010366276731312073
per_obj_mse: ['0.001689973403699696', '0.0003832818183582276']
Reconstruction Loss 0.0010256464588444185
per_obj_mse: ['0.0016808286309242249', '0.00037046417128294706']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=60, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7993, 2, 11)
(1993, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0016021758974625311
per_obj_mse: ['0.0026319026947021484', '0.0005724492366425693']
Reconstruction Loss 0.0015811934949534843
per_obj_mse: ['0.002607229631394148', '0.0005551579524762928']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=80, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7991, 2, 11)
(1991, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0021509278795912162
per_obj_mse: ['0.0035434127785265446', '0.0007584425038658082']
Reconstruction Loss 0.0021368352959885827
per_obj_mse: ['0.0035339740570634604', '0.0007396965520456433']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=100, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7989, 2, 11)
(1989, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.00272434906315271
per_obj_mse: ['0.0044979555532336235', '0.0009507422219030559']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.001501194490119815
per_obj_mse: ['0.0003660997317638248', '0.002636289456859231']
Reconstruction Loss 0.0015274856271936916
per_obj_mse: ['0.0003814907686319202', '0.002673480659723282']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=20, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7997, 2, 11)
(1997, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.006016265068203211
per_obj_mse: ['0.0018265111139044166', '0.010206018574535847']
Reconstruction Loss 0.006293492111655835
per_obj_mse: ['0.001894900924526155', '0.010692085139453411']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=60, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7993, 2, 11)
(1993, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.01825303987987999
per_obj_mse: ['0.005696645937860012', '0.03080943413078785']
Reconstruction Loss 0.01900495689248267
per_obj_mse: ['0.005899814423173666', '0.032110098749399185']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=100, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7989, 2, 11)
(1989, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.03048433394015611
per_obj_mse: ['0.009609510190784931', '0.051359158009290695']
Reconstruction Loss 0.031714388811300606
per_obj_mse: ['0.009908760897815228', '0.05352000892162323']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=140, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7985, 2, 11)
(1985, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.042518475070095804
per_obj_mse: ['0.013417725451290607', '0.07161922752857208']
Reconstruction Loss 0.04445069815672615
per_obj_mse: ['0.013921892270445824', '0.0749795064330101']
cpu
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=False, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/home/hw1415904/data3/codes/2020-10-09//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/home/hw1415904/data3/codes/2020-10-09/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0, inplace=False)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
cpu
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=False, dataset='narma_2d_1_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/home/hw1415904/data3/codes/2020-10-09//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/home/hw1415904/data3/codes/2020-10-09/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_1_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0, inplace=False)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0015011943401768803
per_obj_mse: ['0.0003660994698293507', '0.002636289456859231']
Reconstruction Loss 0.0015274855764694567
per_obj_mse: ['0.0003814906522165984', '0.0026734801940619946']
