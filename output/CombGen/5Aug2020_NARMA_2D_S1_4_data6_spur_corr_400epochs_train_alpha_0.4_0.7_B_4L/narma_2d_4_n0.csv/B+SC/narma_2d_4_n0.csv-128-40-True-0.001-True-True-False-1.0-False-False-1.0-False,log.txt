cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_4_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=400, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_4_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Epoch: 1 [0/5999 (0%)]	Loss: 98.082657
Epoch: 1 [2560/5999 (43%)]	Loss: 16.609316
saving model at:1,0.12200910151004792
Epoch: 1 [5120/5999 (85%)]	Loss: 11.460349
saving model at:1,0.08851202857494354
====> Epoch: 1 Average loss: 0.162598 
Epoch: 2 [0/5999 (0%)]	Loss: 10.890052
Epoch: 2 [2560/5999 (43%)]	Loss: 11.175838
saving model at:2,0.0797861704826355
Epoch: 2 [5120/5999 (85%)]	Loss: 10.302599
saving model at:2,0.06921794199943543
====> Epoch: 2 Average loss: 0.080197 
Epoch: 3 [0/5999 (0%)]	Loss: 7.919736
Epoch: 3 [2560/5999 (43%)]	Loss: 5.014574
saving model at:3,0.03762378430366516
Epoch: 3 [5120/5999 (85%)]	Loss: 3.042923
saving model at:3,0.0254455276876688
====> Epoch: 3 Average loss: 0.040089 
Epoch: 4 [0/5999 (0%)]	Loss: 3.173303
Epoch: 4 [2560/5999 (43%)]	Loss: 2.513073
saving model at:4,0.01794958083331585
Epoch: 4 [5120/5999 (85%)]	Loss: 1.927747
saving model at:4,0.014737029537558555
====> Epoch: 4 Average loss: 0.018552 
Epoch: 5 [0/5999 (0%)]	Loss: 1.758797
Epoch: 5 [2560/5999 (43%)]	Loss: 1.471007
saving model at:5,0.011316796809434891
Epoch: 5 [5120/5999 (85%)]	Loss: 1.274545
saving model at:5,0.010103531993925571
====> Epoch: 5 Average loss: 0.011633 
Epoch: 6 [0/5999 (0%)]	Loss: 1.188968
Epoch: 6 [2560/5999 (43%)]	Loss: 1.190720
saving model at:6,0.009210004307329654
Epoch: 6 [5120/5999 (85%)]	Loss: 1.101577
====> Epoch: 6 Average loss: 0.009508 
Epoch: 7 [0/5999 (0%)]	Loss: 0.977788
Epoch: 7 [2560/5999 (43%)]	Loss: 0.816845
saving model at:7,0.006668073821812868
Epoch: 7 [5120/5999 (85%)]	Loss: 0.774975
====> Epoch: 7 Average loss: 0.006919 
Epoch: 8 [0/5999 (0%)]	Loss: 0.911485
Epoch: 8 [2560/5999 (43%)]	Loss: 0.783364
saving model at:8,0.005368321798741817
Epoch: 8 [5120/5999 (85%)]	Loss: 0.584044
saving model at:8,0.004783672507852316
====> Epoch: 8 Average loss: 0.005532 
Epoch: 9 [0/5999 (0%)]	Loss: 0.690444
Epoch: 9 [2560/5999 (43%)]	Loss: 0.822133
Epoch: 9 [5120/5999 (85%)]	Loss: 0.519650
saving model at:9,0.0041926963850855824
====> Epoch: 9 Average loss: 0.004824 
Epoch: 10 [0/5999 (0%)]	Loss: 0.522629
Epoch: 10 [2560/5999 (43%)]	Loss: 0.586204
saving model at:10,0.0036846149675548077
Epoch: 10 [5120/5999 (85%)]	Loss: 0.462828
====> Epoch: 10 Average loss: 0.004278 
Epoch: 11 [0/5999 (0%)]	Loss: 0.487943
Epoch: 11 [2560/5999 (43%)]	Loss: 0.497458
saving model at:11,0.0033696077819913624
Epoch: 11 [5120/5999 (85%)]	Loss: 0.632034
saving model at:11,0.0033459718674421312
====> Epoch: 11 Average loss: 0.004029 
Epoch: 12 [0/5999 (0%)]	Loss: 0.407334
Epoch: 12 [2560/5999 (43%)]	Loss: 0.425221
Epoch: 12 [5120/5999 (85%)]	Loss: 0.370081
saving model at:12,0.0030461185444146397
====> Epoch: 12 Average loss: 0.003535 
Epoch: 13 [0/5999 (0%)]	Loss: 0.443519
Epoch: 13 [2560/5999 (43%)]	Loss: 0.418630
saving model at:13,0.0028982335068285466
Epoch: 13 [5120/5999 (85%)]	Loss: 0.339481
saving model at:13,0.002567505456507206
====> Epoch: 13 Average loss: 0.003082 
Epoch: 14 [0/5999 (0%)]	Loss: 0.283551
Epoch: 14 [2560/5999 (43%)]	Loss: 0.282786
Epoch: 14 [5120/5999 (85%)]	Loss: 0.334306
saving model at:14,0.0024677349776029587
====> Epoch: 14 Average loss: 0.002913 
Epoch: 15 [0/5999 (0%)]	Loss: 0.597957
Epoch: 15 [2560/5999 (43%)]	Loss: 0.432366
Epoch: 15 [5120/5999 (85%)]	Loss: 0.471992
====> Epoch: 15 Average loss: 0.003269 
Epoch: 16 [0/5999 (0%)]	Loss: 0.324137
Epoch: 16 [2560/5999 (43%)]	Loss: 0.291105
Epoch: 16 [5120/5999 (85%)]	Loss: 0.257416
saving model at:16,0.002050737355835736
====> Epoch: 16 Average loss: 0.003182 
Epoch: 17 [0/5999 (0%)]	Loss: 0.258301
Epoch: 17 [2560/5999 (43%)]	Loss: 0.281213
saving model at:17,0.0019163629971444608
Epoch: 17 [5120/5999 (85%)]	Loss: 0.256691
====> Epoch: 17 Average loss: 0.002281 
Epoch: 18 [0/5999 (0%)]	Loss: 0.274794
Epoch: 18 [2560/5999 (43%)]	Loss: 0.257544
Epoch: 18 [5120/5999 (85%)]	Loss: 0.269705
====> Epoch: 18 Average loss: 0.002237 
Epoch: 19 [0/5999 (0%)]	Loss: 0.233131
Epoch: 19 [2560/5999 (43%)]	Loss: 0.396664
Epoch: 19 [5120/5999 (85%)]	Loss: 0.262788
====> Epoch: 19 Average loss: 0.002438 
Epoch: 20 [0/5999 (0%)]	Loss: 0.247423
Epoch: 20 [2560/5999 (43%)]	Loss: 0.211176
Epoch: 20 [5120/5999 (85%)]	Loss: 0.314775
saving model at:20,0.0017809255439788104
====> Epoch: 20 Average loss: 0.002153 
Epoch: 21 [0/5999 (0%)]	Loss: 0.346085
Epoch: 21 [2560/5999 (43%)]	Loss: 0.233592
saving model at:21,0.0015473852287977933
Epoch: 21 [5120/5999 (85%)]	Loss: 0.226287
====> Epoch: 21 Average loss: 0.002261 
Epoch: 22 [0/5999 (0%)]	Loss: 0.213412
Epoch: 22 [2560/5999 (43%)]	Loss: 0.430126
Epoch: 22 [5120/5999 (85%)]	Loss: 0.531693
saving model at:22,0.001436594749800861
====> Epoch: 22 Average loss: 0.001947 
Epoch: 23 [0/5999 (0%)]	Loss: 0.170574
Epoch: 23 [2560/5999 (43%)]	Loss: 0.144958
saving model at:23,0.001260581759735942
Epoch: 23 [5120/5999 (85%)]	Loss: 0.194497
====> Epoch: 23 Average loss: 0.001495 
Epoch: 24 [0/5999 (0%)]	Loss: 0.175015
Epoch: 24 [2560/5999 (43%)]	Loss: 0.148063
saving model at:24,0.001241003418341279
Epoch: 24 [5120/5999 (85%)]	Loss: 0.163403
saving model at:24,0.0011395515026524662
====> Epoch: 24 Average loss: 0.001535 
Epoch: 25 [0/5999 (0%)]	Loss: 0.305930
Epoch: 25 [2560/5999 (43%)]	Loss: 0.248954
Epoch: 25 [5120/5999 (85%)]	Loss: 0.183259
====> Epoch: 25 Average loss: 0.001963 
Epoch: 26 [0/5999 (0%)]	Loss: 0.252599
Epoch: 26 [2560/5999 (43%)]	Loss: 0.319508
Epoch: 26 [5120/5999 (85%)]	Loss: 0.146693
====> Epoch: 26 Average loss: 0.002066 
Epoch: 27 [0/5999 (0%)]	Loss: 0.171441
Epoch: 27 [2560/5999 (43%)]	Loss: 0.159981
Epoch: 27 [5120/5999 (85%)]	Loss: 0.855701
====> Epoch: 27 Average loss: 0.002011 
Epoch: 28 [0/5999 (0%)]	Loss: 0.210088
Epoch: 28 [2560/5999 (43%)]	Loss: 0.243782
Epoch: 28 [5120/5999 (85%)]	Loss: 0.381798
====> Epoch: 28 Average loss: 0.001827 
Epoch: 29 [0/5999 (0%)]	Loss: 0.313529
Epoch: 29 [2560/5999 (43%)]	Loss: 0.203775
Epoch: 29 [5120/5999 (85%)]	Loss: 0.445971
====> Epoch: 29 Average loss: 0.001733 
Epoch: 30 [0/5999 (0%)]	Loss: 0.186880
Epoch: 30 [2560/5999 (43%)]	Loss: 0.149090
Epoch: 30 [5120/5999 (85%)]	Loss: 0.236698
====> Epoch: 30 Average loss: 0.001604 
Epoch: 31 [0/5999 (0%)]	Loss: 0.274408
Epoch: 31 [2560/5999 (43%)]	Loss: 0.243184
Epoch: 31 [5120/5999 (85%)]	Loss: 0.134089
saving model at:31,0.0010727705135941505
====> Epoch: 31 Average loss: 0.001627 
Epoch: 32 [0/5999 (0%)]	Loss: 0.145658
Epoch: 32 [2560/5999 (43%)]	Loss: 0.117495
saving model at:32,0.0008704498577862977
Epoch: 32 [5120/5999 (85%)]	Loss: 0.127705
saving model at:32,0.0008226444553583861
====> Epoch: 32 Average loss: 0.001339 
Epoch: 33 [0/5999 (0%)]	Loss: 0.107997
Epoch: 33 [2560/5999 (43%)]	Loss: 0.145509
Epoch: 33 [5120/5999 (85%)]	Loss: 0.124264
====> Epoch: 33 Average loss: 0.001390 
Epoch: 34 [0/5999 (0%)]	Loss: 0.122183
Epoch: 34 [2560/5999 (43%)]	Loss: 0.397113
Epoch: 34 [5120/5999 (85%)]	Loss: 0.124004
====> Epoch: 34 Average loss: 0.001274 
Epoch: 35 [0/5999 (0%)]	Loss: 0.220508
Epoch: 35 [2560/5999 (43%)]	Loss: 0.115193
Epoch: 35 [5120/5999 (85%)]	Loss: 0.115953
====> Epoch: 35 Average loss: 0.001541 
Epoch: 36 [0/5999 (0%)]	Loss: 0.290104
Epoch: 36 [2560/5999 (43%)]	Loss: 0.122821
Epoch: 36 [5120/5999 (85%)]	Loss: 0.178803
saving model at:36,0.0008011432513594627
====> Epoch: 36 Average loss: 0.001461 
Epoch: 37 [0/5999 (0%)]	Loss: 0.137629
Epoch: 37 [2560/5999 (43%)]	Loss: 0.140543
Epoch: 37 [5120/5999 (85%)]	Loss: 0.252392
====> Epoch: 37 Average loss: 0.001255 
Epoch: 38 [0/5999 (0%)]	Loss: 0.231130
Epoch: 38 [2560/5999 (43%)]	Loss: 0.161461
Epoch: 38 [5120/5999 (85%)]	Loss: 0.174352
====> Epoch: 38 Average loss: 0.001507 
Epoch: 39 [0/5999 (0%)]	Loss: 0.195729
Epoch: 39 [2560/5999 (43%)]	Loss: 0.122618
saving model at:39,0.0006748748980462551
Epoch: 39 [5120/5999 (85%)]	Loss: 0.183968
====> Epoch: 39 Average loss: 0.001044 
Epoch: 40 [0/5999 (0%)]	Loss: 0.263850
Epoch: 40 [2560/5999 (43%)]	Loss: 0.245398
Epoch: 40 [5120/5999 (85%)]	Loss: 0.141023
====> Epoch: 40 Average loss: 0.001146 
Epoch: 41 [0/5999 (0%)]	Loss: 0.146103
Epoch: 41 [2560/5999 (43%)]	Loss: 0.093168
Epoch: 41 [5120/5999 (85%)]	Loss: 0.096224
====> Epoch: 41 Average loss: 0.001434 
Epoch: 42 [0/5999 (0%)]	Loss: 0.114970
Epoch: 42 [2560/5999 (43%)]	Loss: 0.084750
saving model at:42,0.0006172223081812263
Epoch: 42 [5120/5999 (85%)]	Loss: 0.087596
====> Epoch: 42 Average loss: 0.001095 
Epoch: 43 [0/5999 (0%)]	Loss: 0.091339
Epoch: 43 [2560/5999 (43%)]	Loss: 0.116404
Epoch: 43 [5120/5999 (85%)]	Loss: 0.108037
====> Epoch: 43 Average loss: 0.001126 
Epoch: 44 [0/5999 (0%)]	Loss: 0.103109
Epoch: 44 [2560/5999 (43%)]	Loss: 0.130459
Epoch: 44 [5120/5999 (85%)]	Loss: 0.181361
====> Epoch: 44 Average loss: 0.001264 
Epoch: 45 [0/5999 (0%)]	Loss: 0.152608
Epoch: 45 [2560/5999 (43%)]	Loss: 0.134898
Epoch: 45 [5120/5999 (85%)]	Loss: 0.104408
====> Epoch: 45 Average loss: 0.001035 
Epoch: 46 [0/5999 (0%)]	Loss: 0.129330
Epoch: 46 [2560/5999 (43%)]	Loss: 0.102990
saving model at:46,0.0005888892780058086
Epoch: 46 [5120/5999 (85%)]	Loss: 0.259629
====> Epoch: 46 Average loss: 0.001183 
Epoch: 47 [0/5999 (0%)]	Loss: 0.151350
Epoch: 47 [2560/5999 (43%)]	Loss: 0.094601
Epoch: 47 [5120/5999 (85%)]	Loss: 0.116350
====> Epoch: 47 Average loss: 0.001279 
Epoch: 48 [0/5999 (0%)]	Loss: 0.081199
Epoch: 48 [2560/5999 (43%)]	Loss: 0.195467
Epoch: 48 [5120/5999 (85%)]	Loss: 0.104139
====> Epoch: 48 Average loss: 0.001345 
Epoch: 49 [0/5999 (0%)]	Loss: 0.135768
Epoch: 49 [2560/5999 (43%)]	Loss: 0.263377
Epoch: 49 [5120/5999 (85%)]	Loss: 0.186087
saving model at:49,0.0005281921774148941
====> Epoch: 49 Average loss: 0.000990 
Epoch: 50 [0/5999 (0%)]	Loss: 0.378192
Epoch: 50 [2560/5999 (43%)]	Loss: 0.187485
Epoch: 50 [5120/5999 (85%)]	Loss: 0.190355
====> Epoch: 50 Average loss: 0.001616 
Epoch: 51 [0/5999 (0%)]	Loss: 0.078196
Epoch: 51 [2560/5999 (43%)]	Loss: 0.097587
Epoch: 51 [5120/5999 (85%)]	Loss: 0.230415
====> Epoch: 51 Average loss: 0.001112 
Epoch: 52 [0/5999 (0%)]	Loss: 0.180928
Epoch: 52 [2560/5999 (43%)]	Loss: 0.073718
Epoch: 52 [5120/5999 (85%)]	Loss: 0.114154
====> Epoch: 52 Average loss: 0.001191 
Epoch: 53 [0/5999 (0%)]	Loss: 0.074072
Epoch: 53 [2560/5999 (43%)]	Loss: 0.157537
Epoch: 53 [5120/5999 (85%)]	Loss: 0.100290
====> Epoch: 53 Average loss: 0.001312 
Epoch: 54 [0/5999 (0%)]	Loss: 0.069162
Epoch: 54 [2560/5999 (43%)]	Loss: 0.066650
saving model at:54,0.0005103141954168677
Epoch: 54 [5120/5999 (85%)]	Loss: 0.189439
====> Epoch: 54 Average loss: 0.000964 
Epoch: 55 [0/5999 (0%)]	Loss: 0.404655
Epoch: 55 [2560/5999 (43%)]	Loss: 0.588107
Epoch: 55 [5120/5999 (85%)]	Loss: 0.375297
====> Epoch: 55 Average loss: 0.001699 
Epoch: 56 [0/5999 (0%)]	Loss: 0.380403
Epoch: 56 [2560/5999 (43%)]	Loss: 0.096440
Epoch: 56 [5120/5999 (85%)]	Loss: 0.164993
====> Epoch: 56 Average loss: 0.001273 
Epoch: 57 [0/5999 (0%)]	Loss: 0.173324
Epoch: 57 [2560/5999 (43%)]	Loss: 0.196534
Epoch: 57 [5120/5999 (85%)]	Loss: 0.226249
====> Epoch: 57 Average loss: 0.001228 
Epoch: 58 [0/5999 (0%)]	Loss: 0.554602
Epoch: 58 [2560/5999 (43%)]	Loss: 0.078782
Epoch: 58 [5120/5999 (85%)]	Loss: 0.085205
====> Epoch: 58 Average loss: 0.001025 
Epoch: 59 [0/5999 (0%)]	Loss: 0.081617
Epoch: 59 [2560/5999 (43%)]	Loss: 0.079816
saving model at:59,0.0005066217083949596
Epoch: 59 [5120/5999 (85%)]	Loss: 0.157477
====> Epoch: 59 Average loss: 0.001090 
Epoch: 60 [0/5999 (0%)]	Loss: 0.068008
Epoch: 60 [2560/5999 (43%)]	Loss: 0.087547
Epoch: 60 [5120/5999 (85%)]	Loss: 0.277352
====> Epoch: 60 Average loss: 0.001101 
Epoch: 61 [0/5999 (0%)]	Loss: 0.186859
Epoch: 61 [2560/5999 (43%)]	Loss: 0.074440
saving model at:61,0.00047532623424194754
Epoch: 61 [5120/5999 (85%)]	Loss: 0.082543
====> Epoch: 61 Average loss: 0.000954 
Epoch: 62 [0/5999 (0%)]	Loss: 0.075548
Epoch: 62 [2560/5999 (43%)]	Loss: 0.065764
Epoch: 62 [5120/5999 (85%)]	Loss: 0.202608
====> Epoch: 62 Average loss: 0.000861 
Epoch: 63 [0/5999 (0%)]	Loss: 0.090302
Epoch: 63 [2560/5999 (43%)]	Loss: 0.053881
Epoch: 63 [5120/5999 (85%)]	Loss: 0.066753
saving model at:63,0.00044477217039093375
====> Epoch: 63 Average loss: 0.000852 
Epoch: 64 [0/5999 (0%)]	Loss: 0.085947
Epoch: 64 [2560/5999 (43%)]	Loss: 0.061727
Epoch: 64 [5120/5999 (85%)]	Loss: 0.149505
saving model at:64,0.00043960256688296796
====> Epoch: 64 Average loss: 0.000834 
Epoch: 65 [0/5999 (0%)]	Loss: 0.453281
Epoch: 65 [2560/5999 (43%)]	Loss: 0.076260
Epoch: 65 [5120/5999 (85%)]	Loss: 0.074575
====> Epoch: 65 Average loss: 0.001219 
Epoch: 66 [0/5999 (0%)]	Loss: 0.108467
Epoch: 66 [2560/5999 (43%)]	Loss: 0.085101
Epoch: 66 [5120/5999 (85%)]	Loss: 0.258317
====> Epoch: 66 Average loss: 0.001175 
Epoch: 67 [0/5999 (0%)]	Loss: 0.132251
Epoch: 67 [2560/5999 (43%)]	Loss: 0.099701
Epoch: 67 [5120/5999 (85%)]	Loss: 0.127093
====> Epoch: 67 Average loss: 0.001105 
Epoch: 68 [0/5999 (0%)]	Loss: 0.096498
Epoch: 68 [2560/5999 (43%)]	Loss: 0.121059
Epoch: 68 [5120/5999 (85%)]	Loss: 0.104859
====> Epoch: 68 Average loss: 0.000857 
Epoch: 69 [0/5999 (0%)]	Loss: 0.049993
Epoch: 69 [2560/5999 (43%)]	Loss: 0.059836
Epoch: 69 [5120/5999 (85%)]	Loss: 0.159737
====> Epoch: 69 Average loss: 0.001274 
Epoch: 70 [0/5999 (0%)]	Loss: 0.333420
Epoch: 70 [2560/5999 (43%)]	Loss: 0.136976
Epoch: 70 [5120/5999 (85%)]	Loss: 0.079645
====> Epoch: 70 Average loss: 0.001215 
Epoch: 71 [0/5999 (0%)]	Loss: 0.115535
Epoch: 71 [2560/5999 (43%)]	Loss: 0.076320
saving model at:71,0.0004001558106392622
Epoch: 71 [5120/5999 (85%)]	Loss: 0.086732
====> Epoch: 71 Average loss: 0.000668 
Epoch: 72 [0/5999 (0%)]	Loss: 0.055577
Epoch: 72 [2560/5999 (43%)]	Loss: 0.139173
Epoch: 72 [5120/5999 (85%)]	Loss: 0.054657
====> Epoch: 72 Average loss: 0.000960 
Epoch: 73 [0/5999 (0%)]	Loss: 0.100672
Epoch: 73 [2560/5999 (43%)]	Loss: 0.161109
Epoch: 73 [5120/5999 (85%)]	Loss: 0.062090
====> Epoch: 73 Average loss: 0.000812 
Epoch: 74 [0/5999 (0%)]	Loss: 0.093089
Epoch: 74 [2560/5999 (43%)]	Loss: 0.237913
Epoch: 74 [5120/5999 (85%)]	Loss: 0.099067
====> Epoch: 74 Average loss: 0.001217 
Epoch: 75 [0/5999 (0%)]	Loss: 0.053621
Epoch: 75 [2560/5999 (43%)]	Loss: 0.062286
saving model at:75,0.0003696812482085079
Epoch: 75 [5120/5999 (85%)]	Loss: 0.064530
====> Epoch: 75 Average loss: 0.000797 
Epoch: 76 [0/5999 (0%)]	Loss: 0.065762
Epoch: 76 [2560/5999 (43%)]	Loss: 0.172763
Epoch: 76 [5120/5999 (85%)]	Loss: 0.094918
====> Epoch: 76 Average loss: 0.001049 
Epoch: 77 [0/5999 (0%)]	Loss: 0.189577
Epoch: 77 [2560/5999 (43%)]	Loss: 0.112643
Epoch: 77 [5120/5999 (85%)]	Loss: 0.076630
saving model at:77,0.00033942770399153233
====> Epoch: 77 Average loss: 0.000668 
Epoch: 78 [0/5999 (0%)]	Loss: 0.098314
Epoch: 78 [2560/5999 (43%)]	Loss: 0.075215
Epoch: 78 [5120/5999 (85%)]	Loss: 0.100913
====> Epoch: 78 Average loss: 0.001055 
Epoch: 79 [0/5999 (0%)]	Loss: 0.070802
Epoch: 79 [2560/5999 (43%)]	Loss: 0.067979
Epoch: 79 [5120/5999 (85%)]	Loss: 0.084104
====> Epoch: 79 Average loss: 0.000830 
Epoch: 80 [0/5999 (0%)]	Loss: 0.048566
Epoch: 80 [2560/5999 (43%)]	Loss: 0.054877
Epoch: 80 [5120/5999 (85%)]	Loss: 0.297375
====> Epoch: 80 Average loss: 0.000862 
Epoch: 81 [0/5999 (0%)]	Loss: 0.101685
Epoch: 81 [2560/5999 (43%)]	Loss: 0.065030
Epoch: 81 [5120/5999 (85%)]	Loss: 0.400870
====> Epoch: 81 Average loss: 0.000835 
Epoch: 82 [0/5999 (0%)]	Loss: 0.101474
Epoch: 82 [2560/5999 (43%)]	Loss: 0.099722
Epoch: 82 [5120/5999 (85%)]	Loss: 0.147274
====> Epoch: 82 Average loss: 0.000889 
Epoch: 83 [0/5999 (0%)]	Loss: 0.111738
Epoch: 83 [2560/5999 (43%)]	Loss: 0.060238
saving model at:83,0.00033843750902451576
Epoch: 83 [5120/5999 (85%)]	Loss: 0.052659
====> Epoch: 83 Average loss: 0.000684 
Epoch: 84 [0/5999 (0%)]	Loss: 0.345876
Epoch: 84 [2560/5999 (43%)]	Loss: 0.064459
Epoch: 84 [5120/5999 (85%)]	Loss: 0.054723
====> Epoch: 84 Average loss: 0.000691 
Epoch: 85 [0/5999 (0%)]	Loss: 0.054191
Epoch: 85 [2560/5999 (43%)]	Loss: 0.071332
Epoch: 85 [5120/5999 (85%)]	Loss: 0.073184
saving model at:85,0.0002891318961046636
====> Epoch: 85 Average loss: 0.000620 
Epoch: 86 [0/5999 (0%)]	Loss: 0.046672
Epoch: 86 [2560/5999 (43%)]	Loss: 0.050087
Epoch: 86 [5120/5999 (85%)]	Loss: 0.091253
====> Epoch: 86 Average loss: 0.000513 
Epoch: 87 [0/5999 (0%)]	Loss: 0.129331
Epoch: 87 [2560/5999 (43%)]	Loss: 0.050213
Epoch: 87 [5120/5999 (85%)]	Loss: 0.129543
====> Epoch: 87 Average loss: 0.000924 
Epoch: 88 [0/5999 (0%)]	Loss: 0.389520
Epoch: 88 [2560/5999 (43%)]	Loss: 0.118146
Epoch: 88 [5120/5999 (85%)]	Loss: 0.054093
====> Epoch: 88 Average loss: 0.001058 
Epoch: 89 [0/5999 (0%)]	Loss: 0.058017
Epoch: 89 [2560/5999 (43%)]	Loss: 0.104333
Epoch: 89 [5120/5999 (85%)]	Loss: 0.083376
====> Epoch: 89 Average loss: 0.000680 
Epoch: 90 [0/5999 (0%)]	Loss: 0.159549
Epoch: 90 [2560/5999 (43%)]	Loss: 0.234843
Epoch: 90 [5120/5999 (85%)]	Loss: 0.408513
====> Epoch: 90 Average loss: 0.000936 
Epoch: 91 [0/5999 (0%)]	Loss: 0.115919
Epoch: 91 [2560/5999 (43%)]	Loss: 0.112768
Epoch: 91 [5120/5999 (85%)]	Loss: 0.054076
====> Epoch: 91 Average loss: 0.000723 
Epoch: 92 [0/5999 (0%)]	Loss: 0.286873
Epoch: 92 [2560/5999 (43%)]	Loss: 0.075440
Epoch: 92 [5120/5999 (85%)]	Loss: 0.076111
====> Epoch: 92 Average loss: 0.000588 
Epoch: 93 [0/5999 (0%)]	Loss: 0.043647
Epoch: 93 [2560/5999 (43%)]	Loss: 0.077724
Epoch: 93 [5120/5999 (85%)]	Loss: 0.040628
====> Epoch: 93 Average loss: 0.000709 
Epoch: 94 [0/5999 (0%)]	Loss: 0.139206
Epoch: 94 [2560/5999 (43%)]	Loss: 0.092752
Epoch: 94 [5120/5999 (85%)]	Loss: 0.063263
====> Epoch: 94 Average loss: 0.000946 
Epoch: 95 [0/5999 (0%)]	Loss: 0.056930
Epoch: 95 [2560/5999 (43%)]	Loss: 0.077757
Epoch: 95 [5120/5999 (85%)]	Loss: 0.047127
====> Epoch: 95 Average loss: 0.000621 
Epoch: 96 [0/5999 (0%)]	Loss: 0.078136
Epoch: 96 [2560/5999 (43%)]	Loss: 0.047389
Epoch: 96 [5120/5999 (85%)]	Loss: 0.116150
====> Epoch: 96 Average loss: 0.000776 
Epoch: 97 [0/5999 (0%)]	Loss: 0.122088
Epoch: 97 [2560/5999 (43%)]	Loss: 0.104450
Epoch: 97 [5120/5999 (85%)]	Loss: 0.141527
====> Epoch: 97 Average loss: 0.000968 
Epoch: 98 [0/5999 (0%)]	Loss: 0.108151
Epoch: 98 [2560/5999 (43%)]	Loss: 0.074842
Epoch: 98 [5120/5999 (85%)]	Loss: 0.139903
====> Epoch: 98 Average loss: 0.000756 
Epoch: 99 [0/5999 (0%)]	Loss: 0.178998
Epoch: 99 [2560/5999 (43%)]	Loss: 0.057549
saving model at:99,0.00028221668605692683
Epoch: 99 [5120/5999 (85%)]	Loss: 0.054634
====> Epoch: 99 Average loss: 0.000706 
Epoch: 100 [0/5999 (0%)]	Loss: 0.050404
Epoch: 100 [2560/5999 (43%)]	Loss: 0.070941
Epoch: 100 [5120/5999 (85%)]	Loss: 0.185678
====> Epoch: 100 Average loss: 0.000614 
Epoch: 101 [0/5999 (0%)]	Loss: 0.043141
Epoch: 101 [2560/5999 (43%)]	Loss: 0.050116
saving model at:101,0.00026699894247576595
Epoch: 101 [5120/5999 (85%)]	Loss: 0.169809
====> Epoch: 101 Average loss: 0.001105 
Epoch: 102 [0/5999 (0%)]	Loss: 0.145031
Epoch: 102 [2560/5999 (43%)]	Loss: 0.429934
Epoch: 102 [5120/5999 (85%)]	Loss: 0.213271
====> Epoch: 102 Average loss: 0.001369 
Epoch: 103 [0/5999 (0%)]	Loss: 0.123416
Epoch: 103 [2560/5999 (43%)]	Loss: 0.065214
Epoch: 103 [5120/5999 (85%)]	Loss: 0.121628
====> Epoch: 103 Average loss: 0.000837 
Epoch: 104 [0/5999 (0%)]	Loss: 0.139252
Epoch: 104 [2560/5999 (43%)]	Loss: 0.106508
Epoch: 104 [5120/5999 (85%)]	Loss: 0.464429
====> Epoch: 104 Average loss: 0.000716 
Epoch: 105 [0/5999 (0%)]	Loss: 0.088243
Epoch: 105 [2560/5999 (43%)]	Loss: 0.092262
Epoch: 105 [5120/5999 (85%)]	Loss: 0.086379
====> Epoch: 105 Average loss: 0.001170 
Epoch: 106 [0/5999 (0%)]	Loss: 0.183677
Epoch: 106 [2560/5999 (43%)]	Loss: 0.057167
Epoch: 106 [5120/5999 (85%)]	Loss: 0.123006
====> Epoch: 106 Average loss: 0.000758 
Epoch: 107 [0/5999 (0%)]	Loss: 0.048113
Epoch: 107 [2560/5999 (43%)]	Loss: 0.126122
Epoch: 107 [5120/5999 (85%)]	Loss: 0.095606
====> Epoch: 107 Average loss: 0.000817 
Epoch: 108 [0/5999 (0%)]	Loss: 0.040617
Epoch: 108 [2560/5999 (43%)]	Loss: 0.065285
Epoch: 108 [5120/5999 (85%)]	Loss: 0.032563
saving model at:108,0.0002439533588476479
====> Epoch: 108 Average loss: 0.000527 
Epoch: 109 [0/5999 (0%)]	Loss: 0.175870
Epoch: 109 [2560/5999 (43%)]	Loss: 0.163133
Epoch: 109 [5120/5999 (85%)]	Loss: 0.109841
====> Epoch: 109 Average loss: 0.000998 
Epoch: 110 [0/5999 (0%)]	Loss: 0.110901
Epoch: 110 [2560/5999 (43%)]	Loss: 0.067848
Epoch: 110 [5120/5999 (85%)]	Loss: 0.120968
====> Epoch: 110 Average loss: 0.000738 
Epoch: 111 [0/5999 (0%)]	Loss: 0.069905
Epoch: 111 [2560/5999 (43%)]	Loss: 0.065972
Epoch: 111 [5120/5999 (85%)]	Loss: 0.029038
====> Epoch: 111 Average loss: 0.000614 
Epoch: 112 [0/5999 (0%)]	Loss: 0.036695
Epoch: 112 [2560/5999 (43%)]	Loss: 0.052866
Epoch: 112 [5120/5999 (85%)]	Loss: 0.047571
====> Epoch: 112 Average loss: 0.000711 
Epoch: 113 [0/5999 (0%)]	Loss: 0.037688
Epoch: 113 [2560/5999 (43%)]	Loss: 0.074533
saving model at:113,0.00022588811465539037
Epoch: 113 [5120/5999 (85%)]	Loss: 0.058689
====> Epoch: 113 Average loss: 0.000688 
Epoch: 114 [0/5999 (0%)]	Loss: 0.032957
Epoch: 114 [2560/5999 (43%)]	Loss: 0.084270
Epoch: 114 [5120/5999 (85%)]	Loss: 0.142329
====> Epoch: 114 Average loss: 0.000895 
Epoch: 115 [0/5999 (0%)]	Loss: 0.136066
Epoch: 115 [2560/5999 (43%)]	Loss: 0.123331
Epoch: 115 [5120/5999 (85%)]	Loss: 0.124838
====> Epoch: 115 Average loss: 0.000854 
Epoch: 116 [0/5999 (0%)]	Loss: 0.184960
Epoch: 116 [2560/5999 (43%)]	Loss: 0.057212
Epoch: 116 [5120/5999 (85%)]	Loss: 0.051356
====> Epoch: 116 Average loss: 0.000575 
Epoch: 117 [0/5999 (0%)]	Loss: 0.046182
Epoch: 117 [2560/5999 (43%)]	Loss: 0.095476
Epoch: 117 [5120/5999 (85%)]	Loss: 0.091749
====> Epoch: 117 Average loss: 0.000660 
Epoch: 118 [0/5999 (0%)]	Loss: 0.089545
Epoch: 118 [2560/5999 (43%)]	Loss: 0.113041
Epoch: 118 [5120/5999 (85%)]	Loss: 0.121178
====> Epoch: 118 Average loss: 0.000629 
Epoch: 119 [0/5999 (0%)]	Loss: 0.051691
Epoch: 119 [2560/5999 (43%)]	Loss: 0.219572
Epoch: 119 [5120/5999 (85%)]	Loss: 0.038868
====> Epoch: 119 Average loss: 0.000587 
Epoch: 120 [0/5999 (0%)]	Loss: 0.118251
Epoch: 120 [2560/5999 (43%)]	Loss: 0.041066
Epoch: 120 [5120/5999 (85%)]	Loss: 0.123981
====> Epoch: 120 Average loss: 0.000739 
Epoch: 121 [0/5999 (0%)]	Loss: 0.041886
Epoch: 121 [2560/5999 (43%)]	Loss: 0.037199
Epoch: 121 [5120/5999 (85%)]	Loss: 0.021161
====> Epoch: 121 Average loss: 0.000803 
Epoch: 122 [0/5999 (0%)]	Loss: 0.063043
Epoch: 122 [2560/5999 (43%)]	Loss: 0.106490
Epoch: 122 [5120/5999 (85%)]	Loss: 0.287107
====> Epoch: 122 Average loss: 0.001066 
Epoch: 123 [0/5999 (0%)]	Loss: 0.088079
Epoch: 123 [2560/5999 (43%)]	Loss: 0.064275
Epoch: 123 [5120/5999 (85%)]	Loss: 0.047392
====> Epoch: 123 Average loss: 0.000857 
Epoch: 124 [0/5999 (0%)]	Loss: 0.232809
Epoch: 124 [2560/5999 (43%)]	Loss: 0.115241
Epoch: 124 [5120/5999 (85%)]	Loss: 0.046724
====> Epoch: 124 Average loss: 0.001356 
Epoch: 125 [0/5999 (0%)]	Loss: 0.062833
Epoch: 125 [2560/5999 (43%)]	Loss: 0.135597
Epoch: 125 [5120/5999 (85%)]	Loss: 0.215911
====> Epoch: 125 Average loss: 0.000654 
Epoch: 126 [0/5999 (0%)]	Loss: 0.301895
Epoch: 126 [2560/5999 (43%)]	Loss: 0.052017
Epoch: 126 [5120/5999 (85%)]	Loss: 0.069280
====> Epoch: 126 Average loss: 0.000885 
Epoch: 127 [0/5999 (0%)]	Loss: 0.135704
Epoch: 127 [2560/5999 (43%)]	Loss: 0.058087
Epoch: 127 [5120/5999 (85%)]	Loss: 0.054441
====> Epoch: 127 Average loss: 0.000686 
Epoch: 128 [0/5999 (0%)]	Loss: 0.044195
Epoch: 128 [2560/5999 (43%)]	Loss: 0.051979
Epoch: 128 [5120/5999 (85%)]	Loss: 0.060083
====> Epoch: 128 Average loss: 0.000535 
Epoch: 129 [0/5999 (0%)]	Loss: 0.044413
Epoch: 129 [2560/5999 (43%)]	Loss: 0.030196
Epoch: 129 [5120/5999 (85%)]	Loss: 0.031838
====> Epoch: 129 Average loss: 0.000763 
Epoch: 130 [0/5999 (0%)]	Loss: 0.043563
Epoch: 130 [2560/5999 (43%)]	Loss: 0.073708
Epoch: 130 [5120/5999 (85%)]	Loss: 0.129088
====> Epoch: 130 Average loss: 0.000727 
Epoch: 131 [0/5999 (0%)]	Loss: 0.080842
Epoch: 131 [2560/5999 (43%)]	Loss: 0.039762
Epoch: 131 [5120/5999 (85%)]	Loss: 0.059278
====> Epoch: 131 Average loss: 0.000606 
Epoch: 132 [0/5999 (0%)]	Loss: 0.075732
Epoch: 132 [2560/5999 (43%)]	Loss: 0.044103
Epoch: 132 [5120/5999 (85%)]	Loss: 0.136357
====> Epoch: 132 Average loss: 0.000763 
Epoch: 133 [0/5999 (0%)]	Loss: 0.023307
Epoch: 133 [2560/5999 (43%)]	Loss: 0.087216
Epoch: 133 [5120/5999 (85%)]	Loss: 0.137410
====> Epoch: 133 Average loss: 0.000761 
Epoch: 134 [0/5999 (0%)]	Loss: 0.188857
Epoch: 134 [2560/5999 (43%)]	Loss: 0.143212
Epoch: 134 [5120/5999 (85%)]	Loss: 0.125540
====> Epoch: 134 Average loss: 0.000645 
Epoch: 135 [0/5999 (0%)]	Loss: 0.042656
Epoch: 135 [2560/5999 (43%)]	Loss: 0.027915
saving model at:135,0.0001913123537087813
Epoch: 135 [5120/5999 (85%)]	Loss: 0.059065
saving model at:135,0.00017338078434113414
====> Epoch: 135 Average loss: 0.000444 
Epoch: 136 [0/5999 (0%)]	Loss: 0.032018
Epoch: 136 [2560/5999 (43%)]	Loss: 0.119258
Epoch: 136 [5120/5999 (85%)]	Loss: 0.062135
====> Epoch: 136 Average loss: 0.000924 
Epoch: 137 [0/5999 (0%)]	Loss: 0.172804
Epoch: 137 [2560/5999 (43%)]	Loss: 0.163242
Epoch: 137 [5120/5999 (85%)]	Loss: 0.303802
====> Epoch: 137 Average loss: 0.001138 
Epoch: 138 [0/5999 (0%)]	Loss: 0.112246
Epoch: 138 [2560/5999 (43%)]	Loss: 0.052922
Epoch: 138 [5120/5999 (85%)]	Loss: 0.090060
====> Epoch: 138 Average loss: 0.000666 
Epoch: 139 [0/5999 (0%)]	Loss: 0.036302
Epoch: 139 [2560/5999 (43%)]	Loss: 0.085450
Epoch: 139 [5120/5999 (85%)]	Loss: 0.038490
====> Epoch: 139 Average loss: 0.000584 
Epoch: 140 [0/5999 (0%)]	Loss: 0.024968
Epoch: 140 [2560/5999 (43%)]	Loss: 0.033234
Epoch: 140 [5120/5999 (85%)]	Loss: 0.076436
====> Epoch: 140 Average loss: 0.000797 
Epoch: 141 [0/5999 (0%)]	Loss: 0.178108
Epoch: 141 [2560/5999 (43%)]	Loss: 0.022933
Epoch: 141 [5120/5999 (85%)]	Loss: 0.049328
====> Epoch: 141 Average loss: 0.000539 
Epoch: 142 [0/5999 (0%)]	Loss: 0.060050
Epoch: 142 [2560/5999 (43%)]	Loss: 0.037676
Epoch: 142 [5120/5999 (85%)]	Loss: 0.044644
====> Epoch: 142 Average loss: 0.000592 
Epoch: 143 [0/5999 (0%)]	Loss: 0.056779
Epoch: 143 [2560/5999 (43%)]	Loss: 0.056285
Epoch: 143 [5120/5999 (85%)]	Loss: 0.059243
====> Epoch: 143 Average loss: 0.000567 
Epoch: 144 [0/5999 (0%)]	Loss: 0.036365
Epoch: 144 [2560/5999 (43%)]	Loss: 0.054756
Epoch: 144 [5120/5999 (85%)]	Loss: 0.090280
====> Epoch: 144 Average loss: 0.000578 
Epoch: 145 [0/5999 (0%)]	Loss: 0.052235
Epoch: 145 [2560/5999 (43%)]	Loss: 0.216555
Epoch: 145 [5120/5999 (85%)]	Loss: 0.037246
====> Epoch: 145 Average loss: 0.000491 
Epoch: 146 [0/5999 (0%)]	Loss: 0.044093
Epoch: 146 [2560/5999 (43%)]	Loss: 0.193209
Epoch: 146 [5120/5999 (85%)]	Loss: 0.059293
====> Epoch: 146 Average loss: 0.001162 
Epoch: 147 [0/5999 (0%)]	Loss: 0.075992
Epoch: 147 [2560/5999 (43%)]	Loss: 0.031667
Epoch: 147 [5120/5999 (85%)]	Loss: 0.047201
saving model at:147,0.0001707082826178521
====> Epoch: 147 Average loss: 0.000416 
Epoch: 148 [0/5999 (0%)]	Loss: 0.041812
Epoch: 148 [2560/5999 (43%)]	Loss: 0.108479
Epoch: 148 [5120/5999 (85%)]	Loss: 0.137993
====> Epoch: 148 Average loss: 0.001150 
Epoch: 149 [0/5999 (0%)]	Loss: 0.055686
Epoch: 149 [2560/5999 (43%)]	Loss: 0.051794
Epoch: 149 [5120/5999 (85%)]	Loss: 0.080898
====> Epoch: 149 Average loss: 0.001069 
Epoch: 150 [0/5999 (0%)]	Loss: 0.087107
Epoch: 150 [2560/5999 (43%)]	Loss: 0.127838
Epoch: 150 [5120/5999 (85%)]	Loss: 0.066927
====> Epoch: 150 Average loss: 0.000611 
Epoch: 151 [0/5999 (0%)]	Loss: 0.036495
Epoch: 151 [2560/5999 (43%)]	Loss: 0.039743
Epoch: 151 [5120/5999 (85%)]	Loss: 0.050808
====> Epoch: 151 Average loss: 0.000610 
Epoch: 152 [0/5999 (0%)]	Loss: 0.139840
Epoch: 152 [2560/5999 (43%)]	Loss: 0.157931
Epoch: 152 [5120/5999 (85%)]	Loss: 0.056895
====> Epoch: 152 Average loss: 0.000581 
Epoch: 153 [0/5999 (0%)]	Loss: 0.058594
Epoch: 153 [2560/5999 (43%)]	Loss: 0.086793
Epoch: 153 [5120/5999 (85%)]	Loss: 0.052714
====> Epoch: 153 Average loss: 0.000621 
Epoch: 154 [0/5999 (0%)]	Loss: 0.058323
Epoch: 154 [2560/5999 (43%)]	Loss: 0.039775
Epoch: 154 [5120/5999 (85%)]	Loss: 0.141087
====> Epoch: 154 Average loss: 0.000806 
Epoch: 155 [0/5999 (0%)]	Loss: 0.107722
Epoch: 155 [2560/5999 (43%)]	Loss: 0.069908
Epoch: 155 [5120/5999 (85%)]	Loss: 0.073924
====> Epoch: 155 Average loss: 0.000705 
Epoch: 156 [0/5999 (0%)]	Loss: 0.026808
Epoch: 156 [2560/5999 (43%)]	Loss: 0.043740
Epoch: 156 [5120/5999 (85%)]	Loss: 0.110749
====> Epoch: 156 Average loss: 0.000843 
Epoch: 157 [0/5999 (0%)]	Loss: 0.096620
Epoch: 157 [2560/5999 (43%)]	Loss: 0.072387
Epoch: 157 [5120/5999 (85%)]	Loss: 0.022746
====> Epoch: 157 Average loss: 0.000578 
Epoch: 158 [0/5999 (0%)]	Loss: 0.078179
Epoch: 158 [2560/5999 (43%)]	Loss: 0.036061
Epoch: 158 [5120/5999 (85%)]	Loss: 0.055646
====> Epoch: 158 Average loss: 0.000500 
Epoch: 159 [0/5999 (0%)]	Loss: 0.072065
Epoch: 159 [2560/5999 (43%)]	Loss: 0.030094
Epoch: 159 [5120/5999 (85%)]	Loss: 0.111409
====> Epoch: 159 Average loss: 0.000516 
Epoch: 160 [0/5999 (0%)]	Loss: 0.073065
Epoch: 160 [2560/5999 (43%)]	Loss: 0.054769
Epoch: 160 [5120/5999 (85%)]	Loss: 0.068629
====> Epoch: 160 Average loss: 0.000778 
Epoch: 161 [0/5999 (0%)]	Loss: 0.089251
Epoch: 161 [2560/5999 (43%)]	Loss: 0.027015
Epoch: 161 [5120/5999 (85%)]	Loss: 0.089022
====> Epoch: 161 Average loss: 0.000594 
Epoch: 162 [0/5999 (0%)]	Loss: 0.056313
Epoch: 162 [2560/5999 (43%)]	Loss: 0.080486
Epoch: 162 [5120/5999 (85%)]	Loss: 0.046249
====> Epoch: 162 Average loss: 0.000473 
Epoch: 163 [0/5999 (0%)]	Loss: 0.065117
Epoch: 163 [2560/5999 (43%)]	Loss: 0.063569
Epoch: 163 [5120/5999 (85%)]	Loss: 0.049019
====> Epoch: 163 Average loss: 0.000708 
Epoch: 164 [0/5999 (0%)]	Loss: 0.043457
Epoch: 164 [2560/5999 (43%)]	Loss: 0.075444
Epoch: 164 [5120/5999 (85%)]	Loss: 0.047416
saving model at:164,0.00016843735170550645
====> Epoch: 164 Average loss: 0.000459 
Epoch: 165 [0/5999 (0%)]	Loss: 0.022522
Epoch: 165 [2560/5999 (43%)]	Loss: 0.034231
saving model at:165,0.00016029895609244704
Epoch: 165 [5120/5999 (85%)]	Loss: 0.059840
====> Epoch: 165 Average loss: 0.000536 
Epoch: 166 [0/5999 (0%)]	Loss: 0.054197
Epoch: 166 [2560/5999 (43%)]	Loss: 0.043742
Epoch: 166 [5120/5999 (85%)]	Loss: 0.097061
====> Epoch: 166 Average loss: 0.000719 
Epoch: 167 [0/5999 (0%)]	Loss: 0.036957
Epoch: 167 [2560/5999 (43%)]	Loss: 0.044400
Epoch: 167 [5120/5999 (85%)]	Loss: 0.076051
====> Epoch: 167 Average loss: 0.000765 
Epoch: 168 [0/5999 (0%)]	Loss: 0.069304
Epoch: 168 [2560/5999 (43%)]	Loss: 0.099372
Epoch: 168 [5120/5999 (85%)]	Loss: 0.064190
====> Epoch: 168 Average loss: 0.000535 
Epoch: 169 [0/5999 (0%)]	Loss: 0.029904
Epoch: 169 [2560/5999 (43%)]	Loss: 0.067716
Epoch: 169 [5120/5999 (85%)]	Loss: 0.027675
====> Epoch: 169 Average loss: 0.000581 
Epoch: 170 [0/5999 (0%)]	Loss: 0.178444
Epoch: 170 [2560/5999 (43%)]	Loss: 0.041184
Epoch: 170 [5120/5999 (85%)]	Loss: 0.044603
====> Epoch: 170 Average loss: 0.000524 
Epoch: 171 [0/5999 (0%)]	Loss: 0.207752
Epoch: 171 [2560/5999 (43%)]	Loss: 0.053202
Epoch: 171 [5120/5999 (85%)]	Loss: 0.046003
====> Epoch: 171 Average loss: 0.000661 
Epoch: 172 [0/5999 (0%)]	Loss: 0.060954
Epoch: 172 [2560/5999 (43%)]	Loss: 0.033083
Epoch: 172 [5120/5999 (85%)]	Loss: 0.049766
====> Epoch: 172 Average loss: 0.000429 
Epoch: 173 [0/5999 (0%)]	Loss: 0.059365
Epoch: 173 [2560/5999 (43%)]	Loss: 0.022617
Epoch: 173 [5120/5999 (85%)]	Loss: 0.111112
====> Epoch: 173 Average loss: 0.000698 
Epoch: 174 [0/5999 (0%)]	Loss: 0.087746
Epoch: 174 [2560/5999 (43%)]	Loss: 0.036770
Epoch: 174 [5120/5999 (85%)]	Loss: 0.219508
====> Epoch: 174 Average loss: 0.000705 
Epoch: 175 [0/5999 (0%)]	Loss: 0.043455
Epoch: 175 [2560/5999 (43%)]	Loss: 0.123001
Epoch: 175 [5120/5999 (85%)]	Loss: 0.046429
====> Epoch: 175 Average loss: 0.000478 
Epoch: 176 [0/5999 (0%)]	Loss: 0.052611
Epoch: 176 [2560/5999 (43%)]	Loss: 0.116732
Epoch: 176 [5120/5999 (85%)]	Loss: 0.169596
====> Epoch: 176 Average loss: 0.000765 
Epoch: 177 [0/5999 (0%)]	Loss: 0.079448
Epoch: 177 [2560/5999 (43%)]	Loss: 0.077359
Epoch: 177 [5120/5999 (85%)]	Loss: 0.040508
====> Epoch: 177 Average loss: 0.000550 
Epoch: 178 [0/5999 (0%)]	Loss: 0.051376
Epoch: 178 [2560/5999 (43%)]	Loss: 0.103256
Epoch: 178 [5120/5999 (85%)]	Loss: 0.054512
====> Epoch: 178 Average loss: 0.000456 
Epoch: 179 [0/5999 (0%)]	Loss: 0.021772
Epoch: 179 [2560/5999 (43%)]	Loss: 0.121623
Epoch: 179 [5120/5999 (85%)]	Loss: 0.215129
====> Epoch: 179 Average loss: 0.000623 
Epoch: 180 [0/5999 (0%)]	Loss: 0.065857
Epoch: 180 [2560/5999 (43%)]	Loss: 0.045416
Epoch: 180 [5120/5999 (85%)]	Loss: 0.067524
====> Epoch: 180 Average loss: 0.000503 
Epoch: 181 [0/5999 (0%)]	Loss: 0.033226
Epoch: 181 [2560/5999 (43%)]	Loss: 0.107351
Epoch: 181 [5120/5999 (85%)]	Loss: 0.058784
====> Epoch: 181 Average loss: 0.000440 
Epoch: 182 [0/5999 (0%)]	Loss: 0.018969
Epoch: 182 [2560/5999 (43%)]	Loss: 0.039808
Epoch: 182 [5120/5999 (85%)]	Loss: 0.246155
====> Epoch: 182 Average loss: 0.000635 
Epoch: 183 [0/5999 (0%)]	Loss: 0.049473
Epoch: 183 [2560/5999 (43%)]	Loss: 0.049550
saving model at:183,0.00015591812413185834
Epoch: 183 [5120/5999 (85%)]	Loss: 0.023083
====> Epoch: 183 Average loss: 0.000684 
Epoch: 184 [0/5999 (0%)]	Loss: 0.059268
Epoch: 184 [2560/5999 (43%)]	Loss: 0.035930
Epoch: 184 [5120/5999 (85%)]	Loss: 0.071058
====> Epoch: 184 Average loss: 0.000513 
Epoch: 185 [0/5999 (0%)]	Loss: 0.020287
Epoch: 185 [2560/5999 (43%)]	Loss: 0.020798
saving model at:185,0.00014171553205233068
Epoch: 185 [5120/5999 (85%)]	Loss: 0.165780
====> Epoch: 185 Average loss: 0.000617 
Epoch: 186 [0/5999 (0%)]	Loss: 0.059823
Epoch: 186 [2560/5999 (43%)]	Loss: 0.162319
Epoch: 186 [5120/5999 (85%)]	Loss: 0.034914
====> Epoch: 186 Average loss: 0.000737 
Epoch: 187 [0/5999 (0%)]	Loss: 0.125478
Epoch: 187 [2560/5999 (43%)]	Loss: 0.038743
Epoch: 187 [5120/5999 (85%)]	Loss: 0.030887
====> Epoch: 187 Average loss: 0.000565 
Epoch: 188 [0/5999 (0%)]	Loss: 0.097452
Epoch: 188 [2560/5999 (43%)]	Loss: 0.050005
Epoch: 188 [5120/5999 (85%)]	Loss: 0.213311
====> Epoch: 188 Average loss: 0.000439 
Epoch: 189 [0/5999 (0%)]	Loss: 0.073958
Epoch: 189 [2560/5999 (43%)]	Loss: 0.103084
Epoch: 189 [5120/5999 (85%)]	Loss: 0.050628
====> Epoch: 189 Average loss: 0.000630 
Epoch: 190 [0/5999 (0%)]	Loss: 0.056245
Epoch: 190 [2560/5999 (43%)]	Loss: 0.046225
Epoch: 190 [5120/5999 (85%)]	Loss: 0.036715
====> Epoch: 190 Average loss: 0.000526 
Epoch: 191 [0/5999 (0%)]	Loss: 0.067113
Epoch: 191 [2560/5999 (43%)]	Loss: 0.022648
Epoch: 191 [5120/5999 (85%)]	Loss: 0.078281
====> Epoch: 191 Average loss: 0.000458 
Epoch: 192 [0/5999 (0%)]	Loss: 0.041343
Epoch: 192 [2560/5999 (43%)]	Loss: 0.040112
Epoch: 192 [5120/5999 (85%)]	Loss: 0.022047
====> Epoch: 192 Average loss: 0.000495 
Epoch: 193 [0/5999 (0%)]	Loss: 0.197232
Epoch: 193 [2560/5999 (43%)]	Loss: 0.029877
Epoch: 193 [5120/5999 (85%)]	Loss: 0.058026
====> Epoch: 193 Average loss: 0.000513 
Epoch: 194 [0/5999 (0%)]	Loss: 0.042733
Epoch: 194 [2560/5999 (43%)]	Loss: 0.090620
Epoch: 194 [5120/5999 (85%)]	Loss: 0.078565
====> Epoch: 194 Average loss: 0.000674 
Epoch: 195 [0/5999 (0%)]	Loss: 0.121032
Epoch: 195 [2560/5999 (43%)]	Loss: 0.024778
Epoch: 195 [5120/5999 (85%)]	Loss: 0.025476
====> Epoch: 195 Average loss: 0.000516 
Epoch: 196 [0/5999 (0%)]	Loss: 0.136438
Epoch: 196 [2560/5999 (43%)]	Loss: 0.024930
Epoch: 196 [5120/5999 (85%)]	Loss: 0.021717
====> Epoch: 196 Average loss: 0.000491 
Epoch: 197 [0/5999 (0%)]	Loss: 0.144932
Epoch: 197 [2560/5999 (43%)]	Loss: 0.124495
Epoch: 197 [5120/5999 (85%)]	Loss: 0.099171
====> Epoch: 197 Average loss: 0.000488 
Epoch: 198 [0/5999 (0%)]	Loss: 0.049203
Epoch: 198 [2560/5999 (43%)]	Loss: 0.027692
Epoch: 198 [5120/5999 (85%)]	Loss: 0.091090
====> Epoch: 198 Average loss: 0.000366 
Epoch: 199 [0/5999 (0%)]	Loss: 0.053775
Epoch: 199 [2560/5999 (43%)]	Loss: 0.187325
Epoch: 199 [5120/5999 (85%)]	Loss: 0.031616
====> Epoch: 199 Average loss: 0.000728 
Epoch: 200 [0/5999 (0%)]	Loss: 0.210962
Epoch: 200 [2560/5999 (43%)]	Loss: 0.141799
Epoch: 200 [5120/5999 (85%)]	Loss: 0.025278
====> Epoch: 200 Average loss: 0.000625 
Epoch: 201 [0/5999 (0%)]	Loss: 0.052828
Epoch: 201 [2560/5999 (43%)]	Loss: 0.024856
Epoch: 201 [5120/5999 (85%)]	Loss: 0.040722
saving model at:201,0.00013626079284586012
====> Epoch: 201 Average loss: 0.000459 
Epoch: 202 [0/5999 (0%)]	Loss: 0.028987
Epoch: 202 [2560/5999 (43%)]	Loss: 0.077970
Epoch: 202 [5120/5999 (85%)]	Loss: 0.031571
====> Epoch: 202 Average loss: 0.000473 
Epoch: 203 [0/5999 (0%)]	Loss: 0.023940
Epoch: 203 [2560/5999 (43%)]	Loss: 0.030129
Epoch: 203 [5120/5999 (85%)]	Loss: 0.065926
====> Epoch: 203 Average loss: 0.000366 
Epoch: 204 [0/5999 (0%)]	Loss: 0.059970
Epoch: 204 [2560/5999 (43%)]	Loss: 0.249521
Epoch: 204 [5120/5999 (85%)]	Loss: 0.039126
====> Epoch: 204 Average loss: 0.000337 
Epoch: 205 [0/5999 (0%)]	Loss: 0.022090
Epoch: 205 [2560/5999 (43%)]	Loss: 0.021322
Epoch: 205 [5120/5999 (85%)]	Loss: 0.030207
====> Epoch: 205 Average loss: 0.000480 
Epoch: 206 [0/5999 (0%)]	Loss: 0.123584
Epoch: 206 [2560/5999 (43%)]	Loss: 0.128177
Epoch: 206 [5120/5999 (85%)]	Loss: 0.073133
====> Epoch: 206 Average loss: 0.000544 
Epoch: 207 [0/5999 (0%)]	Loss: 0.028387
Epoch: 207 [2560/5999 (43%)]	Loss: 0.189683
Epoch: 207 [5120/5999 (85%)]	Loss: 0.022810
====> Epoch: 207 Average loss: 0.000621 
Epoch: 208 [0/5999 (0%)]	Loss: 0.032822
Epoch: 208 [2560/5999 (43%)]	Loss: 0.301578
Epoch: 208 [5120/5999 (85%)]	Loss: 0.040502
====> Epoch: 208 Average loss: 0.000544 
Epoch: 209 [0/5999 (0%)]	Loss: 0.054194
Epoch: 209 [2560/5999 (43%)]	Loss: 0.035484
Epoch: 209 [5120/5999 (85%)]	Loss: 0.086907
====> Epoch: 209 Average loss: 0.000459 
Epoch: 210 [0/5999 (0%)]	Loss: 0.027313
Epoch: 210 [2560/5999 (43%)]	Loss: 0.097261
Epoch: 210 [5120/5999 (85%)]	Loss: 0.031957
====> Epoch: 210 Average loss: 0.000579 
Epoch: 211 [0/5999 (0%)]	Loss: 0.104651
Epoch: 211 [2560/5999 (43%)]	Loss: 0.034790
Epoch: 211 [5120/5999 (85%)]	Loss: 0.078931
====> Epoch: 211 Average loss: 0.000510 
Epoch: 212 [0/5999 (0%)]	Loss: 0.041132
Epoch: 212 [2560/5999 (43%)]	Loss: 0.025308
Epoch: 212 [5120/5999 (85%)]	Loss: 0.072719
====> Epoch: 212 Average loss: 0.000350 
Epoch: 213 [0/5999 (0%)]	Loss: 0.019483
Epoch: 213 [2560/5999 (43%)]	Loss: 0.046661
Epoch: 213 [5120/5999 (85%)]	Loss: 0.075080
====> Epoch: 213 Average loss: 0.000431 
Epoch: 214 [0/5999 (0%)]	Loss: 0.029055
Epoch: 214 [2560/5999 (43%)]	Loss: 0.213058
Epoch: 214 [5120/5999 (85%)]	Loss: 0.090483
====> Epoch: 214 Average loss: 0.000741 
Epoch: 215 [0/5999 (0%)]	Loss: 0.038877
Epoch: 215 [2560/5999 (43%)]	Loss: 0.028405
Epoch: 215 [5120/5999 (85%)]	Loss: 0.030962
====> Epoch: 215 Average loss: 0.000749 
Epoch: 216 [0/5999 (0%)]	Loss: 0.088826
Epoch: 216 [2560/5999 (43%)]	Loss: 0.038862
saving model at:216,0.00012688433995936067
Epoch: 216 [5120/5999 (85%)]	Loss: 0.108986
====> Epoch: 216 Average loss: 0.000519 
Epoch: 217 [0/5999 (0%)]	Loss: 0.124247
Epoch: 217 [2560/5999 (43%)]	Loss: 0.048320
Epoch: 217 [5120/5999 (85%)]	Loss: 0.042881
====> Epoch: 217 Average loss: 0.000399 
Epoch: 218 [0/5999 (0%)]	Loss: 0.060914
Epoch: 218 [2560/5999 (43%)]	Loss: 0.015721
Epoch: 218 [5120/5999 (85%)]	Loss: 0.106700
====> Epoch: 218 Average loss: 0.000609 
Epoch: 219 [0/5999 (0%)]	Loss: 0.137607
Epoch: 219 [2560/5999 (43%)]	Loss: 0.055644
Epoch: 219 [5120/5999 (85%)]	Loss: 0.048607
====> Epoch: 219 Average loss: 0.000808 
Epoch: 220 [0/5999 (0%)]	Loss: 0.035974
Epoch: 220 [2560/5999 (43%)]	Loss: 0.091751
Epoch: 220 [5120/5999 (85%)]	Loss: 0.051880
====> Epoch: 220 Average loss: 0.000525 
Epoch: 221 [0/5999 (0%)]	Loss: 0.025185
Epoch: 221 [2560/5999 (43%)]	Loss: 0.101203
Epoch: 221 [5120/5999 (85%)]	Loss: 0.220087
====> Epoch: 221 Average loss: 0.000873 
Epoch: 222 [0/5999 (0%)]	Loss: 0.051694
Epoch: 222 [2560/5999 (43%)]	Loss: 0.118605
Epoch: 222 [5120/5999 (85%)]	Loss: 0.050920
====> Epoch: 222 Average loss: 0.000558 
Epoch: 223 [0/5999 (0%)]	Loss: 0.022104
Epoch: 223 [2560/5999 (43%)]	Loss: 0.032113
Epoch: 223 [5120/5999 (85%)]	Loss: 0.035960
====> Epoch: 223 Average loss: 0.000459 
Epoch: 224 [0/5999 (0%)]	Loss: 0.051769
Epoch: 224 [2560/5999 (43%)]	Loss: 0.056596
Epoch: 224 [5120/5999 (85%)]	Loss: 0.044917
====> Epoch: 224 Average loss: 0.000556 
Epoch: 225 [0/5999 (0%)]	Loss: 0.069754
Epoch: 225 [2560/5999 (43%)]	Loss: 0.121012
Epoch: 225 [5120/5999 (85%)]	Loss: 0.018639
====> Epoch: 225 Average loss: 0.000535 
Epoch: 226 [0/5999 (0%)]	Loss: 0.074656
Epoch: 226 [2560/5999 (43%)]	Loss: 0.111049
Epoch: 226 [5120/5999 (85%)]	Loss: 0.034979
====> Epoch: 226 Average loss: 0.000501 
Epoch: 227 [0/5999 (0%)]	Loss: 0.022236
Epoch: 227 [2560/5999 (43%)]	Loss: 0.034970
saving model at:227,0.00010940127947833389
Epoch: 227 [5120/5999 (85%)]	Loss: 0.085180
====> Epoch: 227 Average loss: 0.000389 
Epoch: 228 [0/5999 (0%)]	Loss: 0.046160
Epoch: 228 [2560/5999 (43%)]	Loss: 0.115750
Epoch: 228 [5120/5999 (85%)]	Loss: 0.069912
====> Epoch: 228 Average loss: 0.000496 
Epoch: 229 [0/5999 (0%)]	Loss: 0.042653
Epoch: 229 [2560/5999 (43%)]	Loss: 0.022514
Epoch: 229 [5120/5999 (85%)]	Loss: 0.052468
====> Epoch: 229 Average loss: 0.000430 
Epoch: 230 [0/5999 (0%)]	Loss: 0.091425
Epoch: 230 [2560/5999 (43%)]	Loss: 0.122857
Epoch: 230 [5120/5999 (85%)]	Loss: 0.143807
====> Epoch: 230 Average loss: 0.000723 
Epoch: 231 [0/5999 (0%)]	Loss: 0.098936
Epoch: 231 [2560/5999 (43%)]	Loss: 0.040027
Epoch: 231 [5120/5999 (85%)]	Loss: 0.081656
====> Epoch: 231 Average loss: 0.000591 
Epoch: 232 [0/5999 (0%)]	Loss: 0.065183
Epoch: 232 [2560/5999 (43%)]	Loss: 0.023806
Epoch: 232 [5120/5999 (85%)]	Loss: 0.195276
====> Epoch: 232 Average loss: 0.000511 
Epoch: 233 [0/5999 (0%)]	Loss: 0.052228
Epoch: 233 [2560/5999 (43%)]	Loss: 0.030785
Epoch: 233 [5120/5999 (85%)]	Loss: 0.063666
====> Epoch: 233 Average loss: 0.000414 
Epoch: 234 [0/5999 (0%)]	Loss: 0.033314
Epoch: 234 [2560/5999 (43%)]	Loss: 0.047725
Epoch: 234 [5120/5999 (85%)]	Loss: 0.112927
====> Epoch: 234 Average loss: 0.000504 
Epoch: 235 [0/5999 (0%)]	Loss: 0.080319
Epoch: 235 [2560/5999 (43%)]	Loss: 0.157873
Epoch: 235 [5120/5999 (85%)]	Loss: 0.052031
====> Epoch: 235 Average loss: 0.000875 
Epoch: 236 [0/5999 (0%)]	Loss: 0.022234
Epoch: 236 [2560/5999 (43%)]	Loss: 0.048687
Epoch: 236 [5120/5999 (85%)]	Loss: 0.044140
====> Epoch: 236 Average loss: 0.000440 
Epoch: 237 [0/5999 (0%)]	Loss: 0.063640
Epoch: 237 [2560/5999 (43%)]	Loss: 0.023144
Epoch: 237 [5120/5999 (85%)]	Loss: 0.042662
====> Epoch: 237 Average loss: 0.000466 
Epoch: 238 [0/5999 (0%)]	Loss: 0.096597
Epoch: 238 [2560/5999 (43%)]	Loss: 0.038514
Epoch: 238 [5120/5999 (85%)]	Loss: 0.335457
====> Epoch: 238 Average loss: 0.000567 
Epoch: 239 [0/5999 (0%)]	Loss: 0.042014
Epoch: 239 [2560/5999 (43%)]	Loss: 0.071179
Epoch: 239 [5120/5999 (85%)]	Loss: 0.029107
====> Epoch: 239 Average loss: 0.000374 
Epoch: 240 [0/5999 (0%)]	Loss: 0.077588
Epoch: 240 [2560/5999 (43%)]	Loss: 0.099205
Epoch: 240 [5120/5999 (85%)]	Loss: 0.036205
====> Epoch: 240 Average loss: 0.000771 
Epoch: 241 [0/5999 (0%)]	Loss: 0.094507
Epoch: 241 [2560/5999 (43%)]	Loss: 0.069759
Epoch: 241 [5120/5999 (85%)]	Loss: 0.052057
====> Epoch: 241 Average loss: 0.000304 
Epoch: 242 [0/5999 (0%)]	Loss: 0.077731
Epoch: 242 [2560/5999 (43%)]	Loss: 0.040542
Epoch: 242 [5120/5999 (85%)]	Loss: 0.046389
====> Epoch: 242 Average loss: 0.000328 
Epoch: 243 [0/5999 (0%)]	Loss: 0.071106
Epoch: 243 [2560/5999 (43%)]	Loss: 0.072085
Epoch: 243 [5120/5999 (85%)]	Loss: 0.152070
====> Epoch: 243 Average loss: 0.000440 
Epoch: 244 [0/5999 (0%)]	Loss: 0.101975
Epoch: 244 [2560/5999 (43%)]	Loss: 0.080160
Epoch: 244 [5120/5999 (85%)]	Loss: 0.253888
====> Epoch: 244 Average loss: 0.000591 
Epoch: 245 [0/5999 (0%)]	Loss: 0.100586
Epoch: 245 [2560/5999 (43%)]	Loss: 0.057353
Epoch: 245 [5120/5999 (85%)]	Loss: 0.025638
====> Epoch: 245 Average loss: 0.000508 
Epoch: 246 [0/5999 (0%)]	Loss: 0.042528
Epoch: 246 [2560/5999 (43%)]	Loss: 0.113171
Epoch: 246 [5120/5999 (85%)]	Loss: 0.044598
====> Epoch: 246 Average loss: 0.000485 
Epoch: 247 [0/5999 (0%)]	Loss: 0.022848
Epoch: 247 [2560/5999 (43%)]	Loss: 0.115050
Epoch: 247 [5120/5999 (85%)]	Loss: 0.031180
====> Epoch: 247 Average loss: 0.000323 
Epoch: 248 [0/5999 (0%)]	Loss: 0.030559
Epoch: 248 [2560/5999 (43%)]	Loss: 0.046970
Epoch: 248 [5120/5999 (85%)]	Loss: 0.105581
====> Epoch: 248 Average loss: 0.000412 
Epoch: 249 [0/5999 (0%)]	Loss: 0.063013
Epoch: 249 [2560/5999 (43%)]	Loss: 0.024886
Epoch: 249 [5120/5999 (85%)]	Loss: 0.020159
saving model at:249,0.00010797457851003856
====> Epoch: 249 Average loss: 0.000403 
Epoch: 250 [0/5999 (0%)]	Loss: 0.018784
Epoch: 250 [2560/5999 (43%)]	Loss: 0.071002
Epoch: 250 [5120/5999 (85%)]	Loss: 0.024260
====> Epoch: 250 Average loss: 0.000419 
Epoch: 251 [0/5999 (0%)]	Loss: 0.052078
Epoch: 251 [2560/5999 (43%)]	Loss: 0.052579
saving model at:251,0.00010312384576536715
Epoch: 251 [5120/5999 (85%)]	Loss: 0.024894
====> Epoch: 251 Average loss: 0.000375 
Epoch: 252 [0/5999 (0%)]	Loss: 0.025169
Epoch: 252 [2560/5999 (43%)]	Loss: 0.069477
Epoch: 252 [5120/5999 (85%)]	Loss: 0.021574
====> Epoch: 252 Average loss: 0.000444 
Epoch: 253 [0/5999 (0%)]	Loss: 0.133474
Epoch: 253 [2560/5999 (43%)]	Loss: 0.125677
Epoch: 253 [5120/5999 (85%)]	Loss: 0.053186
====> Epoch: 253 Average loss: 0.000428 
Epoch: 254 [0/5999 (0%)]	Loss: 0.025376
Epoch: 254 [2560/5999 (43%)]	Loss: 0.037408
Epoch: 254 [5120/5999 (85%)]	Loss: 0.016764
====> Epoch: 254 Average loss: 0.000427 
Epoch: 255 [0/5999 (0%)]	Loss: 0.031543
Epoch: 255 [2560/5999 (43%)]	Loss: 0.015779
saving model at:255,0.00010290682833874598
Epoch: 255 [5120/5999 (85%)]	Loss: 0.053624
====> Epoch: 255 Average loss: 0.000343 
Epoch: 256 [0/5999 (0%)]	Loss: 0.031550
Epoch: 256 [2560/5999 (43%)]	Loss: 0.017938
Epoch: 256 [5120/5999 (85%)]	Loss: 0.079332
====> Epoch: 256 Average loss: 0.000407 
Epoch: 257 [0/5999 (0%)]	Loss: 0.164356
Epoch: 257 [2560/5999 (43%)]	Loss: 0.102220
Epoch: 257 [5120/5999 (85%)]	Loss: 0.050240
====> Epoch: 257 Average loss: 0.000607 
Epoch: 258 [0/5999 (0%)]	Loss: 0.222450
Epoch: 258 [2560/5999 (43%)]	Loss: 0.055891
Epoch: 258 [5120/5999 (85%)]	Loss: 0.061466
====> Epoch: 258 Average loss: 0.000478 
Epoch: 259 [0/5999 (0%)]	Loss: 0.047975
Epoch: 259 [2560/5999 (43%)]	Loss: 0.071030
Epoch: 259 [5120/5999 (85%)]	Loss: 0.056289
====> Epoch: 259 Average loss: 0.000485 
Epoch: 260 [0/5999 (0%)]	Loss: 0.030844
Epoch: 260 [2560/5999 (43%)]	Loss: 0.019531
Epoch: 260 [5120/5999 (85%)]	Loss: 0.025359
====> Epoch: 260 Average loss: 0.000453 
Epoch: 261 [0/5999 (0%)]	Loss: 0.055955
Epoch: 261 [2560/5999 (43%)]	Loss: 0.070980
Epoch: 261 [5120/5999 (85%)]	Loss: 0.036881
====> Epoch: 261 Average loss: 0.000305 
Epoch: 262 [0/5999 (0%)]	Loss: 0.016829
Epoch: 262 [2560/5999 (43%)]	Loss: 0.034797
Epoch: 262 [5120/5999 (85%)]	Loss: 0.035098
====> Epoch: 262 Average loss: 0.000386 
Epoch: 263 [0/5999 (0%)]	Loss: 0.030734
Epoch: 263 [2560/5999 (43%)]	Loss: 0.145571
Epoch: 263 [5120/5999 (85%)]	Loss: 0.025610
====> Epoch: 263 Average loss: 0.000366 
Epoch: 264 [0/5999 (0%)]	Loss: 0.027729
Epoch: 264 [2560/5999 (43%)]	Loss: 0.020824
saving model at:264,0.00010271199163980782
Epoch: 264 [5120/5999 (85%)]	Loss: 0.022841
====> Epoch: 264 Average loss: 0.000327 
Epoch: 265 [0/5999 (0%)]	Loss: 0.107096
Epoch: 265 [2560/5999 (43%)]	Loss: 0.020726
Epoch: 265 [5120/5999 (85%)]	Loss: 0.069168
====> Epoch: 265 Average loss: 0.000498 
Epoch: 266 [0/5999 (0%)]	Loss: 0.024990
Epoch: 266 [2560/5999 (43%)]	Loss: 0.059223
Epoch: 266 [5120/5999 (85%)]	Loss: 0.032857
====> Epoch: 266 Average loss: 0.000403 
Epoch: 267 [0/5999 (0%)]	Loss: 0.017352
Epoch: 267 [2560/5999 (43%)]	Loss: 0.047411
Epoch: 267 [5120/5999 (85%)]	Loss: 0.016530
====> Epoch: 267 Average loss: 0.000406 
Epoch: 268 [0/5999 (0%)]	Loss: 0.018518
Epoch: 268 [2560/5999 (43%)]	Loss: 0.021937
Epoch: 268 [5120/5999 (85%)]	Loss: 0.110249
====> Epoch: 268 Average loss: 0.000283 
Epoch: 269 [0/5999 (0%)]	Loss: 0.040614
Epoch: 269 [2560/5999 (43%)]	Loss: 0.057933
Epoch: 269 [5120/5999 (85%)]	Loss: 0.056826
====> Epoch: 269 Average loss: 0.000395 
Epoch: 270 [0/5999 (0%)]	Loss: 0.019741
Epoch: 270 [2560/5999 (43%)]	Loss: 0.099878
Epoch: 270 [5120/5999 (85%)]	Loss: 0.116361
====> Epoch: 270 Average loss: 0.000499 
Epoch: 271 [0/5999 (0%)]	Loss: 0.049453
Epoch: 271 [2560/5999 (43%)]	Loss: 0.071144
Epoch: 271 [5120/5999 (85%)]	Loss: 0.042441
====> Epoch: 271 Average loss: 0.000384 
Epoch: 272 [0/5999 (0%)]	Loss: 0.091449
Epoch: 272 [2560/5999 (43%)]	Loss: 0.034814
Epoch: 272 [5120/5999 (85%)]	Loss: 0.044207
====> Epoch: 272 Average loss: 0.000418 
Epoch: 273 [0/5999 (0%)]	Loss: 0.056305
Epoch: 273 [2560/5999 (43%)]	Loss: 0.121407
Epoch: 273 [5120/5999 (85%)]	Loss: 0.017233
====> Epoch: 273 Average loss: 0.000354 
Epoch: 274 [0/5999 (0%)]	Loss: 0.037913
Epoch: 274 [2560/5999 (43%)]	Loss: 0.015448
saving model at:274,0.00010069513058988378
Epoch: 274 [5120/5999 (85%)]	Loss: 0.047469
====> Epoch: 274 Average loss: 0.000347 
Epoch: 275 [0/5999 (0%)]	Loss: 0.068717
Epoch: 275 [2560/5999 (43%)]	Loss: 0.024752
Epoch: 275 [5120/5999 (85%)]	Loss: 0.089712
====> Epoch: 275 Average loss: 0.000382 
Epoch: 276 [0/5999 (0%)]	Loss: 0.055564
Epoch: 276 [2560/5999 (43%)]	Loss: 0.254521
Epoch: 276 [5120/5999 (85%)]	Loss: 0.074548
====> Epoch: 276 Average loss: 0.000469 
Epoch: 277 [0/5999 (0%)]	Loss: 0.023327
Epoch: 277 [2560/5999 (43%)]	Loss: 0.020201
Epoch: 277 [5120/5999 (85%)]	Loss: 0.023517
====> Epoch: 277 Average loss: 0.000404 
Epoch: 278 [0/5999 (0%)]	Loss: 0.038779
Epoch: 278 [2560/5999 (43%)]	Loss: 0.037364
Epoch: 278 [5120/5999 (85%)]	Loss: 0.142088
====> Epoch: 278 Average loss: 0.000440 
Epoch: 279 [0/5999 (0%)]	Loss: 0.040873
Epoch: 279 [2560/5999 (43%)]	Loss: 0.027449
Epoch: 279 [5120/5999 (85%)]	Loss: 0.021080
====> Epoch: 279 Average loss: 0.000358 
Epoch: 280 [0/5999 (0%)]	Loss: 0.021552
Epoch: 280 [2560/5999 (43%)]	Loss: 0.026950
Epoch: 280 [5120/5999 (85%)]	Loss: 0.094725
====> Epoch: 280 Average loss: 0.000376 
Epoch: 281 [0/5999 (0%)]	Loss: 0.073201
Epoch: 281 [2560/5999 (43%)]	Loss: 0.095738
Epoch: 281 [5120/5999 (85%)]	Loss: 0.064553
====> Epoch: 281 Average loss: 0.000636 
Epoch: 282 [0/5999 (0%)]	Loss: 0.042320
Epoch: 282 [2560/5999 (43%)]	Loss: 0.022132
Epoch: 282 [5120/5999 (85%)]	Loss: 0.171138
====> Epoch: 282 Average loss: 0.000473 
Epoch: 283 [0/5999 (0%)]	Loss: 0.034364
Epoch: 283 [2560/5999 (43%)]	Loss: 0.042952
Epoch: 283 [5120/5999 (85%)]	Loss: 0.049530
====> Epoch: 283 Average loss: 0.000423 
Epoch: 284 [0/5999 (0%)]	Loss: 0.036755
Epoch: 284 [2560/5999 (43%)]	Loss: 0.025984
Epoch: 284 [5120/5999 (85%)]	Loss: 0.078278
====> Epoch: 284 Average loss: 0.000477 
Epoch: 285 [0/5999 (0%)]	Loss: 0.120153
Epoch: 285 [2560/5999 (43%)]	Loss: 0.028564
Epoch: 285 [5120/5999 (85%)]	Loss: 0.094759
====> Epoch: 285 Average loss: 0.000414 
Epoch: 286 [0/5999 (0%)]	Loss: 0.020153
Epoch: 286 [2560/5999 (43%)]	Loss: 0.032886
Epoch: 286 [5120/5999 (85%)]	Loss: 0.038602
====> Epoch: 286 Average loss: 0.000443 
Epoch: 287 [0/5999 (0%)]	Loss: 0.131591
Epoch: 287 [2560/5999 (43%)]	Loss: 0.277120
Epoch: 287 [5120/5999 (85%)]	Loss: 0.068427
====> Epoch: 287 Average loss: 0.000703 
Epoch: 288 [0/5999 (0%)]	Loss: 0.044761
Epoch: 288 [2560/5999 (43%)]	Loss: 0.218314
Epoch: 288 [5120/5999 (85%)]	Loss: 0.035019
====> Epoch: 288 Average loss: 0.000629 
Epoch: 289 [0/5999 (0%)]	Loss: 0.027879
Epoch: 289 [2560/5999 (43%)]	Loss: 0.032654
Epoch: 289 [5120/5999 (85%)]	Loss: 0.233299
====> Epoch: 289 Average loss: 0.000458 
Epoch: 290 [0/5999 (0%)]	Loss: 0.047425
Epoch: 290 [2560/5999 (43%)]	Loss: 0.016927
Epoch: 290 [5120/5999 (85%)]	Loss: 0.092395
====> Epoch: 290 Average loss: 0.000623 
Epoch: 291 [0/5999 (0%)]	Loss: 0.037371
Epoch: 291 [2560/5999 (43%)]	Loss: 0.049119
Epoch: 291 [5120/5999 (85%)]	Loss: 0.067009
====> Epoch: 291 Average loss: 0.000560 
Epoch: 292 [0/5999 (0%)]	Loss: 0.046991
Epoch: 292 [2560/5999 (43%)]	Loss: 0.036518
Epoch: 292 [5120/5999 (85%)]	Loss: 0.069886
====> Epoch: 292 Average loss: 0.000458 
Epoch: 293 [0/5999 (0%)]	Loss: 0.053527
Epoch: 293 [2560/5999 (43%)]	Loss: 0.029075
Epoch: 293 [5120/5999 (85%)]	Loss: 0.131834
====> Epoch: 293 Average loss: 0.000438 
Epoch: 294 [0/5999 (0%)]	Loss: 0.143078
Epoch: 294 [2560/5999 (43%)]	Loss: 0.066672
Epoch: 294 [5120/5999 (85%)]	Loss: 0.074952
====> Epoch: 294 Average loss: 0.000528 
Epoch: 295 [0/5999 (0%)]	Loss: 0.034698
Epoch: 295 [2560/5999 (43%)]	Loss: 0.020240
Epoch: 295 [5120/5999 (85%)]	Loss: 0.025575
====> Epoch: 295 Average loss: 0.000354 
Epoch: 296 [0/5999 (0%)]	Loss: 0.039514
Epoch: 296 [2560/5999 (43%)]	Loss: 0.021851
Epoch: 296 [5120/5999 (85%)]	Loss: 0.039934
====> Epoch: 296 Average loss: 0.000365 
Epoch: 297 [0/5999 (0%)]	Loss: 0.121763
Epoch: 297 [2560/5999 (43%)]	Loss: 0.043512
Epoch: 297 [5120/5999 (85%)]	Loss: 0.016207
saving model at:297,0.00010003304935526103
====> Epoch: 297 Average loss: 0.000309 
Epoch: 298 [0/5999 (0%)]	Loss: 0.020968
Epoch: 298 [2560/5999 (43%)]	Loss: 0.019002
saving model at:298,8.343066822271794e-05
Epoch: 298 [5120/5999 (85%)]	Loss: 0.038260
====> Epoch: 298 Average loss: 0.000256 
Epoch: 299 [0/5999 (0%)]	Loss: 0.014104
Epoch: 299 [2560/5999 (43%)]	Loss: 0.044306
Epoch: 299 [5120/5999 (85%)]	Loss: 0.018264
====> Epoch: 299 Average loss: 0.000296 
Epoch: 300 [0/5999 (0%)]	Loss: 0.032375
Epoch: 300 [2560/5999 (43%)]	Loss: 0.024050
Epoch: 300 [5120/5999 (85%)]	Loss: 0.019210
====> Epoch: 300 Average loss: 0.000569 
Epoch: 301 [0/5999 (0%)]	Loss: 0.025828
Epoch: 301 [2560/5999 (43%)]	Loss: 0.020243
Epoch: 301 [5120/5999 (85%)]	Loss: 0.030199
====> Epoch: 301 Average loss: 0.000297 
Epoch: 302 [0/5999 (0%)]	Loss: 0.029634
Epoch: 302 [2560/5999 (43%)]	Loss: 0.029172
Epoch: 302 [5120/5999 (85%)]	Loss: 0.018172
====> Epoch: 302 Average loss: 0.000344 
Epoch: 303 [0/5999 (0%)]	Loss: 0.027673
Epoch: 303 [2560/5999 (43%)]	Loss: 0.023272
Epoch: 303 [5120/5999 (85%)]	Loss: 0.024153
====> Epoch: 303 Average loss: 0.000329 
Epoch: 304 [0/5999 (0%)]	Loss: 0.082410
Epoch: 304 [2560/5999 (43%)]	Loss: 0.024532
Epoch: 304 [5120/5999 (85%)]	Loss: 0.039495
====> Epoch: 304 Average loss: 0.000375 
Epoch: 305 [0/5999 (0%)]	Loss: 0.026860
Epoch: 305 [2560/5999 (43%)]	Loss: 0.040467
Epoch: 305 [5120/5999 (85%)]	Loss: 0.024608
====> Epoch: 305 Average loss: 0.000324 
Epoch: 306 [0/5999 (0%)]	Loss: 0.023351
Epoch: 306 [2560/5999 (43%)]	Loss: 0.069517
Epoch: 306 [5120/5999 (85%)]	Loss: 0.021370
====> Epoch: 306 Average loss: 0.000298 
Epoch: 307 [0/5999 (0%)]	Loss: 0.023300
Epoch: 307 [2560/5999 (43%)]	Loss: 0.065308
Epoch: 307 [5120/5999 (85%)]	Loss: 0.024836
====> Epoch: 307 Average loss: 0.000341 
Epoch: 308 [0/5999 (0%)]	Loss: 0.101682
Epoch: 308 [2560/5999 (43%)]	Loss: 0.102519
Epoch: 308 [5120/5999 (85%)]	Loss: 0.026303
====> Epoch: 308 Average loss: 0.000398 
Epoch: 309 [0/5999 (0%)]	Loss: 0.014718
Epoch: 309 [2560/5999 (43%)]	Loss: 0.123065
Epoch: 309 [5120/5999 (85%)]	Loss: 0.059822
====> Epoch: 309 Average loss: 0.000390 
Epoch: 310 [0/5999 (0%)]	Loss: 0.033596
Epoch: 310 [2560/5999 (43%)]	Loss: 0.016992
Epoch: 310 [5120/5999 (85%)]	Loss: 0.064469
====> Epoch: 310 Average loss: 0.000298 
Epoch: 311 [0/5999 (0%)]	Loss: 0.038467
Epoch: 311 [2560/5999 (43%)]	Loss: 0.038009
Epoch: 311 [5120/5999 (85%)]	Loss: 0.031780
====> Epoch: 311 Average loss: 0.000310 
Epoch: 312 [0/5999 (0%)]	Loss: 0.058438
Epoch: 312 [2560/5999 (43%)]	Loss: 0.028404
Epoch: 312 [5120/5999 (85%)]	Loss: 0.027265
====> Epoch: 312 Average loss: 0.000354 
Epoch: 313 [0/5999 (0%)]	Loss: 0.019750
Epoch: 313 [2560/5999 (43%)]	Loss: 0.116466
Epoch: 313 [5120/5999 (85%)]	Loss: 0.009948
saving model at:313,7.067829807056114e-05
====> Epoch: 313 Average loss: 0.000273 
Epoch: 314 [0/5999 (0%)]	Loss: 0.029980
Epoch: 314 [2560/5999 (43%)]	Loss: 0.109334
Epoch: 314 [5120/5999 (85%)]	Loss: 0.024973
====> Epoch: 314 Average loss: 0.000318 
Epoch: 315 [0/5999 (0%)]	Loss: 0.046195
Epoch: 315 [2560/5999 (43%)]	Loss: 0.115394
Epoch: 315 [5120/5999 (85%)]	Loss: 0.023215
====> Epoch: 315 Average loss: 0.000380 
Epoch: 316 [0/5999 (0%)]	Loss: 0.043689
Epoch: 316 [2560/5999 (43%)]	Loss: 0.060253
Epoch: 316 [5120/5999 (85%)]	Loss: 0.018811
====> Epoch: 316 Average loss: 0.000442 
Epoch: 317 [0/5999 (0%)]	Loss: 0.025368
Epoch: 317 [2560/5999 (43%)]	Loss: 0.040657
Epoch: 317 [5120/5999 (85%)]	Loss: 0.053756
====> Epoch: 317 Average loss: 0.000352 
Epoch: 318 [0/5999 (0%)]	Loss: 0.013628
Epoch: 318 [2560/5999 (43%)]	Loss: 0.061957
Epoch: 318 [5120/5999 (85%)]	Loss: 0.054540
====> Epoch: 318 Average loss: 0.000390 
Epoch: 319 [0/5999 (0%)]	Loss: 0.022645
Epoch: 319 [2560/5999 (43%)]	Loss: 0.024852
Epoch: 319 [5120/5999 (85%)]	Loss: 0.043722
====> Epoch: 319 Average loss: 0.000448 
Epoch: 320 [0/5999 (0%)]	Loss: 0.078149
Epoch: 320 [2560/5999 (43%)]	Loss: 0.031386
Epoch: 320 [5120/5999 (85%)]	Loss: 0.038147
====> Epoch: 320 Average loss: 0.000331 
Epoch: 321 [0/5999 (0%)]	Loss: 0.167940
Epoch: 321 [2560/5999 (43%)]	Loss: 0.022712
Epoch: 321 [5120/5999 (85%)]	Loss: 0.055281
====> Epoch: 321 Average loss: 0.000329 
Epoch: 322 [0/5999 (0%)]	Loss: 0.045124
Epoch: 322 [2560/5999 (43%)]	Loss: 0.098796
Epoch: 322 [5120/5999 (85%)]	Loss: 0.031932
====> Epoch: 322 Average loss: 0.000416 
Epoch: 323 [0/5999 (0%)]	Loss: 0.094112
Epoch: 323 [2560/5999 (43%)]	Loss: 0.038609
Epoch: 323 [5120/5999 (85%)]	Loss: 0.033171
====> Epoch: 323 Average loss: 0.000405 
Epoch: 324 [0/5999 (0%)]	Loss: 0.070950
Epoch: 324 [2560/5999 (43%)]	Loss: 0.029861
Epoch: 324 [5120/5999 (85%)]	Loss: 0.064370
====> Epoch: 324 Average loss: 0.000302 
Epoch: 325 [0/5999 (0%)]	Loss: 0.075845
Epoch: 325 [2560/5999 (43%)]	Loss: 0.036139
Epoch: 325 [5120/5999 (85%)]	Loss: 0.088646
====> Epoch: 325 Average loss: 0.000392 
Epoch: 326 [0/5999 (0%)]	Loss: 0.047673
Epoch: 326 [2560/5999 (43%)]	Loss: 0.031547
Epoch: 326 [5120/5999 (85%)]	Loss: 0.019581
====> Epoch: 326 Average loss: 0.000337 
Epoch: 327 [0/5999 (0%)]	Loss: 0.033998
Epoch: 327 [2560/5999 (43%)]	Loss: 0.087413
Epoch: 327 [5120/5999 (85%)]	Loss: 0.055045
====> Epoch: 327 Average loss: 0.000614 
Epoch: 328 [0/5999 (0%)]	Loss: 0.230030
Epoch: 328 [2560/5999 (43%)]	Loss: 0.092923
Epoch: 328 [5120/5999 (85%)]	Loss: 0.072842
====> Epoch: 328 Average loss: 0.000471 
Epoch: 329 [0/5999 (0%)]	Loss: 0.179123
Epoch: 329 [2560/5999 (43%)]	Loss: 0.036865
Epoch: 329 [5120/5999 (85%)]	Loss: 0.204102
====> Epoch: 329 Average loss: 0.000411 
Epoch: 330 [0/5999 (0%)]	Loss: 0.016576
Epoch: 330 [2560/5999 (43%)]	Loss: 0.039616
Epoch: 330 [5120/5999 (85%)]	Loss: 0.081354
====> Epoch: 330 Average loss: 0.000444 
Epoch: 331 [0/5999 (0%)]	Loss: 0.157716
Epoch: 331 [2560/5999 (43%)]	Loss: 0.034701
Epoch: 331 [5120/5999 (85%)]	Loss: 0.031060
====> Epoch: 331 Average loss: 0.000443 
Epoch: 332 [0/5999 (0%)]	Loss: 0.062932
Epoch: 332 [2560/5999 (43%)]	Loss: 0.024722
Epoch: 332 [5120/5999 (85%)]	Loss: 0.047932
====> Epoch: 332 Average loss: 0.000381 
Epoch: 333 [0/5999 (0%)]	Loss: 0.025023
Epoch: 333 [2560/5999 (43%)]	Loss: 0.021379
Epoch: 333 [5120/5999 (85%)]	Loss: 0.020130
====> Epoch: 333 Average loss: 0.000318 
Epoch: 334 [0/5999 (0%)]	Loss: 0.070415
Epoch: 334 [2560/5999 (43%)]	Loss: 0.067952
Epoch: 334 [5120/5999 (85%)]	Loss: 0.039004
====> Epoch: 334 Average loss: 0.000339 
Epoch: 335 [0/5999 (0%)]	Loss: 0.066795
Epoch: 335 [2560/5999 (43%)]	Loss: 0.035102
Epoch: 335 [5120/5999 (85%)]	Loss: 0.073327
====> Epoch: 335 Average loss: 0.000311 
Epoch: 336 [0/5999 (0%)]	Loss: 0.018717
Epoch: 336 [2560/5999 (43%)]	Loss: 0.250111
Epoch: 336 [5120/5999 (85%)]	Loss: 0.163981
====> Epoch: 336 Average loss: 0.000571 
Epoch: 337 [0/5999 (0%)]	Loss: 0.017484
Epoch: 337 [2560/5999 (43%)]	Loss: 0.069148
Epoch: 337 [5120/5999 (85%)]	Loss: 0.052861
====> Epoch: 337 Average loss: 0.000443 
Epoch: 338 [0/5999 (0%)]	Loss: 0.047696
Epoch: 338 [2560/5999 (43%)]	Loss: 0.020481
Epoch: 338 [5120/5999 (85%)]	Loss: 0.038950
====> Epoch: 338 Average loss: 0.000357 
Epoch: 339 [0/5999 (0%)]	Loss: 0.059558
Epoch: 339 [2560/5999 (43%)]	Loss: 0.086904
Epoch: 339 [5120/5999 (85%)]	Loss: 0.015232
====> Epoch: 339 Average loss: 0.000318 
Epoch: 340 [0/5999 (0%)]	Loss: 0.055406
Epoch: 340 [2560/5999 (43%)]	Loss: 0.244022
Epoch: 340 [5120/5999 (85%)]	Loss: 0.069119
====> Epoch: 340 Average loss: 0.000465 
Epoch: 341 [0/5999 (0%)]	Loss: 0.040669
Epoch: 341 [2560/5999 (43%)]	Loss: 0.041766
Epoch: 341 [5120/5999 (85%)]	Loss: 0.019384
====> Epoch: 341 Average loss: 0.000352 
Epoch: 342 [0/5999 (0%)]	Loss: 0.046506
Epoch: 342 [2560/5999 (43%)]	Loss: 0.045155
Epoch: 342 [5120/5999 (85%)]	Loss: 0.016259
====> Epoch: 342 Average loss: 0.000368 
Epoch: 343 [0/5999 (0%)]	Loss: 0.037329
Epoch: 343 [2560/5999 (43%)]	Loss: 0.014145
Epoch: 343 [5120/5999 (85%)]	Loss: 0.135657
====> Epoch: 343 Average loss: 0.000358 
Epoch: 344 [0/5999 (0%)]	Loss: 0.022976
Epoch: 344 [2560/5999 (43%)]	Loss: 0.019121
Epoch: 344 [5120/5999 (85%)]	Loss: 0.018508
====> Epoch: 344 Average loss: 0.000265 
Epoch: 345 [0/5999 (0%)]	Loss: 0.026235
Epoch: 345 [2560/5999 (43%)]	Loss: 0.037474
Epoch: 345 [5120/5999 (85%)]	Loss: 0.029578
====> Epoch: 345 Average loss: 0.000301 
Epoch: 346 [0/5999 (0%)]	Loss: 0.058317
Epoch: 346 [2560/5999 (43%)]	Loss: 0.019650
Epoch: 346 [5120/5999 (85%)]	Loss: 0.038143
====> Epoch: 346 Average loss: 0.000251 
Epoch: 347 [0/5999 (0%)]	Loss: 0.072396
Epoch: 347 [2560/5999 (43%)]	Loss: 0.069734
Epoch: 347 [5120/5999 (85%)]	Loss: 0.013300
====> Epoch: 347 Average loss: 0.000291 
Epoch: 348 [0/5999 (0%)]	Loss: 0.016075
Epoch: 348 [2560/5999 (43%)]	Loss: 0.081478
Epoch: 348 [5120/5999 (85%)]	Loss: 0.029738
====> Epoch: 348 Average loss: 0.000298 
Epoch: 349 [0/5999 (0%)]	Loss: 0.044781
Epoch: 349 [2560/5999 (43%)]	Loss: 0.017247
Epoch: 349 [5120/5999 (85%)]	Loss: 0.045301
====> Epoch: 349 Average loss: 0.000343 
Epoch: 350 [0/5999 (0%)]	Loss: 0.091635
Epoch: 350 [2560/5999 (43%)]	Loss: 0.035863
Epoch: 350 [5120/5999 (85%)]	Loss: 0.075723
====> Epoch: 350 Average loss: 0.000497 
Epoch: 351 [0/5999 (0%)]	Loss: 0.125469
Epoch: 351 [2560/5999 (43%)]	Loss: 0.014196
Epoch: 351 [5120/5999 (85%)]	Loss: 0.024788
====> Epoch: 351 Average loss: 0.000291 
Epoch: 352 [0/5999 (0%)]	Loss: 0.029841
Epoch: 352 [2560/5999 (43%)]	Loss: 0.039146
Epoch: 352 [5120/5999 (85%)]	Loss: 0.042092
====> Epoch: 352 Average loss: 0.000389 
Epoch: 353 [0/5999 (0%)]	Loss: 0.024199
Epoch: 353 [2560/5999 (43%)]	Loss: 0.033368
Epoch: 353 [5120/5999 (85%)]	Loss: 0.017844
====> Epoch: 353 Average loss: 0.000334 
Epoch: 354 [0/5999 (0%)]	Loss: 0.048210
Epoch: 354 [2560/5999 (43%)]	Loss: 0.034168
Epoch: 354 [5120/5999 (85%)]	Loss: 0.037259
====> Epoch: 354 Average loss: 0.000315 
Epoch: 355 [0/5999 (0%)]	Loss: 0.091514
Epoch: 355 [2560/5999 (43%)]	Loss: 0.023223
Epoch: 355 [5120/5999 (85%)]	Loss: 0.023761
====> Epoch: 355 Average loss: 0.000265 
Epoch: 356 [0/5999 (0%)]	Loss: 0.056660
Epoch: 356 [2560/5999 (43%)]	Loss: 0.030329
Epoch: 356 [5120/5999 (85%)]	Loss: 0.013338
====> Epoch: 356 Average loss: 0.000254 
Epoch: 357 [0/5999 (0%)]	Loss: 0.029050
Epoch: 357 [2560/5999 (43%)]	Loss: 0.034523
Epoch: 357 [5120/5999 (85%)]	Loss: 0.033660
====> Epoch: 357 Average loss: 0.000310 
Epoch: 358 [0/5999 (0%)]	Loss: 0.015216
Epoch: 358 [2560/5999 (43%)]	Loss: 0.018674
Epoch: 358 [5120/5999 (85%)]	Loss: 0.147555
====> Epoch: 358 Average loss: 0.000407 
Epoch: 359 [0/5999 (0%)]	Loss: 0.107374
Epoch: 359 [2560/5999 (43%)]	Loss: 0.040435
Epoch: 359 [5120/5999 (85%)]	Loss: 0.031724
====> Epoch: 359 Average loss: 0.000368 
Epoch: 360 [0/5999 (0%)]	Loss: 0.040569
Epoch: 360 [2560/5999 (43%)]	Loss: 0.062535
Epoch: 360 [5120/5999 (85%)]	Loss: 0.052274
====> Epoch: 360 Average loss: 0.000381 
Epoch: 361 [0/5999 (0%)]	Loss: 0.024150
Epoch: 361 [2560/5999 (43%)]	Loss: 0.071461
Epoch: 361 [5120/5999 (85%)]	Loss: 0.052999
====> Epoch: 361 Average loss: 0.000601 
Epoch: 362 [0/5999 (0%)]	Loss: 0.029548
Epoch: 362 [2560/5999 (43%)]	Loss: 0.018081
Epoch: 362 [5120/5999 (85%)]	Loss: 0.143696
====> Epoch: 362 Average loss: 0.000432 
Epoch: 363 [0/5999 (0%)]	Loss: 0.024998
Epoch: 363 [2560/5999 (43%)]	Loss: 0.017256
Epoch: 363 [5120/5999 (85%)]	Loss: 0.068945
====> Epoch: 363 Average loss: 0.000375 
Epoch: 364 [0/5999 (0%)]	Loss: 0.021990
Epoch: 364 [2560/5999 (43%)]	Loss: 0.016542
Epoch: 364 [5120/5999 (85%)]	Loss: 0.028941
====> Epoch: 364 Average loss: 0.000293 
Epoch: 365 [0/5999 (0%)]	Loss: 0.012328
Epoch: 365 [2560/5999 (43%)]	Loss: 0.032550
Epoch: 365 [5120/5999 (85%)]	Loss: 0.047002
====> Epoch: 365 Average loss: 0.000399 
Epoch: 366 [0/5999 (0%)]	Loss: 0.051007
Epoch: 366 [2560/5999 (43%)]	Loss: 0.032753
Epoch: 366 [5120/5999 (85%)]	Loss: 0.017726
====> Epoch: 366 Average loss: 0.000330 
Epoch: 367 [0/5999 (0%)]	Loss: 0.013965
Epoch: 367 [2560/5999 (43%)]	Loss: 0.135729
Epoch: 367 [5120/5999 (85%)]	Loss: 0.048874
====> Epoch: 367 Average loss: 0.000339 
Epoch: 368 [0/5999 (0%)]	Loss: 0.044305
Epoch: 368 [2560/5999 (43%)]	Loss: 0.067048
Epoch: 368 [5120/5999 (85%)]	Loss: 0.029985
====> Epoch: 368 Average loss: 0.000287 
Epoch: 369 [0/5999 (0%)]	Loss: 0.027497
Epoch: 369 [2560/5999 (43%)]	Loss: 0.032342
Epoch: 369 [5120/5999 (85%)]	Loss: 0.019127
====> Epoch: 369 Average loss: 0.000289 
Epoch: 370 [0/5999 (0%)]	Loss: 0.051586
Epoch: 370 [2560/5999 (43%)]	Loss: 0.027803
Epoch: 370 [5120/5999 (85%)]	Loss: 0.026435
====> Epoch: 370 Average loss: 0.000482 
Epoch: 371 [0/5999 (0%)]	Loss: 0.029655
Epoch: 371 [2560/5999 (43%)]	Loss: 0.069701
Epoch: 371 [5120/5999 (85%)]	Loss: 0.033180
====> Epoch: 371 Average loss: 0.000292 
Epoch: 372 [0/5999 (0%)]	Loss: 0.030074
Epoch: 372 [2560/5999 (43%)]	Loss: 0.034515
Epoch: 372 [5120/5999 (85%)]	Loss: 0.018139
====> Epoch: 372 Average loss: 0.000299 
Epoch: 373 [0/5999 (0%)]	Loss: 0.067851
Epoch: 373 [2560/5999 (43%)]	Loss: 0.021489
Epoch: 373 [5120/5999 (85%)]	Loss: 0.020928
====> Epoch: 373 Average loss: 0.000260 
Epoch: 374 [0/5999 (0%)]	Loss: 0.024879
Epoch: 374 [2560/5999 (43%)]	Loss: 0.023488
Epoch: 374 [5120/5999 (85%)]	Loss: 0.034726
====> Epoch: 374 Average loss: 0.000274 
Epoch: 375 [0/5999 (0%)]	Loss: 0.061531
Epoch: 375 [2560/5999 (43%)]	Loss: 0.058559
Epoch: 375 [5120/5999 (85%)]	Loss: 0.036026
====> Epoch: 375 Average loss: 0.000346 
Epoch: 376 [0/5999 (0%)]	Loss: 0.100346
Epoch: 376 [2560/5999 (43%)]	Loss: 0.023055
Epoch: 376 [5120/5999 (85%)]	Loss: 0.016462
saving model at:376,6.950980785768479e-05
====> Epoch: 376 Average loss: 0.000286 
Epoch: 377 [0/5999 (0%)]	Loss: 0.014456
Epoch: 377 [2560/5999 (43%)]	Loss: 0.027219
Epoch: 377 [5120/5999 (85%)]	Loss: 0.026990
====> Epoch: 377 Average loss: 0.000242 
Epoch: 378 [0/5999 (0%)]	Loss: 0.026889
Epoch: 378 [2560/5999 (43%)]	Loss: 0.045995
Epoch: 378 [5120/5999 (85%)]	Loss: 0.086269
====> Epoch: 378 Average loss: 0.000449 
Epoch: 379 [0/5999 (0%)]	Loss: 0.019535
Epoch: 379 [2560/5999 (43%)]	Loss: 0.021679
Epoch: 379 [5120/5999 (85%)]	Loss: 0.036631
====> Epoch: 379 Average loss: 0.000274 
Epoch: 380 [0/5999 (0%)]	Loss: 0.021892
Epoch: 380 [2560/5999 (43%)]	Loss: 0.023229
Epoch: 380 [5120/5999 (85%)]	Loss: 0.030947
====> Epoch: 380 Average loss: 0.000286 
Epoch: 381 [0/5999 (0%)]	Loss: 0.036590
Epoch: 381 [2560/5999 (43%)]	Loss: 0.030874
Epoch: 381 [5120/5999 (85%)]	Loss: 0.030692
====> Epoch: 381 Average loss: 0.000292 
Epoch: 382 [0/5999 (0%)]	Loss: 0.012132
Epoch: 382 [2560/5999 (43%)]	Loss: 0.051478
Epoch: 382 [5120/5999 (85%)]	Loss: 0.030348
====> Epoch: 382 Average loss: 0.000337 
Epoch: 383 [0/5999 (0%)]	Loss: 0.016045
Epoch: 383 [2560/5999 (43%)]	Loss: 0.046012
Epoch: 383 [5120/5999 (85%)]	Loss: 0.019226
====> Epoch: 383 Average loss: 0.000323 
Epoch: 384 [0/5999 (0%)]	Loss: 0.015166
Epoch: 384 [2560/5999 (43%)]	Loss: 0.027149
Epoch: 384 [5120/5999 (85%)]	Loss: 0.013320
====> Epoch: 384 Average loss: 0.000251 
Epoch: 385 [0/5999 (0%)]	Loss: 0.063114
Epoch: 385 [2560/5999 (43%)]	Loss: 0.018401
Epoch: 385 [5120/5999 (85%)]	Loss: 0.023297
====> Epoch: 385 Average loss: 0.000307 
Epoch: 386 [0/5999 (0%)]	Loss: 0.028227
Epoch: 386 [2560/5999 (43%)]	Loss: 0.088554
Epoch: 386 [5120/5999 (85%)]	Loss: 0.017343
====> Epoch: 386 Average loss: 0.000272 
Epoch: 387 [0/5999 (0%)]	Loss: 0.065543
Epoch: 387 [2560/5999 (43%)]	Loss: 0.036832
Epoch: 387 [5120/5999 (85%)]	Loss: 0.027689
====> Epoch: 387 Average loss: 0.000355 
Epoch: 388 [0/5999 (0%)]	Loss: 0.036121
Epoch: 388 [2560/5999 (43%)]	Loss: 0.034894
Epoch: 388 [5120/5999 (85%)]	Loss: 0.059680
====> Epoch: 388 Average loss: 0.000493 
Epoch: 389 [0/5999 (0%)]	Loss: 0.038228
Epoch: 389 [2560/5999 (43%)]	Loss: 0.067164
Epoch: 389 [5120/5999 (85%)]	Loss: 0.041781
====> Epoch: 389 Average loss: 0.000398 
Epoch: 390 [0/5999 (0%)]	Loss: 0.043544
Epoch: 390 [2560/5999 (43%)]	Loss: 0.087757
Epoch: 390 [5120/5999 (85%)]	Loss: 0.039152
====> Epoch: 390 Average loss: 0.000298 
Epoch: 391 [0/5999 (0%)]	Loss: 0.015019
Epoch: 391 [2560/5999 (43%)]	Loss: 0.029228
Epoch: 391 [5120/5999 (85%)]	Loss: 0.016942
====> Epoch: 391 Average loss: 0.000315 
Epoch: 392 [0/5999 (0%)]	Loss: 0.031868
Epoch: 392 [2560/5999 (43%)]	Loss: 0.019008
Epoch: 392 [5120/5999 (85%)]	Loss: 0.106407
====> Epoch: 392 Average loss: 0.000407 
Epoch: 393 [0/5999 (0%)]	Loss: 0.036424
Epoch: 393 [2560/5999 (43%)]	Loss: 0.029474
Epoch: 393 [5120/5999 (85%)]	Loss: 0.012306
====> Epoch: 393 Average loss: 0.000357 
Epoch: 394 [0/5999 (0%)]	Loss: 0.033986
Epoch: 394 [2560/5999 (43%)]	Loss: 0.031041
Epoch: 394 [5120/5999 (85%)]	Loss: 0.017899
====> Epoch: 394 Average loss: 0.000306 
Epoch: 395 [0/5999 (0%)]	Loss: 0.062293
Epoch: 395 [2560/5999 (43%)]	Loss: 0.062619
Epoch: 395 [5120/5999 (85%)]	Loss: 0.054236
====> Epoch: 395 Average loss: 0.000238 
Epoch: 396 [0/5999 (0%)]	Loss: 0.019913
Epoch: 396 [2560/5999 (43%)]	Loss: 0.192222
Epoch: 396 [5120/5999 (85%)]	Loss: 0.058475
====> Epoch: 396 Average loss: 0.000431 
Epoch: 397 [0/5999 (0%)]	Loss: 0.148571
Epoch: 397 [2560/5999 (43%)]	Loss: 0.060140
Epoch: 397 [5120/5999 (85%)]	Loss: 0.043928
====> Epoch: 397 Average loss: 0.000364 
Epoch: 398 [0/5999 (0%)]	Loss: 0.026224
Epoch: 398 [2560/5999 (43%)]	Loss: 0.016901
Epoch: 398 [5120/5999 (85%)]	Loss: 0.071203
====> Epoch: 398 Average loss: 0.000238 
Epoch: 399 [0/5999 (0%)]	Loss: 0.024174
Epoch: 399 [2560/5999 (43%)]	Loss: 0.013228
Epoch: 399 [5120/5999 (85%)]	Loss: 0.038009
====> Epoch: 399 Average loss: 0.000346 
Epoch: 400 [0/5999 (0%)]	Loss: 0.044578
Epoch: 400 [2560/5999 (43%)]	Loss: 0.061438
Epoch: 400 [5120/5999 (85%)]	Loss: 0.021982
====> Epoch: 400 Average loss: 0.000413 
Reconstruction Loss 6.950980512192472e-05
per_obj_mse: ['9.45118154049851e-05', '4.450780397746712e-05']
Reconstruction Loss 6.895268745648756e-05
per_obj_mse: ['9.371775377076119e-05', '4.418763273861259e-05']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_4_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_4_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_4_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_4_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.00013420747488271446
per_obj_mse: ['0.00014749412366654724', '0.00012092084944015369']
Reconstruction Loss 0.0001395944611673379
per_obj_mse: ['0.00015196263848338276', '0.0001272262743441388']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_4_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_4_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.00013420747488271446
per_obj_mse: ['0.00014749412366654724', '0.00012092084944015369']
Reconstruction Loss 0.0001395944611673379
per_obj_mse: ['0.00015196263848338276', '0.0001272262743441388']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_4_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=20, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_4_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7997, 2, 11)
(1997, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0011590897068381309
per_obj_mse: ['0.0016621273243799806', '0.0006560520851053298']
Reconstruction Loss 0.001206011352647874
per_obj_mse: ['0.0017225464107468724', '0.0006894762045703828']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_4_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=60, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_4_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7993, 2, 11)
(1993, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.003947707167032243
per_obj_mse: ['0.005783023778349161', '0.0021123909391462803']
Reconstruction Loss 0.004080322258944525
per_obj_mse: ['0.005942997056990862', '0.0022176471538841724']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_4_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=100, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_4_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7989, 2, 11)
(1989, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.006713632614005942
per_obj_mse: ['0.009875963442027569', '0.0035513013135641813']
Reconstruction Loss 0.006952783495773992
per_obj_mse: ['0.010160577483475208', '0.003744992194697261']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_4_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=140, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_4_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7985, 2, 11)
(1985, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.009489959238762103
per_obj_mse: ['0.01396607980132103', '0.005013839807361364']
Reconstruction Loss 0.009829876546557965
per_obj_mse: ['0.014384529553353786', '0.005275221541523933']
