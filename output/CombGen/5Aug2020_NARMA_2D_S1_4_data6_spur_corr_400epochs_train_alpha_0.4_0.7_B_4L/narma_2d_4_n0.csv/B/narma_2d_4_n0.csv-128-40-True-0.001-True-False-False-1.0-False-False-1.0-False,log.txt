cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_4_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=400, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_4_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Epoch: 1 [0/5999 (0%)]	Loss: 130.891876
Epoch: 1 [2560/5999 (43%)]	Loss: 20.878485
saving model at:1,0.15360819828510283
Epoch: 1 [5120/5999 (85%)]	Loss: 13.892222
saving model at:1,0.1054117238521576
====> Epoch: 1 Average loss: 0.205621 
Epoch: 2 [0/5999 (0%)]	Loss: 13.236537
Epoch: 2 [2560/5999 (43%)]	Loss: 11.963988
saving model at:2,0.08374526584148408
Epoch: 2 [5120/5999 (85%)]	Loss: 10.192472
saving model at:2,0.08026381552219392
====> Epoch: 2 Average loss: 0.086949 
Epoch: 3 [0/5999 (0%)]	Loss: 10.014944
Epoch: 3 [2560/5999 (43%)]	Loss: 7.238007
saving model at:3,0.05470786702632904
Epoch: 3 [5120/5999 (85%)]	Loss: 4.292885
saving model at:3,0.029342690631747246
====> Epoch: 3 Average loss: 0.053108 
Epoch: 4 [0/5999 (0%)]	Loss: 3.787585
Epoch: 4 [2560/5999 (43%)]	Loss: 2.500515
saving model at:4,0.020742572665214537
Epoch: 4 [5120/5999 (85%)]	Loss: 2.436389
saving model at:4,0.017586428701877595
====> Epoch: 4 Average loss: 0.021497 
Epoch: 5 [0/5999 (0%)]	Loss: 2.785990
Epoch: 5 [2560/5999 (43%)]	Loss: 1.894883
saving model at:5,0.014371061228215694
Epoch: 5 [5120/5999 (85%)]	Loss: 1.810853
saving model at:5,0.012611494451761246
====> Epoch: 5 Average loss: 0.015088 
Epoch: 6 [0/5999 (0%)]	Loss: 1.423780
Epoch: 6 [2560/5999 (43%)]	Loss: 1.429052
saving model at:6,0.009937972240149974
Epoch: 6 [5120/5999 (85%)]	Loss: 1.102682
saving model at:6,0.008580757595598698
====> Epoch: 6 Average loss: 0.010483 
Epoch: 7 [0/5999 (0%)]	Loss: 1.149154
Epoch: 7 [2560/5999 (43%)]	Loss: 1.053077
saving model at:7,0.006990587424486876
Epoch: 7 [5120/5999 (85%)]	Loss: 0.925063
saving model at:7,0.006616765309125185
====> Epoch: 7 Average loss: 0.007961 
Epoch: 8 [0/5999 (0%)]	Loss: 0.796167
Epoch: 8 [2560/5999 (43%)]	Loss: 0.756674
saving model at:8,0.005864980280399322
Epoch: 8 [5120/5999 (85%)]	Loss: 1.059291
====> Epoch: 8 Average loss: 0.006379 
Epoch: 9 [0/5999 (0%)]	Loss: 1.000844
Epoch: 9 [2560/5999 (43%)]	Loss: 0.663787
saving model at:9,0.004781874977052212
Epoch: 9 [5120/5999 (85%)]	Loss: 0.742648
====> Epoch: 9 Average loss: 0.005291 
Epoch: 10 [0/5999 (0%)]	Loss: 0.692118
Epoch: 10 [2560/5999 (43%)]	Loss: 0.620495
saving model at:10,0.00420569110289216
Epoch: 10 [5120/5999 (85%)]	Loss: 0.512903
saving model at:10,0.004034515488892793
====> Epoch: 10 Average loss: 0.004556 
Epoch: 11 [0/5999 (0%)]	Loss: 0.580399
Epoch: 11 [2560/5999 (43%)]	Loss: 0.638962
saving model at:11,0.003771434335038066
Epoch: 11 [5120/5999 (85%)]	Loss: 0.625553
====> Epoch: 11 Average loss: 0.004210 
Epoch: 12 [0/5999 (0%)]	Loss: 0.504700
Epoch: 12 [2560/5999 (43%)]	Loss: 0.492739
saving model at:12,0.003466796562075615
Epoch: 12 [5120/5999 (85%)]	Loss: 0.557830
====> Epoch: 12 Average loss: 0.003845 
Epoch: 13 [0/5999 (0%)]	Loss: 0.746631
Epoch: 13 [2560/5999 (43%)]	Loss: 0.504665
saving model at:13,0.0033494909144937994
Epoch: 13 [5120/5999 (85%)]	Loss: 0.423823
saving model at:13,0.0032522549238055945
====> Epoch: 13 Average loss: 0.003999 
Epoch: 14 [0/5999 (0%)]	Loss: 0.457642
Epoch: 14 [2560/5999 (43%)]	Loss: 0.425502
saving model at:14,0.0031195001304149626
Epoch: 14 [5120/5999 (85%)]	Loss: 0.461322
saving model at:14,0.002949152389541268
====> Epoch: 14 Average loss: 0.003312 
Epoch: 15 [0/5999 (0%)]	Loss: 0.345821
Epoch: 15 [2560/5999 (43%)]	Loss: 0.549499
Epoch: 15 [5120/5999 (85%)]	Loss: 0.359109
====> Epoch: 15 Average loss: 0.003908 
Epoch: 16 [0/5999 (0%)]	Loss: 0.441433
Epoch: 16 [2560/5999 (43%)]	Loss: 0.439639
saving model at:16,0.0028911925815045835
Epoch: 16 [5120/5999 (85%)]	Loss: 0.369921
saving model at:16,0.00271151115372777
====> Epoch: 16 Average loss: 0.003869 
Epoch: 17 [0/5999 (0%)]	Loss: 0.354769
Epoch: 17 [2560/5999 (43%)]	Loss: 0.361021
Epoch: 17 [5120/5999 (85%)]	Loss: 0.354939
saving model at:17,0.002557375468313694
====> Epoch: 17 Average loss: 0.003143 
Epoch: 18 [0/5999 (0%)]	Loss: 0.659683
Epoch: 18 [2560/5999 (43%)]	Loss: 0.405518
Epoch: 18 [5120/5999 (85%)]	Loss: 0.414203
====> Epoch: 18 Average loss: 0.003077 
Epoch: 19 [0/5999 (0%)]	Loss: 0.543141
Epoch: 19 [2560/5999 (43%)]	Loss: 0.315800
Epoch: 19 [5120/5999 (85%)]	Loss: 0.325891
saving model at:19,0.0025007069204002618
====> Epoch: 19 Average loss: 0.002787 
Epoch: 20 [0/5999 (0%)]	Loss: 0.326392
Epoch: 20 [2560/5999 (43%)]	Loss: 0.304252
Epoch: 20 [5120/5999 (85%)]	Loss: 0.373873
====> Epoch: 20 Average loss: 0.002977 
Epoch: 21 [0/5999 (0%)]	Loss: 0.299926
Epoch: 21 [2560/5999 (43%)]	Loss: 0.481755
saving model at:21,0.0023920547850430013
Epoch: 21 [5120/5999 (85%)]	Loss: 0.376461
====> Epoch: 21 Average loss: 0.002846 
Epoch: 22 [0/5999 (0%)]	Loss: 0.305317
Epoch: 22 [2560/5999 (43%)]	Loss: 0.361580
saving model at:22,0.0022414081487804652
Epoch: 22 [5120/5999 (85%)]	Loss: 0.361568
====> Epoch: 22 Average loss: 0.002894 
Epoch: 23 [0/5999 (0%)]	Loss: 0.291805
Epoch: 23 [2560/5999 (43%)]	Loss: 0.353419
Epoch: 23 [5120/5999 (85%)]	Loss: 0.263319
saving model at:23,0.00205768221989274
====> Epoch: 23 Average loss: 0.002352 
Epoch: 24 [0/5999 (0%)]	Loss: 0.300631
Epoch: 24 [2560/5999 (43%)]	Loss: 0.336084
Epoch: 24 [5120/5999 (85%)]	Loss: 0.274772
====> Epoch: 24 Average loss: 0.002270 
Epoch: 25 [0/5999 (0%)]	Loss: 0.359304
Epoch: 25 [2560/5999 (43%)]	Loss: 0.251600
saving model at:25,0.0019096041824668646
Epoch: 25 [5120/5999 (85%)]	Loss: 0.296606
====> Epoch: 25 Average loss: 0.002558 
Epoch: 26 [0/5999 (0%)]	Loss: 0.369567
Epoch: 26 [2560/5999 (43%)]	Loss: 0.525554
Epoch: 26 [5120/5999 (85%)]	Loss: 0.312104
====> Epoch: 26 Average loss: 0.002878 
Epoch: 27 [0/5999 (0%)]	Loss: 0.254115
Epoch: 27 [2560/5999 (43%)]	Loss: 0.288511
Epoch: 27 [5120/5999 (85%)]	Loss: 0.315898
====> Epoch: 27 Average loss: 0.002708 
Epoch: 28 [0/5999 (0%)]	Loss: 0.322631
Epoch: 28 [2560/5999 (43%)]	Loss: 0.257110
saving model at:28,0.0018240310801193118
Epoch: 28 [5120/5999 (85%)]	Loss: 0.290236
====> Epoch: 28 Average loss: 0.002349 
Epoch: 29 [0/5999 (0%)]	Loss: 0.225830
Epoch: 29 [2560/5999 (43%)]	Loss: 0.272808
Epoch: 29 [5120/5999 (85%)]	Loss: 0.277016
saving model at:29,0.0017771072797477245
====> Epoch: 29 Average loss: 0.002233 
Epoch: 30 [0/5999 (0%)]	Loss: 0.226643
Epoch: 30 [2560/5999 (43%)]	Loss: 0.371532
Epoch: 30 [5120/5999 (85%)]	Loss: 0.251628
saving model at:30,0.0016772011052817106
====> Epoch: 30 Average loss: 0.002228 
Epoch: 31 [0/5999 (0%)]	Loss: 0.241697
Epoch: 31 [2560/5999 (43%)]	Loss: 0.210898
saving model at:31,0.0016573760723695159
Epoch: 31 [5120/5999 (85%)]	Loss: 0.379458
====> Epoch: 31 Average loss: 0.002090 
Epoch: 32 [0/5999 (0%)]	Loss: 0.210773
Epoch: 32 [2560/5999 (43%)]	Loss: 0.330945
saving model at:32,0.0016480842735618353
Epoch: 32 [5120/5999 (85%)]	Loss: 0.252112
saving model at:32,0.0016220970544964075
====> Epoch: 32 Average loss: 0.001824 
Epoch: 33 [0/5999 (0%)]	Loss: 0.215536
Epoch: 33 [2560/5999 (43%)]	Loss: 0.338159
Epoch: 33 [5120/5999 (85%)]	Loss: 0.295093
====> Epoch: 33 Average loss: 0.002005 
Epoch: 34 [0/5999 (0%)]	Loss: 0.307942
Epoch: 34 [2560/5999 (43%)]	Loss: 0.256893
Epoch: 34 [5120/5999 (85%)]	Loss: 0.309672
====> Epoch: 34 Average loss: 0.001967 
Epoch: 35 [0/5999 (0%)]	Loss: 0.211505
Epoch: 35 [2560/5999 (43%)]	Loss: 0.207546
saving model at:35,0.0015756748225539924
Epoch: 35 [5120/5999 (85%)]	Loss: 0.336032
====> Epoch: 35 Average loss: 0.002014 
Epoch: 36 [0/5999 (0%)]	Loss: 0.252582
Epoch: 36 [2560/5999 (43%)]	Loss: 0.282324
saving model at:36,0.0015137518495321274
Epoch: 36 [5120/5999 (85%)]	Loss: 0.215484
====> Epoch: 36 Average loss: 0.002075 
Epoch: 37 [0/5999 (0%)]	Loss: 0.202234
Epoch: 37 [2560/5999 (43%)]	Loss: 0.216845
saving model at:37,0.0014789572143927216
Epoch: 37 [5120/5999 (85%)]	Loss: 0.368421
====> Epoch: 37 Average loss: 0.001769 
Epoch: 38 [0/5999 (0%)]	Loss: 0.294385
Epoch: 38 [2560/5999 (43%)]	Loss: 0.219257
Epoch: 38 [5120/5999 (85%)]	Loss: 0.191517
====> Epoch: 38 Average loss: 0.001771 
Epoch: 39 [0/5999 (0%)]	Loss: 0.302873
Epoch: 39 [2560/5999 (43%)]	Loss: 0.227526
saving model at:39,0.001441220497712493
Epoch: 39 [5120/5999 (85%)]	Loss: 0.336762
====> Epoch: 39 Average loss: 0.001847 
Epoch: 40 [0/5999 (0%)]	Loss: 0.276865
Epoch: 40 [2560/5999 (43%)]	Loss: 0.252778
Epoch: 40 [5120/5999 (85%)]	Loss: 0.261116
saving model at:40,0.0014230344071984292
====> Epoch: 40 Average loss: 0.001897 
Epoch: 41 [0/5999 (0%)]	Loss: 0.222760
Epoch: 41 [2560/5999 (43%)]	Loss: 0.172963
saving model at:41,0.0013640708532184362
Epoch: 41 [5120/5999 (85%)]	Loss: 0.217536
====> Epoch: 41 Average loss: 0.001866 
Epoch: 42 [0/5999 (0%)]	Loss: 0.229544
Epoch: 42 [2560/5999 (43%)]	Loss: 0.225004
Epoch: 42 [5120/5999 (85%)]	Loss: 0.457202
====> Epoch: 42 Average loss: 0.002351 
Epoch: 43 [0/5999 (0%)]	Loss: 0.239186
Epoch: 43 [2560/5999 (43%)]	Loss: 0.214337
Epoch: 43 [5120/5999 (85%)]	Loss: 0.210641
====> Epoch: 43 Average loss: 0.002004 
Epoch: 44 [0/5999 (0%)]	Loss: 0.179239
Epoch: 44 [2560/5999 (43%)]	Loss: 0.201188
Epoch: 44 [5120/5999 (85%)]	Loss: 0.229268
====> Epoch: 44 Average loss: 0.001715 
Epoch: 45 [0/5999 (0%)]	Loss: 0.333689
Epoch: 45 [2560/5999 (43%)]	Loss: 0.234609
Epoch: 45 [5120/5999 (85%)]	Loss: 0.194505
saving model at:45,0.0013182981992140412
====> Epoch: 45 Average loss: 0.001940 
Epoch: 46 [0/5999 (0%)]	Loss: 0.161236
Epoch: 46 [2560/5999 (43%)]	Loss: 0.375031
Epoch: 46 [5120/5999 (85%)]	Loss: 0.189738
====> Epoch: 46 Average loss: 0.001786 
Epoch: 47 [0/5999 (0%)]	Loss: 0.173495
Epoch: 47 [2560/5999 (43%)]	Loss: 0.297249
Epoch: 47 [5120/5999 (85%)]	Loss: 0.158080
saving model at:47,0.0012655974142253398
====> Epoch: 47 Average loss: 0.001784 
Epoch: 48 [0/5999 (0%)]	Loss: 0.201328
Epoch: 48 [2560/5999 (43%)]	Loss: 0.155211
Epoch: 48 [5120/5999 (85%)]	Loss: 0.187805
====> Epoch: 48 Average loss: 0.001753 
Epoch: 49 [0/5999 (0%)]	Loss: 0.233369
Epoch: 49 [2560/5999 (43%)]	Loss: 0.186499
Epoch: 49 [5120/5999 (85%)]	Loss: 0.320494
====> Epoch: 49 Average loss: 0.001979 
Epoch: 50 [0/5999 (0%)]	Loss: 0.211094
Epoch: 50 [2560/5999 (43%)]	Loss: 0.188338
Epoch: 50 [5120/5999 (85%)]	Loss: 0.211584
====> Epoch: 50 Average loss: 0.002029 
Epoch: 51 [0/5999 (0%)]	Loss: 0.280379
Epoch: 51 [2560/5999 (43%)]	Loss: 0.172611
Epoch: 51 [5120/5999 (85%)]	Loss: 0.252263
====> Epoch: 51 Average loss: 0.001823 
Epoch: 52 [0/5999 (0%)]	Loss: 0.240443
Epoch: 52 [2560/5999 (43%)]	Loss: 0.175968
Epoch: 52 [5120/5999 (85%)]	Loss: 0.216351
====> Epoch: 52 Average loss: 0.002244 
Epoch: 53 [0/5999 (0%)]	Loss: 0.164261
Epoch: 53 [2560/5999 (43%)]	Loss: 0.141055
saving model at:53,0.001239444530569017
Epoch: 53 [5120/5999 (85%)]	Loss: 0.163408
saving model at:53,0.0012164181638509035
====> Epoch: 53 Average loss: 0.001641 
Epoch: 54 [0/5999 (0%)]	Loss: 0.174122
Epoch: 54 [2560/5999 (43%)]	Loss: 0.166327
Epoch: 54 [5120/5999 (85%)]	Loss: 0.223391
====> Epoch: 54 Average loss: 0.001530 
Epoch: 55 [0/5999 (0%)]	Loss: 0.202336
Epoch: 55 [2560/5999 (43%)]	Loss: 0.301377
Epoch: 55 [5120/5999 (85%)]	Loss: 0.377848
====> Epoch: 55 Average loss: 0.001735 
Epoch: 56 [0/5999 (0%)]	Loss: 0.188506
Epoch: 56 [2560/5999 (43%)]	Loss: 0.178765
saving model at:56,0.0011186806168407201
Epoch: 56 [5120/5999 (85%)]	Loss: 0.153783
====> Epoch: 56 Average loss: 0.001507 
Epoch: 57 [0/5999 (0%)]	Loss: 0.177659
Epoch: 57 [2560/5999 (43%)]	Loss: 0.164098
Epoch: 57 [5120/5999 (85%)]	Loss: 0.274063
====> Epoch: 57 Average loss: 0.001778 
Epoch: 58 [0/5999 (0%)]	Loss: 0.165204
Epoch: 58 [2560/5999 (43%)]	Loss: 0.204957
Epoch: 58 [5120/5999 (85%)]	Loss: 0.174325
saving model at:58,0.0011145852617919444
====> Epoch: 58 Average loss: 0.001487 
Epoch: 59 [0/5999 (0%)]	Loss: 0.212895
Epoch: 59 [2560/5999 (43%)]	Loss: 0.140580
Epoch: 59 [5120/5999 (85%)]	Loss: 0.842539
====> Epoch: 59 Average loss: 0.002161 
Epoch: 60 [0/5999 (0%)]	Loss: 0.393537
Epoch: 60 [2560/5999 (43%)]	Loss: 0.280012
Epoch: 60 [5120/5999 (85%)]	Loss: 0.204914
====> Epoch: 60 Average loss: 0.001740 
Epoch: 61 [0/5999 (0%)]	Loss: 0.198509
Epoch: 61 [2560/5999 (43%)]	Loss: 0.442159
Epoch: 61 [5120/5999 (85%)]	Loss: 0.339430
====> Epoch: 61 Average loss: 0.001773 
Epoch: 62 [0/5999 (0%)]	Loss: 0.329701
Epoch: 62 [2560/5999 (43%)]	Loss: 0.251599
Epoch: 62 [5120/5999 (85%)]	Loss: 0.138651
====> Epoch: 62 Average loss: 0.001937 
Epoch: 63 [0/5999 (0%)]	Loss: 0.200260
Epoch: 63 [2560/5999 (43%)]	Loss: 0.149562
saving model at:63,0.0010614781538024545
Epoch: 63 [5120/5999 (85%)]	Loss: 0.155163
====> Epoch: 63 Average loss: 0.001480 
Epoch: 64 [0/5999 (0%)]	Loss: 0.150049
Epoch: 64 [2560/5999 (43%)]	Loss: 0.137951
Epoch: 64 [5120/5999 (85%)]	Loss: 0.184539
====> Epoch: 64 Average loss: 0.001361 
Epoch: 65 [0/5999 (0%)]	Loss: 0.164186
Epoch: 65 [2560/5999 (43%)]	Loss: 0.409419
Epoch: 65 [5120/5999 (85%)]	Loss: 0.179059
====> Epoch: 65 Average loss: 0.001680 
Epoch: 66 [0/5999 (0%)]	Loss: 0.128982
Epoch: 66 [2560/5999 (43%)]	Loss: 0.174787
Epoch: 66 [5120/5999 (85%)]	Loss: 0.145549
saving model at:66,0.0009928316958248615
====> Epoch: 66 Average loss: 0.001513 
Epoch: 67 [0/5999 (0%)]	Loss: 0.133695
Epoch: 67 [2560/5999 (43%)]	Loss: 0.190897
Epoch: 67 [5120/5999 (85%)]	Loss: 0.119392
====> Epoch: 67 Average loss: 0.001460 
Epoch: 68 [0/5999 (0%)]	Loss: 0.152060
Epoch: 68 [2560/5999 (43%)]	Loss: 0.392098
Epoch: 68 [5120/5999 (85%)]	Loss: 0.288075
====> Epoch: 68 Average loss: 0.001959 
Epoch: 69 [0/5999 (0%)]	Loss: 0.146009
Epoch: 69 [2560/5999 (43%)]	Loss: 0.136111
Epoch: 69 [5120/5999 (85%)]	Loss: 0.152218
====> Epoch: 69 Average loss: 0.001602 
Epoch: 70 [0/5999 (0%)]	Loss: 0.171486
Epoch: 70 [2560/5999 (43%)]	Loss: 0.169284
saving model at:70,0.0009530613077804446
Epoch: 70 [5120/5999 (85%)]	Loss: 0.306825
====> Epoch: 70 Average loss: 0.001955 
Epoch: 71 [0/5999 (0%)]	Loss: 0.193573
Epoch: 71 [2560/5999 (43%)]	Loss: 0.129153
Epoch: 71 [5120/5999 (85%)]	Loss: 0.128847
====> Epoch: 71 Average loss: 0.001350 
Epoch: 72 [0/5999 (0%)]	Loss: 0.130769
Epoch: 72 [2560/5999 (43%)]	Loss: 0.157691
saving model at:72,0.0009429534673690796
Epoch: 72 [5120/5999 (85%)]	Loss: 0.133678
====> Epoch: 72 Average loss: 0.001332 
Epoch: 73 [0/5999 (0%)]	Loss: 0.188231
Epoch: 73 [2560/5999 (43%)]	Loss: 0.165471
Epoch: 73 [5120/5999 (85%)]	Loss: 0.238603
====> Epoch: 73 Average loss: 0.001802 
Epoch: 74 [0/5999 (0%)]	Loss: 0.141738
Epoch: 74 [2560/5999 (43%)]	Loss: 0.167780
Epoch: 74 [5120/5999 (85%)]	Loss: 0.166321
====> Epoch: 74 Average loss: 0.001208 
Epoch: 75 [0/5999 (0%)]	Loss: 0.179277
Epoch: 75 [2560/5999 (43%)]	Loss: 0.158683
Epoch: 75 [5120/5999 (85%)]	Loss: 0.181797
====> Epoch: 75 Average loss: 0.001535 
Epoch: 76 [0/5999 (0%)]	Loss: 0.439122
Epoch: 76 [2560/5999 (43%)]	Loss: 0.134689
Epoch: 76 [5120/5999 (85%)]	Loss: 0.238219
====> Epoch: 76 Average loss: 0.001544 
Epoch: 77 [0/5999 (0%)]	Loss: 0.117737
Epoch: 77 [2560/5999 (43%)]	Loss: 0.150053
saving model at:77,0.0009219730636104941
Epoch: 77 [5120/5999 (85%)]	Loss: 0.224242
====> Epoch: 77 Average loss: 0.001348 
Epoch: 78 [0/5999 (0%)]	Loss: 0.127005
Epoch: 78 [2560/5999 (43%)]	Loss: 0.205796
Epoch: 78 [5120/5999 (85%)]	Loss: 0.317408
====> Epoch: 78 Average loss: 0.001598 
Epoch: 79 [0/5999 (0%)]	Loss: 0.347031
Epoch: 79 [2560/5999 (43%)]	Loss: 0.260460
Epoch: 79 [5120/5999 (85%)]	Loss: 0.131622
====> Epoch: 79 Average loss: 0.001328 
Epoch: 80 [0/5999 (0%)]	Loss: 0.216257
Epoch: 80 [2560/5999 (43%)]	Loss: 0.133188
Epoch: 80 [5120/5999 (85%)]	Loss: 0.209326
====> Epoch: 80 Average loss: 0.001266 
Epoch: 81 [0/5999 (0%)]	Loss: 0.274375
Epoch: 81 [2560/5999 (43%)]	Loss: 0.151144
Epoch: 81 [5120/5999 (85%)]	Loss: 0.265688
====> Epoch: 81 Average loss: 0.001369 
Epoch: 82 [0/5999 (0%)]	Loss: 0.128255
Epoch: 82 [2560/5999 (43%)]	Loss: 0.226283
Epoch: 82 [5120/5999 (85%)]	Loss: 0.151511
====> Epoch: 82 Average loss: 0.001276 
Epoch: 83 [0/5999 (0%)]	Loss: 0.133479
Epoch: 83 [2560/5999 (43%)]	Loss: 0.191781
Epoch: 83 [5120/5999 (85%)]	Loss: 0.133299
saving model at:83,0.0009018370476551354
====> Epoch: 83 Average loss: 0.001255 
Epoch: 84 [0/5999 (0%)]	Loss: 0.198089
Epoch: 84 [2560/5999 (43%)]	Loss: 0.115950
Epoch: 84 [5120/5999 (85%)]	Loss: 0.243371
====> Epoch: 84 Average loss: 0.001503 
Epoch: 85 [0/5999 (0%)]	Loss: 0.302783
Epoch: 85 [2560/5999 (43%)]	Loss: 0.133299
Epoch: 85 [5120/5999 (85%)]	Loss: 0.118448
====> Epoch: 85 Average loss: 0.001412 
Epoch: 86 [0/5999 (0%)]	Loss: 0.126659
Epoch: 86 [2560/5999 (43%)]	Loss: 0.196001
Epoch: 86 [5120/5999 (85%)]	Loss: 0.273497
====> Epoch: 86 Average loss: 0.001696 
Epoch: 87 [0/5999 (0%)]	Loss: 0.157281
Epoch: 87 [2560/5999 (43%)]	Loss: 0.154545
Epoch: 87 [5120/5999 (85%)]	Loss: 0.130990
====> Epoch: 87 Average loss: 0.001215 
Epoch: 88 [0/5999 (0%)]	Loss: 0.116307
Epoch: 88 [2560/5999 (43%)]	Loss: 0.167979
Epoch: 88 [5120/5999 (85%)]	Loss: 0.186845
saving model at:88,0.0008698252458125353
====> Epoch: 88 Average loss: 0.001293 
Epoch: 89 [0/5999 (0%)]	Loss: 0.171568
Epoch: 89 [2560/5999 (43%)]	Loss: 0.121189
Epoch: 89 [5120/5999 (85%)]	Loss: 0.305708
====> Epoch: 89 Average loss: 0.001562 
Epoch: 90 [0/5999 (0%)]	Loss: 0.265653
Epoch: 90 [2560/5999 (43%)]	Loss: 0.146567
Epoch: 90 [5120/5999 (85%)]	Loss: 0.211947
====> Epoch: 90 Average loss: 0.001345 
Epoch: 91 [0/5999 (0%)]	Loss: 0.176927
Epoch: 91 [2560/5999 (43%)]	Loss: 0.141362
saving model at:91,0.0008470558049157262
Epoch: 91 [5120/5999 (85%)]	Loss: 0.163977
====> Epoch: 91 Average loss: 0.001373 
Epoch: 92 [0/5999 (0%)]	Loss: 0.271917
Epoch: 92 [2560/5999 (43%)]	Loss: 0.126411
Epoch: 92 [5120/5999 (85%)]	Loss: 0.109602
====> Epoch: 92 Average loss: 0.001408 
Epoch: 93 [0/5999 (0%)]	Loss: 0.214890
Epoch: 93 [2560/5999 (43%)]	Loss: 0.170544
Epoch: 93 [5120/5999 (85%)]	Loss: 0.321216
====> Epoch: 93 Average loss: 0.001543 
Epoch: 94 [0/5999 (0%)]	Loss: 0.285206
Epoch: 94 [2560/5999 (43%)]	Loss: 0.155392
Epoch: 94 [5120/5999 (85%)]	Loss: 0.155855
====> Epoch: 94 Average loss: 0.001526 
Epoch: 95 [0/5999 (0%)]	Loss: 0.178938
Epoch: 95 [2560/5999 (43%)]	Loss: 0.160177
Epoch: 95 [5120/5999 (85%)]	Loss: 0.108776
====> Epoch: 95 Average loss: 0.001290 
Epoch: 96 [0/5999 (0%)]	Loss: 0.394669
Epoch: 96 [2560/5999 (43%)]	Loss: 0.114087
Epoch: 96 [5120/5999 (85%)]	Loss: 0.109922
====> Epoch: 96 Average loss: 0.001122 
Epoch: 97 [0/5999 (0%)]	Loss: 0.110775
Epoch: 97 [2560/5999 (43%)]	Loss: 0.148932
Epoch: 97 [5120/5999 (85%)]	Loss: 0.129718
saving model at:97,0.0008272057250142097
====> Epoch: 97 Average loss: 0.001181 
Epoch: 98 [0/5999 (0%)]	Loss: 0.133461
Epoch: 98 [2560/5999 (43%)]	Loss: 0.614666
Epoch: 98 [5120/5999 (85%)]	Loss: 0.291068
====> Epoch: 98 Average loss: 0.001987 
Epoch: 99 [0/5999 (0%)]	Loss: 0.214567
Epoch: 99 [2560/5999 (43%)]	Loss: 0.159608
Epoch: 99 [5120/5999 (85%)]	Loss: 0.338522
====> Epoch: 99 Average loss: 0.001595 
Epoch: 100 [0/5999 (0%)]	Loss: 0.128395
Epoch: 100 [2560/5999 (43%)]	Loss: 0.268353
Epoch: 100 [5120/5999 (85%)]	Loss: 0.131196
====> Epoch: 100 Average loss: 0.001297 
Epoch: 101 [0/5999 (0%)]	Loss: 0.131814
Epoch: 101 [2560/5999 (43%)]	Loss: 0.130635
saving model at:101,0.0007994242943823338
Epoch: 101 [5120/5999 (85%)]	Loss: 0.171213
====> Epoch: 101 Average loss: 0.001143 
Epoch: 102 [0/5999 (0%)]	Loss: 0.123894
Epoch: 102 [2560/5999 (43%)]	Loss: 0.222205
Epoch: 102 [5120/5999 (85%)]	Loss: 0.119912
====> Epoch: 102 Average loss: 0.001148 
Epoch: 103 [0/5999 (0%)]	Loss: 0.103956
Epoch: 103 [2560/5999 (43%)]	Loss: 0.225718
Epoch: 103 [5120/5999 (85%)]	Loss: 0.134619
====> Epoch: 103 Average loss: 0.001094 
Epoch: 104 [0/5999 (0%)]	Loss: 0.151973
Epoch: 104 [2560/5999 (43%)]	Loss: 0.159875
Epoch: 104 [5120/5999 (85%)]	Loss: 0.323424
====> Epoch: 104 Average loss: 0.001644 
Epoch: 105 [0/5999 (0%)]	Loss: 0.239153
Epoch: 105 [2560/5999 (43%)]	Loss: 0.252690
Epoch: 105 [5120/5999 (85%)]	Loss: 0.167330
====> Epoch: 105 Average loss: 0.001459 
Epoch: 106 [0/5999 (0%)]	Loss: 0.126225
Epoch: 106 [2560/5999 (43%)]	Loss: 0.122821
saving model at:106,0.000777068430557847
Epoch: 106 [5120/5999 (85%)]	Loss: 0.343552
====> Epoch: 106 Average loss: 0.001347 
Epoch: 107 [0/5999 (0%)]	Loss: 0.256831
Epoch: 107 [2560/5999 (43%)]	Loss: 0.144067
Epoch: 107 [5120/5999 (85%)]	Loss: 0.116566
====> Epoch: 107 Average loss: 0.001571 
Epoch: 108 [0/5999 (0%)]	Loss: 0.184389
Epoch: 108 [2560/5999 (43%)]	Loss: 0.124706
Epoch: 108 [5120/5999 (85%)]	Loss: 0.312297
====> Epoch: 108 Average loss: 0.001316 
Epoch: 109 [0/5999 (0%)]	Loss: 0.187087
Epoch: 109 [2560/5999 (43%)]	Loss: 0.116259
Epoch: 109 [5120/5999 (85%)]	Loss: 0.188778
====> Epoch: 109 Average loss: 0.001132 
Epoch: 110 [0/5999 (0%)]	Loss: 0.150876
Epoch: 110 [2560/5999 (43%)]	Loss: 0.119642
Epoch: 110 [5120/5999 (85%)]	Loss: 0.166469
====> Epoch: 110 Average loss: 0.001304 
Epoch: 111 [0/5999 (0%)]	Loss: 0.155686
Epoch: 111 [2560/5999 (43%)]	Loss: 0.136253
Epoch: 111 [5120/5999 (85%)]	Loss: 0.115917
====> Epoch: 111 Average loss: 0.001513 
Epoch: 112 [0/5999 (0%)]	Loss: 0.148921
Epoch: 112 [2560/5999 (43%)]	Loss: 0.165047
Epoch: 112 [5120/5999 (85%)]	Loss: 0.182034
====> Epoch: 112 Average loss: 0.001106 
Epoch: 113 [0/5999 (0%)]	Loss: 0.271412
Epoch: 113 [2560/5999 (43%)]	Loss: 0.121153
Epoch: 113 [5120/5999 (85%)]	Loss: 0.107060
====> Epoch: 113 Average loss: 0.001150 
Epoch: 114 [0/5999 (0%)]	Loss: 0.167725
Epoch: 114 [2560/5999 (43%)]	Loss: 0.120859
Epoch: 114 [5120/5999 (85%)]	Loss: 0.112410
====> Epoch: 114 Average loss: 0.001300 
Epoch: 115 [0/5999 (0%)]	Loss: 0.091855
Epoch: 115 [2560/5999 (43%)]	Loss: 0.204086
Epoch: 115 [5120/5999 (85%)]	Loss: 0.137282
====> Epoch: 115 Average loss: 0.001240 
Epoch: 116 [0/5999 (0%)]	Loss: 0.130174
Epoch: 116 [2560/5999 (43%)]	Loss: 0.140879
saving model at:116,0.000775333103723824
Epoch: 116 [5120/5999 (85%)]	Loss: 0.100781
====> Epoch: 116 Average loss: 0.001140 
Epoch: 117 [0/5999 (0%)]	Loss: 0.105540
Epoch: 117 [2560/5999 (43%)]	Loss: 0.107860
Epoch: 117 [5120/5999 (85%)]	Loss: 0.115614
====> Epoch: 117 Average loss: 0.001164 
Epoch: 118 [0/5999 (0%)]	Loss: 0.104158
Epoch: 118 [2560/5999 (43%)]	Loss: 0.133862
Epoch: 118 [5120/5999 (85%)]	Loss: 0.106694
====> Epoch: 118 Average loss: 0.001288 
Epoch: 119 [0/5999 (0%)]	Loss: 0.150665
Epoch: 119 [2560/5999 (43%)]	Loss: 0.150094
Epoch: 119 [5120/5999 (85%)]	Loss: 0.142864
====> Epoch: 119 Average loss: 0.001133 
Epoch: 120 [0/5999 (0%)]	Loss: 0.130668
Epoch: 120 [2560/5999 (43%)]	Loss: 0.155952
Epoch: 120 [5120/5999 (85%)]	Loss: 0.315532
====> Epoch: 120 Average loss: 0.001319 
Epoch: 121 [0/5999 (0%)]	Loss: 0.155499
Epoch: 121 [2560/5999 (43%)]	Loss: 0.209613
Epoch: 121 [5120/5999 (85%)]	Loss: 0.136855
====> Epoch: 121 Average loss: 0.001185 
Epoch: 122 [0/5999 (0%)]	Loss: 0.266432
Epoch: 122 [2560/5999 (43%)]	Loss: 0.112474
saving model at:122,0.0007733972985297441
Epoch: 122 [5120/5999 (85%)]	Loss: 0.182030
====> Epoch: 122 Average loss: 0.001265 
Epoch: 123 [0/5999 (0%)]	Loss: 0.153269
Epoch: 123 [2560/5999 (43%)]	Loss: 0.254133
Epoch: 123 [5120/5999 (85%)]	Loss: 0.119858
saving model at:123,0.0007730675200000405
====> Epoch: 123 Average loss: 0.001524 
Epoch: 124 [0/5999 (0%)]	Loss: 0.094512
Epoch: 124 [2560/5999 (43%)]	Loss: 0.121077
Epoch: 124 [5120/5999 (85%)]	Loss: 0.128086
====> Epoch: 124 Average loss: 0.001171 
Epoch: 125 [0/5999 (0%)]	Loss: 0.090176
Epoch: 125 [2560/5999 (43%)]	Loss: 0.104447
saving model at:125,0.0007339017260819674
Epoch: 125 [5120/5999 (85%)]	Loss: 0.105526
saving model at:125,0.0007226420920342206
====> Epoch: 125 Average loss: 0.000988 
Epoch: 126 [0/5999 (0%)]	Loss: 0.137129
Epoch: 126 [2560/5999 (43%)]	Loss: 0.207101
Epoch: 126 [5120/5999 (85%)]	Loss: 0.150019
====> Epoch: 126 Average loss: 0.001125 
Epoch: 127 [0/5999 (0%)]	Loss: 0.106551
Epoch: 127 [2560/5999 (43%)]	Loss: 0.099745
saving model at:127,0.0007067448454909027
Epoch: 127 [5120/5999 (85%)]	Loss: 0.137035
====> Epoch: 127 Average loss: 0.001158 
Epoch: 128 [0/5999 (0%)]	Loss: 0.105408
Epoch: 128 [2560/5999 (43%)]	Loss: 0.123285
Epoch: 128 [5120/5999 (85%)]	Loss: 0.148751
====> Epoch: 128 Average loss: 0.001129 
Epoch: 129 [0/5999 (0%)]	Loss: 0.130712
Epoch: 129 [2560/5999 (43%)]	Loss: 0.132400
Epoch: 129 [5120/5999 (85%)]	Loss: 0.121751
====> Epoch: 129 Average loss: 0.001011 
Epoch: 130 [0/5999 (0%)]	Loss: 0.100407
Epoch: 130 [2560/5999 (43%)]	Loss: 0.106543
Epoch: 130 [5120/5999 (85%)]	Loss: 0.094561
====> Epoch: 130 Average loss: 0.001058 
Epoch: 131 [0/5999 (0%)]	Loss: 0.110060
Epoch: 131 [2560/5999 (43%)]	Loss: 0.122589
Epoch: 131 [5120/5999 (85%)]	Loss: 0.160898
====> Epoch: 131 Average loss: 0.001185 
Epoch: 132 [0/5999 (0%)]	Loss: 0.133471
Epoch: 132 [2560/5999 (43%)]	Loss: 0.109917
Epoch: 132 [5120/5999 (85%)]	Loss: 0.292751
====> Epoch: 132 Average loss: 0.001174 
Epoch: 133 [0/5999 (0%)]	Loss: 0.174085
Epoch: 133 [2560/5999 (43%)]	Loss: 0.108249
Epoch: 133 [5120/5999 (85%)]	Loss: 0.168714
====> Epoch: 133 Average loss: 0.001235 
Epoch: 134 [0/5999 (0%)]	Loss: 0.184782
Epoch: 134 [2560/5999 (43%)]	Loss: 0.125180
Epoch: 134 [5120/5999 (85%)]	Loss: 0.138390
====> Epoch: 134 Average loss: 0.001037 
Epoch: 135 [0/5999 (0%)]	Loss: 0.102216
Epoch: 135 [2560/5999 (43%)]	Loss: 0.198729
Epoch: 135 [5120/5999 (85%)]	Loss: 0.141890
====> Epoch: 135 Average loss: 0.001233 
Epoch: 136 [0/5999 (0%)]	Loss: 0.126359
Epoch: 136 [2560/5999 (43%)]	Loss: 0.128248
saving model at:136,0.0007009593644179404
Epoch: 136 [5120/5999 (85%)]	Loss: 0.136990
====> Epoch: 136 Average loss: 0.000997 
Epoch: 137 [0/5999 (0%)]	Loss: 0.122329
Epoch: 137 [2560/5999 (43%)]	Loss: 0.192134
Epoch: 137 [5120/5999 (85%)]	Loss: 0.131716
====> Epoch: 137 Average loss: 0.001121 
Epoch: 138 [0/5999 (0%)]	Loss: 0.188319
Epoch: 138 [2560/5999 (43%)]	Loss: 0.107881
Epoch: 138 [5120/5999 (85%)]	Loss: 0.102168
saving model at:138,0.0006972611909732223
====> Epoch: 138 Average loss: 0.001058 
Epoch: 139 [0/5999 (0%)]	Loss: 0.089062
Epoch: 139 [2560/5999 (43%)]	Loss: 0.093644
saving model at:139,0.0006696000183001161
Epoch: 139 [5120/5999 (85%)]	Loss: 0.095657
====> Epoch: 139 Average loss: 0.000930 
Epoch: 140 [0/5999 (0%)]	Loss: 0.271683
Epoch: 140 [2560/5999 (43%)]	Loss: 0.237635
Epoch: 140 [5120/5999 (85%)]	Loss: 0.096824
====> Epoch: 140 Average loss: 0.001263 
Epoch: 141 [0/5999 (0%)]	Loss: 0.146241
Epoch: 141 [2560/5999 (43%)]	Loss: 0.166562
Epoch: 141 [5120/5999 (85%)]	Loss: 0.129789
====> Epoch: 141 Average loss: 0.001158 
Epoch: 142 [0/5999 (0%)]	Loss: 0.108137
Epoch: 142 [2560/5999 (43%)]	Loss: 0.094030
saving model at:142,0.0006688286541029811
Epoch: 142 [5120/5999 (85%)]	Loss: 0.124272
====> Epoch: 142 Average loss: 0.000933 
Epoch: 143 [0/5999 (0%)]	Loss: 0.178722
Epoch: 143 [2560/5999 (43%)]	Loss: 0.180363
Epoch: 143 [5120/5999 (85%)]	Loss: 0.099819
====> Epoch: 143 Average loss: 0.001133 
Epoch: 144 [0/5999 (0%)]	Loss: 0.177474
Epoch: 144 [2560/5999 (43%)]	Loss: 0.278312
Epoch: 144 [5120/5999 (85%)]	Loss: 0.097803
====> Epoch: 144 Average loss: 0.001162 
Epoch: 145 [0/5999 (0%)]	Loss: 0.114366
Epoch: 145 [2560/5999 (43%)]	Loss: 0.093444
Epoch: 145 [5120/5999 (85%)]	Loss: 0.087633
====> Epoch: 145 Average loss: 0.001018 
Epoch: 146 [0/5999 (0%)]	Loss: 0.106016
Epoch: 146 [2560/5999 (43%)]	Loss: 0.170605
Epoch: 146 [5120/5999 (85%)]	Loss: 0.169024
====> Epoch: 146 Average loss: 0.001083 
Epoch: 147 [0/5999 (0%)]	Loss: 0.219460
Epoch: 147 [2560/5999 (43%)]	Loss: 0.124028
Epoch: 147 [5120/5999 (85%)]	Loss: 0.141561
====> Epoch: 147 Average loss: 0.001158 
Epoch: 148 [0/5999 (0%)]	Loss: 0.090069
Epoch: 148 [2560/5999 (43%)]	Loss: 0.107466
Epoch: 148 [5120/5999 (85%)]	Loss: 0.124506
====> Epoch: 148 Average loss: 0.000989 
Epoch: 149 [0/5999 (0%)]	Loss: 0.087983
Epoch: 149 [2560/5999 (43%)]	Loss: 0.112049
Epoch: 149 [5120/5999 (85%)]	Loss: 0.117596
====> Epoch: 149 Average loss: 0.001010 
Epoch: 150 [0/5999 (0%)]	Loss: 0.149267
Epoch: 150 [2560/5999 (43%)]	Loss: 0.077956
saving model at:150,0.0006685759723186493
Epoch: 150 [5120/5999 (85%)]	Loss: 0.134624
====> Epoch: 150 Average loss: 0.000994 
Epoch: 151 [0/5999 (0%)]	Loss: 0.083940
Epoch: 151 [2560/5999 (43%)]	Loss: 0.094735
Epoch: 151 [5120/5999 (85%)]	Loss: 0.169216
====> Epoch: 151 Average loss: 0.001014 
Epoch: 152 [0/5999 (0%)]	Loss: 0.162509
Epoch: 152 [2560/5999 (43%)]	Loss: 0.094323
Epoch: 152 [5120/5999 (85%)]	Loss: 0.129960
====> Epoch: 152 Average loss: 0.001142 
Epoch: 153 [0/5999 (0%)]	Loss: 0.104297
Epoch: 153 [2560/5999 (43%)]	Loss: 0.117382
Epoch: 153 [5120/5999 (85%)]	Loss: 0.119103
====> Epoch: 153 Average loss: 0.000933 
Epoch: 154 [0/5999 (0%)]	Loss: 0.098747
Epoch: 154 [2560/5999 (43%)]	Loss: 0.177474
Epoch: 154 [5120/5999 (85%)]	Loss: 0.189940
====> Epoch: 154 Average loss: 0.001256 
Epoch: 155 [0/5999 (0%)]	Loss: 0.153883
Epoch: 155 [2560/5999 (43%)]	Loss: 0.102845
Epoch: 155 [5120/5999 (85%)]	Loss: 0.102051
====> Epoch: 155 Average loss: 0.001008 
Epoch: 156 [0/5999 (0%)]	Loss: 0.161930
Epoch: 156 [2560/5999 (43%)]	Loss: 0.109573
Epoch: 156 [5120/5999 (85%)]	Loss: 0.133794
====> Epoch: 156 Average loss: 0.001028 
Epoch: 157 [0/5999 (0%)]	Loss: 0.089270
Epoch: 157 [2560/5999 (43%)]	Loss: 0.096080
saving model at:157,0.0006363060157746076
Epoch: 157 [5120/5999 (85%)]	Loss: 0.085151
====> Epoch: 157 Average loss: 0.000874 
Epoch: 158 [0/5999 (0%)]	Loss: 0.111818
Epoch: 158 [2560/5999 (43%)]	Loss: 0.170849
Epoch: 158 [5120/5999 (85%)]	Loss: 0.114002
====> Epoch: 158 Average loss: 0.001225 
Epoch: 159 [0/5999 (0%)]	Loss: 0.133130
Epoch: 159 [2560/5999 (43%)]	Loss: 0.110707
Epoch: 159 [5120/5999 (85%)]	Loss: 0.174422
====> Epoch: 159 Average loss: 0.001353 
Epoch: 160 [0/5999 (0%)]	Loss: 0.163830
Epoch: 160 [2560/5999 (43%)]	Loss: 0.107088
Epoch: 160 [5120/5999 (85%)]	Loss: 0.082494
====> Epoch: 160 Average loss: 0.001039 
Epoch: 161 [0/5999 (0%)]	Loss: 0.093634
Epoch: 161 [2560/5999 (43%)]	Loss: 0.107395
Epoch: 161 [5120/5999 (85%)]	Loss: 0.178724
====> Epoch: 161 Average loss: 0.000894 
Epoch: 162 [0/5999 (0%)]	Loss: 0.091260
Epoch: 162 [2560/5999 (43%)]	Loss: 0.095317
Epoch: 162 [5120/5999 (85%)]	Loss: 0.102227
====> Epoch: 162 Average loss: 0.001013 
Epoch: 163 [0/5999 (0%)]	Loss: 0.111029
Epoch: 163 [2560/5999 (43%)]	Loss: 0.091965
Epoch: 163 [5120/5999 (85%)]	Loss: 0.099141
====> Epoch: 163 Average loss: 0.000921 
Epoch: 164 [0/5999 (0%)]	Loss: 0.250715
Epoch: 164 [2560/5999 (43%)]	Loss: 0.121757
Epoch: 164 [5120/5999 (85%)]	Loss: 0.129945
====> Epoch: 164 Average loss: 0.001019 
Epoch: 165 [0/5999 (0%)]	Loss: 0.102808
Epoch: 165 [2560/5999 (43%)]	Loss: 0.091281
Epoch: 165 [5120/5999 (85%)]	Loss: 0.153097
====> Epoch: 165 Average loss: 0.001274 
Epoch: 166 [0/5999 (0%)]	Loss: 0.114768
Epoch: 166 [2560/5999 (43%)]	Loss: 0.187032
Epoch: 166 [5120/5999 (85%)]	Loss: 0.116836
====> Epoch: 166 Average loss: 0.001123 
Epoch: 167 [0/5999 (0%)]	Loss: 0.129356
Epoch: 167 [2560/5999 (43%)]	Loss: 0.102494
Epoch: 167 [5120/5999 (85%)]	Loss: 0.218222
====> Epoch: 167 Average loss: 0.001235 
Epoch: 168 [0/5999 (0%)]	Loss: 0.159644
Epoch: 168 [2560/5999 (43%)]	Loss: 0.093721
Epoch: 168 [5120/5999 (85%)]	Loss: 0.103383
====> Epoch: 168 Average loss: 0.001117 
Epoch: 169 [0/5999 (0%)]	Loss: 0.173803
Epoch: 169 [2560/5999 (43%)]	Loss: 0.113612
Epoch: 169 [5120/5999 (85%)]	Loss: 0.169864
====> Epoch: 169 Average loss: 0.001434 
Epoch: 170 [0/5999 (0%)]	Loss: 0.106026
Epoch: 170 [2560/5999 (43%)]	Loss: 0.146723
Epoch: 170 [5120/5999 (85%)]	Loss: 0.171376
====> Epoch: 170 Average loss: 0.001222 
Epoch: 171 [0/5999 (0%)]	Loss: 0.160249
Epoch: 171 [2560/5999 (43%)]	Loss: 0.289270
Epoch: 171 [5120/5999 (85%)]	Loss: 0.118141
====> Epoch: 171 Average loss: 0.001141 
Epoch: 172 [0/5999 (0%)]	Loss: 0.091943
Epoch: 172 [2560/5999 (43%)]	Loss: 0.149735
Epoch: 172 [5120/5999 (85%)]	Loss: 0.104560
saving model at:172,0.000635616013314575
====> Epoch: 172 Average loss: 0.000998 
Epoch: 173 [0/5999 (0%)]	Loss: 0.102002
Epoch: 173 [2560/5999 (43%)]	Loss: 0.133198
Epoch: 173 [5120/5999 (85%)]	Loss: 0.244069
====> Epoch: 173 Average loss: 0.001319 
Epoch: 174 [0/5999 (0%)]	Loss: 0.102010
Epoch: 174 [2560/5999 (43%)]	Loss: 0.102596
Epoch: 174 [5120/5999 (85%)]	Loss: 0.119243
====> Epoch: 174 Average loss: 0.000944 
Epoch: 175 [0/5999 (0%)]	Loss: 0.125210
Epoch: 175 [2560/5999 (43%)]	Loss: 0.321047
Epoch: 175 [5120/5999 (85%)]	Loss: 0.086410
====> Epoch: 175 Average loss: 0.001052 
Epoch: 176 [0/5999 (0%)]	Loss: 0.103045
Epoch: 176 [2560/5999 (43%)]	Loss: 0.103832
Epoch: 176 [5120/5999 (85%)]	Loss: 0.116631
====> Epoch: 176 Average loss: 0.001007 
Epoch: 177 [0/5999 (0%)]	Loss: 0.207516
Epoch: 177 [2560/5999 (43%)]	Loss: 0.110102
Epoch: 177 [5120/5999 (85%)]	Loss: 0.182367
====> Epoch: 177 Average loss: 0.000921 
Epoch: 178 [0/5999 (0%)]	Loss: 0.106451
Epoch: 178 [2560/5999 (43%)]	Loss: 0.133017
Epoch: 178 [5120/5999 (85%)]	Loss: 0.144959
====> Epoch: 178 Average loss: 0.001158 
Epoch: 179 [0/5999 (0%)]	Loss: 0.120800
Epoch: 179 [2560/5999 (43%)]	Loss: 0.094119
Epoch: 179 [5120/5999 (85%)]	Loss: 0.192062
====> Epoch: 179 Average loss: 0.001043 
Epoch: 180 [0/5999 (0%)]	Loss: 0.135912
Epoch: 180 [2560/5999 (43%)]	Loss: 0.094965
saving model at:180,0.0006130199269391597
Epoch: 180 [5120/5999 (85%)]	Loss: 0.173816
====> Epoch: 180 Average loss: 0.001017 
Epoch: 181 [0/5999 (0%)]	Loss: 0.171234
Epoch: 181 [2560/5999 (43%)]	Loss: 0.093333
Epoch: 181 [5120/5999 (85%)]	Loss: 0.092883
====> Epoch: 181 Average loss: 0.000954 
Epoch: 182 [0/5999 (0%)]	Loss: 0.099714
Epoch: 182 [2560/5999 (43%)]	Loss: 0.080479
saving model at:182,0.0006109334323555232
Epoch: 182 [5120/5999 (85%)]	Loss: 0.118941
====> Epoch: 182 Average loss: 0.000932 
Epoch: 183 [0/5999 (0%)]	Loss: 0.104921
Epoch: 183 [2560/5999 (43%)]	Loss: 0.093005
Epoch: 183 [5120/5999 (85%)]	Loss: 0.096489
====> Epoch: 183 Average loss: 0.000863 
Epoch: 184 [0/5999 (0%)]	Loss: 0.117174
Epoch: 184 [2560/5999 (43%)]	Loss: 0.107825
Epoch: 184 [5120/5999 (85%)]	Loss: 0.139272
====> Epoch: 184 Average loss: 0.001039 
Epoch: 185 [0/5999 (0%)]	Loss: 0.074832
Epoch: 185 [2560/5999 (43%)]	Loss: 0.102421
Epoch: 185 [5120/5999 (85%)]	Loss: 0.081103
====> Epoch: 185 Average loss: 0.000955 
Epoch: 186 [0/5999 (0%)]	Loss: 0.091658
Epoch: 186 [2560/5999 (43%)]	Loss: 0.213871
Epoch: 186 [5120/5999 (85%)]	Loss: 0.115563
====> Epoch: 186 Average loss: 0.000982 
Epoch: 187 [0/5999 (0%)]	Loss: 0.134973
Epoch: 187 [2560/5999 (43%)]	Loss: 0.099931
Epoch: 187 [5120/5999 (85%)]	Loss: 0.190701
====> Epoch: 187 Average loss: 0.001275 
Epoch: 188 [0/5999 (0%)]	Loss: 0.105644
Epoch: 188 [2560/5999 (43%)]	Loss: 0.123492
Epoch: 188 [5120/5999 (85%)]	Loss: 0.218511
====> Epoch: 188 Average loss: 0.001166 
Epoch: 189 [0/5999 (0%)]	Loss: 0.163355
Epoch: 189 [2560/5999 (43%)]	Loss: 0.091329
Epoch: 189 [5120/5999 (85%)]	Loss: 0.083715
====> Epoch: 189 Average loss: 0.001002 
Epoch: 190 [0/5999 (0%)]	Loss: 0.124700
Epoch: 190 [2560/5999 (43%)]	Loss: 0.078087
saving model at:190,0.0005747319287620485
Epoch: 190 [5120/5999 (85%)]	Loss: 0.119187
====> Epoch: 190 Average loss: 0.000938 
Epoch: 191 [0/5999 (0%)]	Loss: 0.121078
Epoch: 191 [2560/5999 (43%)]	Loss: 0.196903
Epoch: 191 [5120/5999 (85%)]	Loss: 0.094697
====> Epoch: 191 Average loss: 0.001193 
Epoch: 192 [0/5999 (0%)]	Loss: 0.064819
Epoch: 192 [2560/5999 (43%)]	Loss: 0.086964
Epoch: 192 [5120/5999 (85%)]	Loss: 0.092511
====> Epoch: 192 Average loss: 0.000956 
Epoch: 193 [0/5999 (0%)]	Loss: 0.094962
Epoch: 193 [2560/5999 (43%)]	Loss: 0.090259
saving model at:193,0.0005379574792459607
Epoch: 193 [5120/5999 (85%)]	Loss: 0.072153
saving model at:193,0.0005277591231279075
====> Epoch: 193 Average loss: 0.000783 
Epoch: 194 [0/5999 (0%)]	Loss: 0.080752
Epoch: 194 [2560/5999 (43%)]	Loss: 0.084127
Epoch: 194 [5120/5999 (85%)]	Loss: 0.142915
====> Epoch: 194 Average loss: 0.000985 
Epoch: 195 [0/5999 (0%)]	Loss: 0.075923
Epoch: 195 [2560/5999 (43%)]	Loss: 0.100672
Epoch: 195 [5120/5999 (85%)]	Loss: 0.191092
====> Epoch: 195 Average loss: 0.000914 
Epoch: 196 [0/5999 (0%)]	Loss: 0.075378
Epoch: 196 [2560/5999 (43%)]	Loss: 0.070116
saving model at:196,0.0005095547139644623
Epoch: 196 [5120/5999 (85%)]	Loss: 0.081956
====> Epoch: 196 Average loss: 0.000755 
Epoch: 197 [0/5999 (0%)]	Loss: 0.061326
Epoch: 197 [2560/5999 (43%)]	Loss: 0.317180
Epoch: 197 [5120/5999 (85%)]	Loss: 0.078052
====> Epoch: 197 Average loss: 0.000859 
Epoch: 198 [0/5999 (0%)]	Loss: 0.065831
Epoch: 198 [2560/5999 (43%)]	Loss: 0.063473
Epoch: 198 [5120/5999 (85%)]	Loss: 0.140393
====> Epoch: 198 Average loss: 0.001066 
Epoch: 199 [0/5999 (0%)]	Loss: 0.078388
Epoch: 199 [2560/5999 (43%)]	Loss: 0.070701
saving model at:199,0.00044063048949465153
Epoch: 199 [5120/5999 (85%)]	Loss: 0.056585
saving model at:199,0.0004298123337794095
====> Epoch: 199 Average loss: 0.000666 
Epoch: 200 [0/5999 (0%)]	Loss: 0.114763
Epoch: 200 [2560/5999 (43%)]	Loss: 0.074510
Epoch: 200 [5120/5999 (85%)]	Loss: 0.069596
====> Epoch: 200 Average loss: 0.000781 
Epoch: 201 [0/5999 (0%)]	Loss: 0.055165
Epoch: 201 [2560/5999 (43%)]	Loss: 0.135036
Epoch: 201 [5120/5999 (85%)]	Loss: 0.069996
====> Epoch: 201 Average loss: 0.000715 
Epoch: 202 [0/5999 (0%)]	Loss: 0.060733
Epoch: 202 [2560/5999 (43%)]	Loss: 0.164290
Epoch: 202 [5120/5999 (85%)]	Loss: 0.084581
====> Epoch: 202 Average loss: 0.000901 
Epoch: 203 [0/5999 (0%)]	Loss: 0.077105
Epoch: 203 [2560/5999 (43%)]	Loss: 0.142749
Epoch: 203 [5120/5999 (85%)]	Loss: 0.072228
====> Epoch: 203 Average loss: 0.000919 
Epoch: 204 [0/5999 (0%)]	Loss: 0.059082
Epoch: 204 [2560/5999 (43%)]	Loss: 0.050992
saving model at:204,0.0004067881952505559
Epoch: 204 [5120/5999 (85%)]	Loss: 0.146485
====> Epoch: 204 Average loss: 0.000637 
Epoch: 205 [0/5999 (0%)]	Loss: 0.056535
Epoch: 205 [2560/5999 (43%)]	Loss: 0.057741
Epoch: 205 [5120/5999 (85%)]	Loss: 0.060016
saving model at:205,0.0003897578720934689
====> Epoch: 205 Average loss: 0.000687 
Epoch: 206 [0/5999 (0%)]	Loss: 0.098921
Epoch: 206 [2560/5999 (43%)]	Loss: 0.053352
saving model at:206,0.00034646329283714297
Epoch: 206 [5120/5999 (85%)]	Loss: 0.047304
====> Epoch: 206 Average loss: 0.000692 
Epoch: 207 [0/5999 (0%)]	Loss: 0.194581
Epoch: 207 [2560/5999 (43%)]	Loss: 0.072458
Epoch: 207 [5120/5999 (85%)]	Loss: 0.053745
====> Epoch: 207 Average loss: 0.000518 
Epoch: 208 [0/5999 (0%)]	Loss: 0.100239
Epoch: 208 [2560/5999 (43%)]	Loss: 0.135509
Epoch: 208 [5120/5999 (85%)]	Loss: 0.117401
====> Epoch: 208 Average loss: 0.000926 
Epoch: 209 [0/5999 (0%)]	Loss: 0.070026
Epoch: 209 [2560/5999 (43%)]	Loss: 0.120773
Epoch: 209 [5120/5999 (85%)]	Loss: 0.057118
====> Epoch: 209 Average loss: 0.000726 
Epoch: 210 [0/5999 (0%)]	Loss: 0.167728
Epoch: 210 [2560/5999 (43%)]	Loss: 0.100682
Epoch: 210 [5120/5999 (85%)]	Loss: 0.049889
====> Epoch: 210 Average loss: 0.000641 
Epoch: 211 [0/5999 (0%)]	Loss: 0.285849
Epoch: 211 [2560/5999 (43%)]	Loss: 0.053627
Epoch: 211 [5120/5999 (85%)]	Loss: 0.079068
====> Epoch: 211 Average loss: 0.000662 
Epoch: 212 [0/5999 (0%)]	Loss: 0.221645
Epoch: 212 [2560/5999 (43%)]	Loss: 0.069677
Epoch: 212 [5120/5999 (85%)]	Loss: 0.045623
====> Epoch: 212 Average loss: 0.000723 
Epoch: 213 [0/5999 (0%)]	Loss: 0.044983
Epoch: 213 [2560/5999 (43%)]	Loss: 0.070189
Epoch: 213 [5120/5999 (85%)]	Loss: 0.071573
====> Epoch: 213 Average loss: 0.000893 
Epoch: 214 [0/5999 (0%)]	Loss: 0.084144
Epoch: 214 [2560/5999 (43%)]	Loss: 0.059566
saving model at:214,0.00031498578609898684
Epoch: 214 [5120/5999 (85%)]	Loss: 0.110688
====> Epoch: 214 Average loss: 0.000656 
Epoch: 215 [0/5999 (0%)]	Loss: 0.070241
Epoch: 215 [2560/5999 (43%)]	Loss: 0.070181
Epoch: 215 [5120/5999 (85%)]	Loss: 0.054612
====> Epoch: 215 Average loss: 0.000795 
Epoch: 216 [0/5999 (0%)]	Loss: 0.062451
Epoch: 216 [2560/5999 (43%)]	Loss: 0.052213
saving model at:216,0.00029097299044951794
Epoch: 216 [5120/5999 (85%)]	Loss: 0.068856
====> Epoch: 216 Average loss: 0.000469 
Epoch: 217 [0/5999 (0%)]	Loss: 0.070535
Epoch: 217 [2560/5999 (43%)]	Loss: 0.173517
Epoch: 217 [5120/5999 (85%)]	Loss: 0.049308
====> Epoch: 217 Average loss: 0.000651 
Epoch: 218 [0/5999 (0%)]	Loss: 0.071303
Epoch: 218 [2560/5999 (43%)]	Loss: 0.111982
Epoch: 218 [5120/5999 (85%)]	Loss: 0.049288
====> Epoch: 218 Average loss: 0.000783 
Epoch: 219 [0/5999 (0%)]	Loss: 0.066996
Epoch: 219 [2560/5999 (43%)]	Loss: 0.096429
Epoch: 219 [5120/5999 (85%)]	Loss: 0.045390
saving model at:219,0.00029011970828287303
====> Epoch: 219 Average loss: 0.000460 
Epoch: 220 [0/5999 (0%)]	Loss: 0.128317
Epoch: 220 [2560/5999 (43%)]	Loss: 0.047068
saving model at:220,0.00028457143786363306
Epoch: 220 [5120/5999 (85%)]	Loss: 0.036616
saving model at:220,0.00026506474381312727
====> Epoch: 220 Average loss: 0.000527 
Epoch: 221 [0/5999 (0%)]	Loss: 0.049859
Epoch: 221 [2560/5999 (43%)]	Loss: 0.078995
Epoch: 221 [5120/5999 (85%)]	Loss: 0.065564
====> Epoch: 221 Average loss: 0.000651 
Epoch: 222 [0/5999 (0%)]	Loss: 0.125976
Epoch: 222 [2560/5999 (43%)]	Loss: 0.073740
Epoch: 222 [5120/5999 (85%)]	Loss: 0.081351
====> Epoch: 222 Average loss: 0.000631 
Epoch: 223 [0/5999 (0%)]	Loss: 0.046810
Epoch: 223 [2560/5999 (43%)]	Loss: 0.195998
Epoch: 223 [5120/5999 (85%)]	Loss: 0.059546
====> Epoch: 223 Average loss: 0.000761 
Epoch: 224 [0/5999 (0%)]	Loss: 0.077653
Epoch: 224 [2560/5999 (43%)]	Loss: 0.127604
Epoch: 224 [5120/5999 (85%)]	Loss: 0.056534
====> Epoch: 224 Average loss: 0.000611 
Epoch: 225 [0/5999 (0%)]	Loss: 0.086593
Epoch: 225 [2560/5999 (43%)]	Loss: 0.114922
Epoch: 225 [5120/5999 (85%)]	Loss: 0.051812
====> Epoch: 225 Average loss: 0.000786 
Epoch: 226 [0/5999 (0%)]	Loss: 0.100112
Epoch: 226 [2560/5999 (43%)]	Loss: 0.058970
Epoch: 226 [5120/5999 (85%)]	Loss: 0.112060
====> Epoch: 226 Average loss: 0.000573 
Epoch: 227 [0/5999 (0%)]	Loss: 0.196258
Epoch: 227 [2560/5999 (43%)]	Loss: 0.050986
Epoch: 227 [5120/5999 (85%)]	Loss: 0.043574
====> Epoch: 227 Average loss: 0.000665 
Epoch: 228 [0/5999 (0%)]	Loss: 0.075559
Epoch: 228 [2560/5999 (43%)]	Loss: 0.167310
Epoch: 228 [5120/5999 (85%)]	Loss: 0.057785
====> Epoch: 228 Average loss: 0.000663 
Epoch: 229 [0/5999 (0%)]	Loss: 0.152283
Epoch: 229 [2560/5999 (43%)]	Loss: 0.083866
Epoch: 229 [5120/5999 (85%)]	Loss: 0.061087
====> Epoch: 229 Average loss: 0.000624 
Epoch: 230 [0/5999 (0%)]	Loss: 0.043030
Epoch: 230 [2560/5999 (43%)]	Loss: 0.044582
Epoch: 230 [5120/5999 (85%)]	Loss: 0.111356
====> Epoch: 230 Average loss: 0.000516 
Epoch: 231 [0/5999 (0%)]	Loss: 0.087101
Epoch: 231 [2560/5999 (43%)]	Loss: 0.070360
Epoch: 231 [5120/5999 (85%)]	Loss: 0.094488
====> Epoch: 231 Average loss: 0.000787 
Epoch: 232 [0/5999 (0%)]	Loss: 0.169883
Epoch: 232 [2560/5999 (43%)]	Loss: 0.047183
Epoch: 232 [5120/5999 (85%)]	Loss: 0.037871
saving model at:232,0.00024718175129964946
====> Epoch: 232 Average loss: 0.000527 
Epoch: 233 [0/5999 (0%)]	Loss: 0.042055
Epoch: 233 [2560/5999 (43%)]	Loss: 0.115150
Epoch: 233 [5120/5999 (85%)]	Loss: 0.040786
====> Epoch: 233 Average loss: 0.000552 
Epoch: 234 [0/5999 (0%)]	Loss: 0.052436
Epoch: 234 [2560/5999 (43%)]	Loss: 0.054309
Epoch: 234 [5120/5999 (85%)]	Loss: 0.050916
====> Epoch: 234 Average loss: 0.000699 
Epoch: 235 [0/5999 (0%)]	Loss: 0.060080
Epoch: 235 [2560/5999 (43%)]	Loss: 0.051158
Epoch: 235 [5120/5999 (85%)]	Loss: 0.194937
====> Epoch: 235 Average loss: 0.000640 
Epoch: 236 [0/5999 (0%)]	Loss: 0.070809
Epoch: 236 [2560/5999 (43%)]	Loss: 0.044464
Epoch: 236 [5120/5999 (85%)]	Loss: 0.058621
====> Epoch: 236 Average loss: 0.000498 
Epoch: 237 [0/5999 (0%)]	Loss: 0.083725
Epoch: 237 [2560/5999 (43%)]	Loss: 0.043213
Epoch: 237 [5120/5999 (85%)]	Loss: 0.055877
====> Epoch: 237 Average loss: 0.000486 
Epoch: 238 [0/5999 (0%)]	Loss: 0.052112
Epoch: 238 [2560/5999 (43%)]	Loss: 0.055012
Epoch: 238 [5120/5999 (85%)]	Loss: 0.039568
====> Epoch: 238 Average loss: 0.000631 
Epoch: 239 [0/5999 (0%)]	Loss: 0.074747
Epoch: 239 [2560/5999 (43%)]	Loss: 0.099895
Epoch: 239 [5120/5999 (85%)]	Loss: 0.036265
saving model at:239,0.00022614256350789218
====> Epoch: 239 Average loss: 0.000623 
Epoch: 240 [0/5999 (0%)]	Loss: 0.039718
Epoch: 240 [2560/5999 (43%)]	Loss: 0.069258
Epoch: 240 [5120/5999 (85%)]	Loss: 0.063452
====> Epoch: 240 Average loss: 0.000767 
Epoch: 241 [0/5999 (0%)]	Loss: 0.079267
Epoch: 241 [2560/5999 (43%)]	Loss: 0.057828
Epoch: 241 [5120/5999 (85%)]	Loss: 0.059845
====> Epoch: 241 Average loss: 0.000561 
Epoch: 242 [0/5999 (0%)]	Loss: 0.045809
Epoch: 242 [2560/5999 (43%)]	Loss: 0.035416
Epoch: 242 [5120/5999 (85%)]	Loss: 0.054002
====> Epoch: 242 Average loss: 0.000545 
Epoch: 243 [0/5999 (0%)]	Loss: 0.073015
Epoch: 243 [2560/5999 (43%)]	Loss: 0.042435
Epoch: 243 [5120/5999 (85%)]	Loss: 0.112127
====> Epoch: 243 Average loss: 0.000650 
Epoch: 244 [0/5999 (0%)]	Loss: 0.047155
Epoch: 244 [2560/5999 (43%)]	Loss: 0.036992
Epoch: 244 [5120/5999 (85%)]	Loss: 0.058449
====> Epoch: 244 Average loss: 0.000467 
Epoch: 245 [0/5999 (0%)]	Loss: 0.029981
Epoch: 245 [2560/5999 (43%)]	Loss: 0.147990
Epoch: 245 [5120/5999 (85%)]	Loss: 0.057371
====> Epoch: 245 Average loss: 0.000497 
Epoch: 246 [0/5999 (0%)]	Loss: 0.036399
Epoch: 246 [2560/5999 (43%)]	Loss: 0.043458
Epoch: 246 [5120/5999 (85%)]	Loss: 0.066377
====> Epoch: 246 Average loss: 0.000481 
Epoch: 247 [0/5999 (0%)]	Loss: 0.058946
Epoch: 247 [2560/5999 (43%)]	Loss: 0.040554
Epoch: 247 [5120/5999 (85%)]	Loss: 0.057346
====> Epoch: 247 Average loss: 0.000575 
Epoch: 248 [0/5999 (0%)]	Loss: 0.107060
Epoch: 248 [2560/5999 (43%)]	Loss: 0.160510
Epoch: 248 [5120/5999 (85%)]	Loss: 0.063343
====> Epoch: 248 Average loss: 0.000549 
Epoch: 249 [0/5999 (0%)]	Loss: 0.070641
Epoch: 249 [2560/5999 (43%)]	Loss: 0.056074
Epoch: 249 [5120/5999 (85%)]	Loss: 0.077450
====> Epoch: 249 Average loss: 0.000517 
Epoch: 250 [0/5999 (0%)]	Loss: 0.052987
Epoch: 250 [2560/5999 (43%)]	Loss: 0.030117
Epoch: 250 [5120/5999 (85%)]	Loss: 0.041881
====> Epoch: 250 Average loss: 0.000578 
Epoch: 251 [0/5999 (0%)]	Loss: 0.076440
Epoch: 251 [2560/5999 (43%)]	Loss: 0.032061
Epoch: 251 [5120/5999 (85%)]	Loss: 0.043305
====> Epoch: 251 Average loss: 0.000588 
Epoch: 252 [0/5999 (0%)]	Loss: 0.044048
Epoch: 252 [2560/5999 (43%)]	Loss: 0.083382
Epoch: 252 [5120/5999 (85%)]	Loss: 0.083826
====> Epoch: 252 Average loss: 0.000669 
Epoch: 253 [0/5999 (0%)]	Loss: 0.040725
Epoch: 253 [2560/5999 (43%)]	Loss: 0.095120
Epoch: 253 [5120/5999 (85%)]	Loss: 0.065774
====> Epoch: 253 Average loss: 0.000530 
Epoch: 254 [0/5999 (0%)]	Loss: 0.037809
Epoch: 254 [2560/5999 (43%)]	Loss: 0.062020
Epoch: 254 [5120/5999 (85%)]	Loss: 0.087247
====> Epoch: 254 Average loss: 0.000440 
Epoch: 255 [0/5999 (0%)]	Loss: 0.078532
Epoch: 255 [2560/5999 (43%)]	Loss: 0.039296
saving model at:255,0.0002140914995688945
Epoch: 255 [5120/5999 (85%)]	Loss: 0.095875
====> Epoch: 255 Average loss: 0.000446 
Epoch: 256 [0/5999 (0%)]	Loss: 0.027003
Epoch: 256 [2560/5999 (43%)]	Loss: 0.047724
saving model at:256,0.00019499245786573738
Epoch: 256 [5120/5999 (85%)]	Loss: 0.039456
====> Epoch: 256 Average loss: 0.000433 
Epoch: 257 [0/5999 (0%)]	Loss: 0.031170
Epoch: 257 [2560/5999 (43%)]	Loss: 0.035410
Epoch: 257 [5120/5999 (85%)]	Loss: 0.032661
====> Epoch: 257 Average loss: 0.000405 
Epoch: 258 [0/5999 (0%)]	Loss: 0.054753
Epoch: 258 [2560/5999 (43%)]	Loss: 0.036978
Epoch: 258 [5120/5999 (85%)]	Loss: 0.051928
====> Epoch: 258 Average loss: 0.000603 
Epoch: 259 [0/5999 (0%)]	Loss: 0.031723
Epoch: 259 [2560/5999 (43%)]	Loss: 0.037841
Epoch: 259 [5120/5999 (85%)]	Loss: 0.041932
====> Epoch: 259 Average loss: 0.000560 
Epoch: 260 [0/5999 (0%)]	Loss: 0.064117
Epoch: 260 [2560/5999 (43%)]	Loss: 0.038922
Epoch: 260 [5120/5999 (85%)]	Loss: 0.155185
====> Epoch: 260 Average loss: 0.000565 
Epoch: 261 [0/5999 (0%)]	Loss: 0.049479
Epoch: 261 [2560/5999 (43%)]	Loss: 0.043650
Epoch: 261 [5120/5999 (85%)]	Loss: 0.160600
====> Epoch: 261 Average loss: 0.000531 
Epoch: 262 [0/5999 (0%)]	Loss: 0.035416
Epoch: 262 [2560/5999 (43%)]	Loss: 0.033437
Epoch: 262 [5120/5999 (85%)]	Loss: 0.133219
====> Epoch: 262 Average loss: 0.000469 
Epoch: 263 [0/5999 (0%)]	Loss: 0.040439
Epoch: 263 [2560/5999 (43%)]	Loss: 0.057076
Epoch: 263 [5120/5999 (85%)]	Loss: 0.060137
====> Epoch: 263 Average loss: 0.000484 
Epoch: 264 [0/5999 (0%)]	Loss: 0.049650
Epoch: 264 [2560/5999 (43%)]	Loss: 0.030507
saving model at:264,0.0001879410535329953
Epoch: 264 [5120/5999 (85%)]	Loss: 0.062594
====> Epoch: 264 Average loss: 0.000510 
Epoch: 265 [0/5999 (0%)]	Loss: 0.062394
Epoch: 265 [2560/5999 (43%)]	Loss: 0.037366
Epoch: 265 [5120/5999 (85%)]	Loss: 0.032492
====> Epoch: 265 Average loss: 0.000431 
Epoch: 266 [0/5999 (0%)]	Loss: 0.044687
Epoch: 266 [2560/5999 (43%)]	Loss: 0.121090
Epoch: 266 [5120/5999 (85%)]	Loss: 0.042821
====> Epoch: 266 Average loss: 0.000487 
Epoch: 267 [0/5999 (0%)]	Loss: 0.062186
Epoch: 267 [2560/5999 (43%)]	Loss: 0.030921
Epoch: 267 [5120/5999 (85%)]	Loss: 0.090241
====> Epoch: 267 Average loss: 0.000739 
Epoch: 268 [0/5999 (0%)]	Loss: 0.054839
Epoch: 268 [2560/5999 (43%)]	Loss: 0.038509
Epoch: 268 [5120/5999 (85%)]	Loss: 0.046442
====> Epoch: 268 Average loss: 0.000526 
Epoch: 269 [0/5999 (0%)]	Loss: 0.061938
Epoch: 269 [2560/5999 (43%)]	Loss: 0.039805
Epoch: 269 [5120/5999 (85%)]	Loss: 0.039925
====> Epoch: 269 Average loss: 0.000411 
Epoch: 270 [0/5999 (0%)]	Loss: 0.029606
Epoch: 270 [2560/5999 (43%)]	Loss: 0.051877
Epoch: 270 [5120/5999 (85%)]	Loss: 0.060672
====> Epoch: 270 Average loss: 0.000353 
Epoch: 271 [0/5999 (0%)]	Loss: 0.035792
Epoch: 271 [2560/5999 (43%)]	Loss: 0.083719
Epoch: 271 [5120/5999 (85%)]	Loss: 0.058789
====> Epoch: 271 Average loss: 0.000488 
Epoch: 272 [0/5999 (0%)]	Loss: 0.070799
Epoch: 272 [2560/5999 (43%)]	Loss: 0.039247
Epoch: 272 [5120/5999 (85%)]	Loss: 0.036096
saving model at:272,0.00017532187211327256
====> Epoch: 272 Average loss: 0.000454 
Epoch: 273 [0/5999 (0%)]	Loss: 0.047657
Epoch: 273 [2560/5999 (43%)]	Loss: 0.023120
Epoch: 273 [5120/5999 (85%)]	Loss: 0.072522
====> Epoch: 273 Average loss: 0.000598 
Epoch: 274 [0/5999 (0%)]	Loss: 0.024353
Epoch: 274 [2560/5999 (43%)]	Loss: 0.133105
Epoch: 274 [5120/5999 (85%)]	Loss: 0.042594
====> Epoch: 274 Average loss: 0.000667 
Epoch: 275 [0/5999 (0%)]	Loss: 0.173838
Epoch: 275 [2560/5999 (43%)]	Loss: 0.049079
Epoch: 275 [5120/5999 (85%)]	Loss: 0.041047
====> Epoch: 275 Average loss: 0.000575 
Epoch: 276 [0/5999 (0%)]	Loss: 0.027252
Epoch: 276 [2560/5999 (43%)]	Loss: 0.041251
Epoch: 276 [5120/5999 (85%)]	Loss: 0.046998
====> Epoch: 276 Average loss: 0.000512 
Epoch: 277 [0/5999 (0%)]	Loss: 0.022829
Epoch: 277 [2560/5999 (43%)]	Loss: 0.040912
Epoch: 277 [5120/5999 (85%)]	Loss: 0.075500
====> Epoch: 277 Average loss: 0.000488 
Epoch: 278 [0/5999 (0%)]	Loss: 0.282764
Epoch: 278 [2560/5999 (43%)]	Loss: 0.261746
Epoch: 278 [5120/5999 (85%)]	Loss: 0.052739
====> Epoch: 278 Average loss: 0.000602 
Epoch: 279 [0/5999 (0%)]	Loss: 0.085520
Epoch: 279 [2560/5999 (43%)]	Loss: 0.030694
Epoch: 279 [5120/5999 (85%)]	Loss: 0.022116
====> Epoch: 279 Average loss: 0.000421 
Epoch: 280 [0/5999 (0%)]	Loss: 0.082244
Epoch: 280 [2560/5999 (43%)]	Loss: 0.035632
Epoch: 280 [5120/5999 (85%)]	Loss: 0.063573
====> Epoch: 280 Average loss: 0.000685 
Epoch: 281 [0/5999 (0%)]	Loss: 0.049272
Epoch: 281 [2560/5999 (43%)]	Loss: 0.032450
Epoch: 281 [5120/5999 (85%)]	Loss: 0.029795
====> Epoch: 281 Average loss: 0.000494 
Epoch: 282 [0/5999 (0%)]	Loss: 0.089536
Epoch: 282 [2560/5999 (43%)]	Loss: 0.076575
Epoch: 282 [5120/5999 (85%)]	Loss: 0.059604
====> Epoch: 282 Average loss: 0.000648 
Epoch: 283 [0/5999 (0%)]	Loss: 0.047161
Epoch: 283 [2560/5999 (43%)]	Loss: 0.049489
Epoch: 283 [5120/5999 (85%)]	Loss: 0.051358
====> Epoch: 283 Average loss: 0.000478 
Epoch: 284 [0/5999 (0%)]	Loss: 0.029356
Epoch: 284 [2560/5999 (43%)]	Loss: 0.083682
Epoch: 284 [5120/5999 (85%)]	Loss: 0.056952
====> Epoch: 284 Average loss: 0.000369 
Epoch: 285 [0/5999 (0%)]	Loss: 0.069657
Epoch: 285 [2560/5999 (43%)]	Loss: 0.064011
Epoch: 285 [5120/5999 (85%)]	Loss: 0.052213
====> Epoch: 285 Average loss: 0.000578 
Epoch: 286 [0/5999 (0%)]	Loss: 0.033133
Epoch: 286 [2560/5999 (43%)]	Loss: 0.132903
Epoch: 286 [5120/5999 (85%)]	Loss: 0.067851
====> Epoch: 286 Average loss: 0.000451 
Epoch: 287 [0/5999 (0%)]	Loss: 0.063499
Epoch: 287 [2560/5999 (43%)]	Loss: 0.028581
saving model at:287,0.00017266248841769994
Epoch: 287 [5120/5999 (85%)]	Loss: 0.238052
====> Epoch: 287 Average loss: 0.000532 
Epoch: 288 [0/5999 (0%)]	Loss: 0.099925
Epoch: 288 [2560/5999 (43%)]	Loss: 0.064193
Epoch: 288 [5120/5999 (85%)]	Loss: 0.044938
====> Epoch: 288 Average loss: 0.000479 
Epoch: 289 [0/5999 (0%)]	Loss: 0.043103
Epoch: 289 [2560/5999 (43%)]	Loss: 0.030330
Epoch: 289 [5120/5999 (85%)]	Loss: 0.095232
====> Epoch: 289 Average loss: 0.000418 
Epoch: 290 [0/5999 (0%)]	Loss: 0.030198
Epoch: 290 [2560/5999 (43%)]	Loss: 0.074268
Epoch: 290 [5120/5999 (85%)]	Loss: 0.041839
====> Epoch: 290 Average loss: 0.000639 
Epoch: 291 [0/5999 (0%)]	Loss: 0.036089
Epoch: 291 [2560/5999 (43%)]	Loss: 0.136732
Epoch: 291 [5120/5999 (85%)]	Loss: 0.098957
====> Epoch: 291 Average loss: 0.000653 
Epoch: 292 [0/5999 (0%)]	Loss: 0.038110
Epoch: 292 [2560/5999 (43%)]	Loss: 0.041174
Epoch: 292 [5120/5999 (85%)]	Loss: 0.033541
====> Epoch: 292 Average loss: 0.000544 
Epoch: 293 [0/5999 (0%)]	Loss: 0.048311
Epoch: 293 [2560/5999 (43%)]	Loss: 0.082681
Epoch: 293 [5120/5999 (85%)]	Loss: 0.026839
====> Epoch: 293 Average loss: 0.000608 
Epoch: 294 [0/5999 (0%)]	Loss: 0.082511
Epoch: 294 [2560/5999 (43%)]	Loss: 0.028466
Epoch: 294 [5120/5999 (85%)]	Loss: 0.117705
====> Epoch: 294 Average loss: 0.000433 
Epoch: 295 [0/5999 (0%)]	Loss: 0.033786
Epoch: 295 [2560/5999 (43%)]	Loss: 0.106777
Epoch: 295 [5120/5999 (85%)]	Loss: 0.100186
====> Epoch: 295 Average loss: 0.000763 
Epoch: 296 [0/5999 (0%)]	Loss: 0.209092
Epoch: 296 [2560/5999 (43%)]	Loss: 0.043116
Epoch: 296 [5120/5999 (85%)]	Loss: 0.056378
====> Epoch: 296 Average loss: 0.000585 
Epoch: 297 [0/5999 (0%)]	Loss: 0.026680
Epoch: 297 [2560/5999 (43%)]	Loss: 0.031092
Epoch: 297 [5120/5999 (85%)]	Loss: 0.132111
====> Epoch: 297 Average loss: 0.000388 
Epoch: 298 [0/5999 (0%)]	Loss: 0.062821
Epoch: 298 [2560/5999 (43%)]	Loss: 0.036244
Epoch: 298 [5120/5999 (85%)]	Loss: 0.024093
====> Epoch: 298 Average loss: 0.000446 
Epoch: 299 [0/5999 (0%)]	Loss: 0.035095
Epoch: 299 [2560/5999 (43%)]	Loss: 0.047888
Epoch: 299 [5120/5999 (85%)]	Loss: 0.089414
====> Epoch: 299 Average loss: 0.000386 
Epoch: 300 [0/5999 (0%)]	Loss: 0.041814
Epoch: 300 [2560/5999 (43%)]	Loss: 0.038000
Epoch: 300 [5120/5999 (85%)]	Loss: 0.079283
====> Epoch: 300 Average loss: 0.000563 
Epoch: 301 [0/5999 (0%)]	Loss: 0.073595
Epoch: 301 [2560/5999 (43%)]	Loss: 0.060901
Epoch: 301 [5120/5999 (85%)]	Loss: 0.093187
====> Epoch: 301 Average loss: 0.000516 
Epoch: 302 [0/5999 (0%)]	Loss: 0.095033
Epoch: 302 [2560/5999 (43%)]	Loss: 0.073888
Epoch: 302 [5120/5999 (85%)]	Loss: 0.049718
====> Epoch: 302 Average loss: 0.000563 
Epoch: 303 [0/5999 (0%)]	Loss: 0.033739
Epoch: 303 [2560/5999 (43%)]	Loss: 0.041827
Epoch: 303 [5120/5999 (85%)]	Loss: 0.026700
====> Epoch: 303 Average loss: 0.000587 
Epoch: 304 [0/5999 (0%)]	Loss: 0.056577
Epoch: 304 [2560/5999 (43%)]	Loss: 0.029132
Epoch: 304 [5120/5999 (85%)]	Loss: 0.081931
====> Epoch: 304 Average loss: 0.000525 
Epoch: 305 [0/5999 (0%)]	Loss: 0.069132
Epoch: 305 [2560/5999 (43%)]	Loss: 0.035950
Epoch: 305 [5120/5999 (85%)]	Loss: 0.027938
saving model at:305,0.00015788745624013245
====> Epoch: 305 Average loss: 0.000404 
Epoch: 306 [0/5999 (0%)]	Loss: 0.046803
Epoch: 306 [2560/5999 (43%)]	Loss: 0.029940
Epoch: 306 [5120/5999 (85%)]	Loss: 0.052736
====> Epoch: 306 Average loss: 0.000394 
Epoch: 307 [0/5999 (0%)]	Loss: 0.038274
Epoch: 307 [2560/5999 (43%)]	Loss: 0.107869
Epoch: 307 [5120/5999 (85%)]	Loss: 0.086990
====> Epoch: 307 Average loss: 0.000462 
Epoch: 308 [0/5999 (0%)]	Loss: 0.077896
Epoch: 308 [2560/5999 (43%)]	Loss: 0.030863
Epoch: 308 [5120/5999 (85%)]	Loss: 0.026651
====> Epoch: 308 Average loss: 0.000490 
Epoch: 309 [0/5999 (0%)]	Loss: 0.043001
Epoch: 309 [2560/5999 (43%)]	Loss: 0.068884
Epoch: 309 [5120/5999 (85%)]	Loss: 0.102923
====> Epoch: 309 Average loss: 0.000376 
Epoch: 310 [0/5999 (0%)]	Loss: 0.057607
Epoch: 310 [2560/5999 (43%)]	Loss: 0.069042
Epoch: 310 [5120/5999 (85%)]	Loss: 0.026990
saving model at:310,0.00014430340880062432
====> Epoch: 310 Average loss: 0.000448 
Epoch: 311 [0/5999 (0%)]	Loss: 0.130751
Epoch: 311 [2560/5999 (43%)]	Loss: 0.111640
Epoch: 311 [5120/5999 (85%)]	Loss: 0.081460
====> Epoch: 311 Average loss: 0.000526 
Epoch: 312 [0/5999 (0%)]	Loss: 0.040290
Epoch: 312 [2560/5999 (43%)]	Loss: 0.051534
Epoch: 312 [5120/5999 (85%)]	Loss: 0.036168
====> Epoch: 312 Average loss: 0.000389 
Epoch: 313 [0/5999 (0%)]	Loss: 0.019435
Epoch: 313 [2560/5999 (43%)]	Loss: 0.055226
Epoch: 313 [5120/5999 (85%)]	Loss: 0.065065
====> Epoch: 313 Average loss: 0.000442 
Epoch: 314 [0/5999 (0%)]	Loss: 0.026444
Epoch: 314 [2560/5999 (43%)]	Loss: 0.043912
Epoch: 314 [5120/5999 (85%)]	Loss: 0.069554
====> Epoch: 314 Average loss: 0.000370 
Epoch: 315 [0/5999 (0%)]	Loss: 0.075045
Epoch: 315 [2560/5999 (43%)]	Loss: 0.035802
Epoch: 315 [5120/5999 (85%)]	Loss: 0.087376
====> Epoch: 315 Average loss: 0.000406 
Epoch: 316 [0/5999 (0%)]	Loss: 0.039417
Epoch: 316 [2560/5999 (43%)]	Loss: 0.023248
Epoch: 316 [5120/5999 (85%)]	Loss: 0.024265
====> Epoch: 316 Average loss: 0.000341 
Epoch: 317 [0/5999 (0%)]	Loss: 0.085895
Epoch: 317 [2560/5999 (43%)]	Loss: 0.040885
Epoch: 317 [5120/5999 (85%)]	Loss: 0.049803
====> Epoch: 317 Average loss: 0.000604 
Epoch: 318 [0/5999 (0%)]	Loss: 0.096699
Epoch: 318 [2560/5999 (43%)]	Loss: 0.019808
Epoch: 318 [5120/5999 (85%)]	Loss: 0.041303
====> Epoch: 318 Average loss: 0.000458 
Epoch: 319 [0/5999 (0%)]	Loss: 0.089998
Epoch: 319 [2560/5999 (43%)]	Loss: 0.024518
Epoch: 319 [5120/5999 (85%)]	Loss: 0.098251
====> Epoch: 319 Average loss: 0.000437 
Epoch: 320 [0/5999 (0%)]	Loss: 0.053276
Epoch: 320 [2560/5999 (43%)]	Loss: 0.034601
Epoch: 320 [5120/5999 (85%)]	Loss: 0.061846
====> Epoch: 320 Average loss: 0.000409 
Epoch: 321 [0/5999 (0%)]	Loss: 0.035790
Epoch: 321 [2560/5999 (43%)]	Loss: 0.045463
Epoch: 321 [5120/5999 (85%)]	Loss: 0.028288
====> Epoch: 321 Average loss: 0.000442 
Epoch: 322 [0/5999 (0%)]	Loss: 0.019173
Epoch: 322 [2560/5999 (43%)]	Loss: 0.052754
Epoch: 322 [5120/5999 (85%)]	Loss: 0.034664
====> Epoch: 322 Average loss: 0.000433 
Epoch: 323 [0/5999 (0%)]	Loss: 0.058486
Epoch: 323 [2560/5999 (43%)]	Loss: 0.049699
Epoch: 323 [5120/5999 (85%)]	Loss: 0.067902
====> Epoch: 323 Average loss: 0.000402 
Epoch: 324 [0/5999 (0%)]	Loss: 0.018957
Epoch: 324 [2560/5999 (43%)]	Loss: 0.074019
Epoch: 324 [5120/5999 (85%)]	Loss: 0.047918
====> Epoch: 324 Average loss: 0.000481 
Epoch: 325 [0/5999 (0%)]	Loss: 0.051538
Epoch: 325 [2560/5999 (43%)]	Loss: 0.024898
Epoch: 325 [5120/5999 (85%)]	Loss: 0.043427
====> Epoch: 325 Average loss: 0.000532 
Epoch: 326 [0/5999 (0%)]	Loss: 0.040277
Epoch: 326 [2560/5999 (43%)]	Loss: 0.128499
Epoch: 326 [5120/5999 (85%)]	Loss: 0.057013
====> Epoch: 326 Average loss: 0.000783 
Epoch: 327 [0/5999 (0%)]	Loss: 0.079860
Epoch: 327 [2560/5999 (43%)]	Loss: 0.053656
Epoch: 327 [5120/5999 (85%)]	Loss: 0.022321
====> Epoch: 327 Average loss: 0.000409 
Epoch: 328 [0/5999 (0%)]	Loss: 0.033197
Epoch: 328 [2560/5999 (43%)]	Loss: 0.023370
saving model at:328,0.00013488759170286358
Epoch: 328 [5120/5999 (85%)]	Loss: 0.042662
====> Epoch: 328 Average loss: 0.000434 
Epoch: 329 [0/5999 (0%)]	Loss: 0.173688
Epoch: 329 [2560/5999 (43%)]	Loss: 0.084695
Epoch: 329 [5120/5999 (85%)]	Loss: 0.058621
====> Epoch: 329 Average loss: 0.000684 
Epoch: 330 [0/5999 (0%)]	Loss: 0.071800
Epoch: 330 [2560/5999 (43%)]	Loss: 0.023006
Epoch: 330 [5120/5999 (85%)]	Loss: 0.141671
====> Epoch: 330 Average loss: 0.000539 
Epoch: 331 [0/5999 (0%)]	Loss: 0.051106
Epoch: 331 [2560/5999 (43%)]	Loss: 0.036729
Epoch: 331 [5120/5999 (85%)]	Loss: 0.104941
====> Epoch: 331 Average loss: 0.000348 
Epoch: 332 [0/5999 (0%)]	Loss: 0.022102
Epoch: 332 [2560/5999 (43%)]	Loss: 0.025920
Epoch: 332 [5120/5999 (85%)]	Loss: 0.125737
====> Epoch: 332 Average loss: 0.000560 
Epoch: 333 [0/5999 (0%)]	Loss: 0.120885
Epoch: 333 [2560/5999 (43%)]	Loss: 0.037000
Epoch: 333 [5120/5999 (85%)]	Loss: 0.071569
====> Epoch: 333 Average loss: 0.000397 
Epoch: 334 [0/5999 (0%)]	Loss: 0.151481
Epoch: 334 [2560/5999 (43%)]	Loss: 0.019791
Epoch: 334 [5120/5999 (85%)]	Loss: 0.118702
====> Epoch: 334 Average loss: 0.000408 
Epoch: 335 [0/5999 (0%)]	Loss: 0.141501
Epoch: 335 [2560/5999 (43%)]	Loss: 0.066514
Epoch: 335 [5120/5999 (85%)]	Loss: 0.044481
====> Epoch: 335 Average loss: 0.000738 
Epoch: 336 [0/5999 (0%)]	Loss: 0.035285
Epoch: 336 [2560/5999 (43%)]	Loss: 0.029713
Epoch: 336 [5120/5999 (85%)]	Loss: 0.028067
====> Epoch: 336 Average loss: 0.000467 
Epoch: 337 [0/5999 (0%)]	Loss: 0.035131
Epoch: 337 [2560/5999 (43%)]	Loss: 0.032020
Epoch: 337 [5120/5999 (85%)]	Loss: 0.053365
====> Epoch: 337 Average loss: 0.000389 
Epoch: 338 [0/5999 (0%)]	Loss: 0.022973
Epoch: 338 [2560/5999 (43%)]	Loss: 0.028835
Epoch: 338 [5120/5999 (85%)]	Loss: 0.025543
====> Epoch: 338 Average loss: 0.000398 
Epoch: 339 [0/5999 (0%)]	Loss: 0.067759
Epoch: 339 [2560/5999 (43%)]	Loss: 0.053488
Epoch: 339 [5120/5999 (85%)]	Loss: 0.085550
====> Epoch: 339 Average loss: 0.000526 
Epoch: 340 [0/5999 (0%)]	Loss: 0.060447
Epoch: 340 [2560/5999 (43%)]	Loss: 0.055451
Epoch: 340 [5120/5999 (85%)]	Loss: 0.053212
====> Epoch: 340 Average loss: 0.000418 
Epoch: 341 [0/5999 (0%)]	Loss: 0.082140
Epoch: 341 [2560/5999 (43%)]	Loss: 0.055868
Epoch: 341 [5120/5999 (85%)]	Loss: 0.032403
====> Epoch: 341 Average loss: 0.000367 
Epoch: 342 [0/5999 (0%)]	Loss: 0.148678
Epoch: 342 [2560/5999 (43%)]	Loss: 0.020421
Epoch: 342 [5120/5999 (85%)]	Loss: 0.020232
====> Epoch: 342 Average loss: 0.000381 
Epoch: 343 [0/5999 (0%)]	Loss: 0.110021
Epoch: 343 [2560/5999 (43%)]	Loss: 0.032820
Epoch: 343 [5120/5999 (85%)]	Loss: 0.054738
====> Epoch: 343 Average loss: 0.000664 
Epoch: 344 [0/5999 (0%)]	Loss: 0.065652
Epoch: 344 [2560/5999 (43%)]	Loss: 0.028654
Epoch: 344 [5120/5999 (85%)]	Loss: 0.029685
====> Epoch: 344 Average loss: 0.000369 
Epoch: 345 [0/5999 (0%)]	Loss: 0.025633
Epoch: 345 [2560/5999 (43%)]	Loss: 0.030773
Epoch: 345 [5120/5999 (85%)]	Loss: 0.097612
====> Epoch: 345 Average loss: 0.000387 
Epoch: 346 [0/5999 (0%)]	Loss: 0.114102
Epoch: 346 [2560/5999 (43%)]	Loss: 0.080304
Epoch: 346 [5120/5999 (85%)]	Loss: 0.069389
====> Epoch: 346 Average loss: 0.000394 
Epoch: 347 [0/5999 (0%)]	Loss: 0.038810
Epoch: 347 [2560/5999 (43%)]	Loss: 0.245991
Epoch: 347 [5120/5999 (85%)]	Loss: 0.066818
====> Epoch: 347 Average loss: 0.000566 
Epoch: 348 [0/5999 (0%)]	Loss: 0.076669
Epoch: 348 [2560/5999 (43%)]	Loss: 0.039731
Epoch: 348 [5120/5999 (85%)]	Loss: 0.046522
====> Epoch: 348 Average loss: 0.000496 
Epoch: 349 [0/5999 (0%)]	Loss: 0.022521
Epoch: 349 [2560/5999 (43%)]	Loss: 0.134955
Epoch: 349 [5120/5999 (85%)]	Loss: 0.066031
====> Epoch: 349 Average loss: 0.000414 
Epoch: 350 [0/5999 (0%)]	Loss: 0.018423
Epoch: 350 [2560/5999 (43%)]	Loss: 0.025346
Epoch: 350 [5120/5999 (85%)]	Loss: 0.036782
====> Epoch: 350 Average loss: 0.000485 
Epoch: 351 [0/5999 (0%)]	Loss: 0.143082
Epoch: 351 [2560/5999 (43%)]	Loss: 0.016623
saving model at:351,0.00012076772144064307
Epoch: 351 [5120/5999 (85%)]	Loss: 0.039028
====> Epoch: 351 Average loss: 0.000343 
Epoch: 352 [0/5999 (0%)]	Loss: 0.033364
Epoch: 352 [2560/5999 (43%)]	Loss: 0.034262
Epoch: 352 [5120/5999 (85%)]	Loss: 0.111309
====> Epoch: 352 Average loss: 0.000403 
Epoch: 353 [0/5999 (0%)]	Loss: 0.046033
Epoch: 353 [2560/5999 (43%)]	Loss: 0.035900
Epoch: 353 [5120/5999 (85%)]	Loss: 0.051637
====> Epoch: 353 Average loss: 0.000365 
Epoch: 354 [0/5999 (0%)]	Loss: 0.052691
Epoch: 354 [2560/5999 (43%)]	Loss: 0.035684
Epoch: 354 [5120/5999 (85%)]	Loss: 0.022307
====> Epoch: 354 Average loss: 0.000344 
Epoch: 355 [0/5999 (0%)]	Loss: 0.109359
Epoch: 355 [2560/5999 (43%)]	Loss: 0.100418
Epoch: 355 [5120/5999 (85%)]	Loss: 0.132383
====> Epoch: 355 Average loss: 0.000428 
Epoch: 356 [0/5999 (0%)]	Loss: 0.050413
Epoch: 356 [2560/5999 (43%)]	Loss: 0.092134
Epoch: 356 [5120/5999 (85%)]	Loss: 0.033373
====> Epoch: 356 Average loss: 0.000446 
Epoch: 357 [0/5999 (0%)]	Loss: 0.027776
Epoch: 357 [2560/5999 (43%)]	Loss: 0.045472
Epoch: 357 [5120/5999 (85%)]	Loss: 0.079554
====> Epoch: 357 Average loss: 0.000468 
Epoch: 358 [0/5999 (0%)]	Loss: 0.033537
Epoch: 358 [2560/5999 (43%)]	Loss: 0.076242
Epoch: 358 [5120/5999 (85%)]	Loss: 0.019227
====> Epoch: 358 Average loss: 0.000336 
Epoch: 359 [0/5999 (0%)]	Loss: 0.063903
Epoch: 359 [2560/5999 (43%)]	Loss: 0.023931
Epoch: 359 [5120/5999 (85%)]	Loss: 0.022046
====> Epoch: 359 Average loss: 0.000404 
Epoch: 360 [0/5999 (0%)]	Loss: 0.097066
Epoch: 360 [2560/5999 (43%)]	Loss: 0.127333
Epoch: 360 [5120/5999 (85%)]	Loss: 0.067585
====> Epoch: 360 Average loss: 0.000488 
Epoch: 361 [0/5999 (0%)]	Loss: 0.036356
Epoch: 361 [2560/5999 (43%)]	Loss: 0.075045
Epoch: 361 [5120/5999 (85%)]	Loss: 0.101930
====> Epoch: 361 Average loss: 0.000376 
Epoch: 362 [0/5999 (0%)]	Loss: 0.132009
Epoch: 362 [2560/5999 (43%)]	Loss: 0.024743
Epoch: 362 [5120/5999 (85%)]	Loss: 0.138151
====> Epoch: 362 Average loss: 0.000514 
Epoch: 363 [0/5999 (0%)]	Loss: 0.031766
Epoch: 363 [2560/5999 (43%)]	Loss: 0.030913
Epoch: 363 [5120/5999 (85%)]	Loss: 0.020901
====> Epoch: 363 Average loss: 0.000446 
Epoch: 364 [0/5999 (0%)]	Loss: 0.032990
Epoch: 364 [2560/5999 (43%)]	Loss: 0.027546
Epoch: 364 [5120/5999 (85%)]	Loss: 0.045880
====> Epoch: 364 Average loss: 0.000392 
Epoch: 365 [0/5999 (0%)]	Loss: 0.097626
Epoch: 365 [2560/5999 (43%)]	Loss: 0.054265
Epoch: 365 [5120/5999 (85%)]	Loss: 0.015952
====> Epoch: 365 Average loss: 0.000421 
Epoch: 366 [0/5999 (0%)]	Loss: 0.032974
Epoch: 366 [2560/5999 (43%)]	Loss: 0.021942
Epoch: 366 [5120/5999 (85%)]	Loss: 0.028293
====> Epoch: 366 Average loss: 0.000361 
Epoch: 367 [0/5999 (0%)]	Loss: 0.061198
Epoch: 367 [2560/5999 (43%)]	Loss: 0.050161
Epoch: 367 [5120/5999 (85%)]	Loss: 0.034674
====> Epoch: 367 Average loss: 0.000478 
Epoch: 368 [0/5999 (0%)]	Loss: 0.240289
Epoch: 368 [2560/5999 (43%)]	Loss: 0.066126
Epoch: 368 [5120/5999 (85%)]	Loss: 0.107678
====> Epoch: 368 Average loss: 0.000608 
Epoch: 369 [0/5999 (0%)]	Loss: 0.041901
Epoch: 369 [2560/5999 (43%)]	Loss: 0.040223
Epoch: 369 [5120/5999 (85%)]	Loss: 0.023676
saving model at:369,0.00011956392414867877
====> Epoch: 369 Average loss: 0.000383 
Epoch: 370 [0/5999 (0%)]	Loss: 0.025527
Epoch: 370 [2560/5999 (43%)]	Loss: 0.096601
Epoch: 370 [5120/5999 (85%)]	Loss: 0.075754
====> Epoch: 370 Average loss: 0.000470 
Epoch: 371 [0/5999 (0%)]	Loss: 0.063359
Epoch: 371 [2560/5999 (43%)]	Loss: 0.120081
Epoch: 371 [5120/5999 (85%)]	Loss: 0.117856
====> Epoch: 371 Average loss: 0.000668 
Epoch: 372 [0/5999 (0%)]	Loss: 0.064493
Epoch: 372 [2560/5999 (43%)]	Loss: 0.032730
Epoch: 372 [5120/5999 (85%)]	Loss: 0.022500
====> Epoch: 372 Average loss: 0.000387 
Epoch: 373 [0/5999 (0%)]	Loss: 0.034088
Epoch: 373 [2560/5999 (43%)]	Loss: 0.029902
Epoch: 373 [5120/5999 (85%)]	Loss: 0.031029
====> Epoch: 373 Average loss: 0.000405 
Epoch: 374 [0/5999 (0%)]	Loss: 0.022828
Epoch: 374 [2560/5999 (43%)]	Loss: 0.071392
Epoch: 374 [5120/5999 (85%)]	Loss: 0.038336
====> Epoch: 374 Average loss: 0.000535 
Epoch: 375 [0/5999 (0%)]	Loss: 0.118397
Epoch: 375 [2560/5999 (43%)]	Loss: 0.033241
Epoch: 375 [5120/5999 (85%)]	Loss: 0.018202
====> Epoch: 375 Average loss: 0.000394 
Epoch: 376 [0/5999 (0%)]	Loss: 0.045037
Epoch: 376 [2560/5999 (43%)]	Loss: 0.038953
Epoch: 376 [5120/5999 (85%)]	Loss: 0.213119
====> Epoch: 376 Average loss: 0.000378 
Epoch: 377 [0/5999 (0%)]	Loss: 0.023102
Epoch: 377 [2560/5999 (43%)]	Loss: 0.023846
Epoch: 377 [5120/5999 (85%)]	Loss: 0.032033
====> Epoch: 377 Average loss: 0.000436 
Epoch: 378 [0/5999 (0%)]	Loss: 0.125682
Epoch: 378 [2560/5999 (43%)]	Loss: 0.027394
Epoch: 378 [5120/5999 (85%)]	Loss: 0.081612
====> Epoch: 378 Average loss: 0.000414 
Epoch: 379 [0/5999 (0%)]	Loss: 0.030263
Epoch: 379 [2560/5999 (43%)]	Loss: 0.030086
Epoch: 379 [5120/5999 (85%)]	Loss: 0.075092
====> Epoch: 379 Average loss: 0.000333 
Epoch: 380 [0/5999 (0%)]	Loss: 0.020824
Epoch: 380 [2560/5999 (43%)]	Loss: 0.017146
saving model at:380,0.00011907346692169085
Epoch: 380 [5120/5999 (85%)]	Loss: 0.054054
====> Epoch: 380 Average loss: 0.000387 
Epoch: 381 [0/5999 (0%)]	Loss: 0.075805
Epoch: 381 [2560/5999 (43%)]	Loss: 0.051957
Epoch: 381 [5120/5999 (85%)]	Loss: 0.028543
====> Epoch: 381 Average loss: 0.000446 
Epoch: 382 [0/5999 (0%)]	Loss: 0.065268
Epoch: 382 [2560/5999 (43%)]	Loss: 0.038437
Epoch: 382 [5120/5999 (85%)]	Loss: 0.039922
====> Epoch: 382 Average loss: 0.000401 
Epoch: 383 [0/5999 (0%)]	Loss: 0.050682
Epoch: 383 [2560/5999 (43%)]	Loss: 0.200351
Epoch: 383 [5120/5999 (85%)]	Loss: 0.047341
====> Epoch: 383 Average loss: 0.000388 
Epoch: 384 [0/5999 (0%)]	Loss: 0.070345
Epoch: 384 [2560/5999 (43%)]	Loss: 0.051576
Epoch: 384 [5120/5999 (85%)]	Loss: 0.036857
====> Epoch: 384 Average loss: 0.000482 
Epoch: 385 [0/5999 (0%)]	Loss: 0.019470
Epoch: 385 [2560/5999 (43%)]	Loss: 0.035850
Epoch: 385 [5120/5999 (85%)]	Loss: 0.064424
====> Epoch: 385 Average loss: 0.000391 
Epoch: 386 [0/5999 (0%)]	Loss: 0.029451
Epoch: 386 [2560/5999 (43%)]	Loss: 0.041250
Epoch: 386 [5120/5999 (85%)]	Loss: 0.026191
saving model at:386,0.00011647712404374033
====> Epoch: 386 Average loss: 0.000383 
Epoch: 387 [0/5999 (0%)]	Loss: 0.028476
Epoch: 387 [2560/5999 (43%)]	Loss: 0.056146
Epoch: 387 [5120/5999 (85%)]	Loss: 0.041660
====> Epoch: 387 Average loss: 0.000603 
Epoch: 388 [0/5999 (0%)]	Loss: 0.056275
Epoch: 388 [2560/5999 (43%)]	Loss: 0.040260
Epoch: 388 [5120/5999 (85%)]	Loss: 0.062545
====> Epoch: 388 Average loss: 0.000475 
Epoch: 389 [0/5999 (0%)]	Loss: 0.084687
Epoch: 389 [2560/5999 (43%)]	Loss: 0.034218
Epoch: 389 [5120/5999 (85%)]	Loss: 0.026203
====> Epoch: 389 Average loss: 0.000421 
Epoch: 390 [0/5999 (0%)]	Loss: 0.022655
Epoch: 390 [2560/5999 (43%)]	Loss: 0.128396
Epoch: 390 [5120/5999 (85%)]	Loss: 0.040518
====> Epoch: 390 Average loss: 0.000418 
Epoch: 391 [0/5999 (0%)]	Loss: 0.023707
Epoch: 391 [2560/5999 (43%)]	Loss: 0.060434
Epoch: 391 [5120/5999 (85%)]	Loss: 0.062175
====> Epoch: 391 Average loss: 0.000466 
Epoch: 392 [0/5999 (0%)]	Loss: 0.051175
Epoch: 392 [2560/5999 (43%)]	Loss: 0.036077
Epoch: 392 [5120/5999 (85%)]	Loss: 0.039810
====> Epoch: 392 Average loss: 0.000360 
Epoch: 393 [0/5999 (0%)]	Loss: 0.037163
Epoch: 393 [2560/5999 (43%)]	Loss: 0.020244
Epoch: 393 [5120/5999 (85%)]	Loss: 0.087926
====> Epoch: 393 Average loss: 0.000345 
Epoch: 394 [0/5999 (0%)]	Loss: 0.026317
Epoch: 394 [2560/5999 (43%)]	Loss: 0.040888
Epoch: 394 [5120/5999 (85%)]	Loss: 0.051424
====> Epoch: 394 Average loss: 0.000472 
Epoch: 395 [0/5999 (0%)]	Loss: 0.021865
Epoch: 395 [2560/5999 (43%)]	Loss: 0.051418
Epoch: 395 [5120/5999 (85%)]	Loss: 0.056711
====> Epoch: 395 Average loss: 0.000345 
Epoch: 396 [0/5999 (0%)]	Loss: 0.046995
Epoch: 396 [2560/5999 (43%)]	Loss: 0.022559
Epoch: 396 [5120/5999 (85%)]	Loss: 0.021003
====> Epoch: 396 Average loss: 0.000373 
Epoch: 397 [0/5999 (0%)]	Loss: 0.030260
Epoch: 397 [2560/5999 (43%)]	Loss: 0.052122
Epoch: 397 [5120/5999 (85%)]	Loss: 0.029253
====> Epoch: 397 Average loss: 0.000413 
Epoch: 398 [0/5999 (0%)]	Loss: 0.095271
Epoch: 398 [2560/5999 (43%)]	Loss: 0.033196
Epoch: 398 [5120/5999 (85%)]	Loss: 0.025213
====> Epoch: 398 Average loss: 0.000424 
Epoch: 399 [0/5999 (0%)]	Loss: 0.029649
Epoch: 399 [2560/5999 (43%)]	Loss: 0.030386
Epoch: 399 [5120/5999 (85%)]	Loss: 0.039590
====> Epoch: 399 Average loss: 0.000444 
Epoch: 400 [0/5999 (0%)]	Loss: 0.082524
Epoch: 400 [2560/5999 (43%)]	Loss: 0.024638
Epoch: 400 [5120/5999 (85%)]	Loss: 0.039303
====> Epoch: 400 Average loss: 0.000340 
Reconstruction Loss 0.00011647712934063747
per_obj_mse: ['0.00014050451864022762', '9.244973625754938e-05']
Reconstruction Loss 0.00011425162825697084
per_obj_mse: ['0.00013794211554341018', '9.056114504346624e-05']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_4_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_4_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_4_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_4_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0016322460137307643
per_obj_mse: ['0.0016878345049917698', '0.0015766576398164034']
Reconstruction Loss 0.0015820923501629384
per_obj_mse: ['0.0016281632706522942', '0.001536021358333528']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_4_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_4_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0016322460137307643
per_obj_mse: ['0.0016878345049917698', '0.0015766576398164034']
Reconstruction Loss 0.0015820923501629384
per_obj_mse: ['0.0016281632706522942', '0.001536021358333528']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_4_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=20, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_4_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7997, 2, 11)
(1997, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.007129995614290238
per_obj_mse: ['0.007854252122342587', '0.0064057400450110435']
Reconstruction Loss 0.0072808301388288385
per_obj_mse: ['0.007973790168762207', '0.006587871350347996']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_4_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=60, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_4_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7993, 2, 11)
(1993, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.02170852297500499
per_obj_mse: ['0.02404019795358181', '0.019376851618289948']
Reconstruction Loss 0.022265566524157437
per_obj_mse: ['0.02459561452269554', '0.019935516640543938']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_4_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=100, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_4_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7989, 2, 11)
(1989, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.03626871488995857
per_obj_mse: ['0.040189046412706375', '0.032348379492759705']
Reconstruction Loss 0.0372271781675836
per_obj_mse: ['0.041192520409822464', '0.033261846750974655']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_4_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=140, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_4_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7985, 2, 11)
(1985, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.050712189066765244
per_obj_mse: ['0.056230638176202774', '0.04519375041127205']
Reconstruction Loss 0.05220842490756242
per_obj_mse: ['0.05780710652470589', '0.046609751880168915']
