cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_3_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=400, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_3_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Epoch: 1 [0/5999 (0%)]	Loss: 111.150909
Epoch: 1 [2560/5999 (43%)]	Loss: 15.304096
saving model at:1,0.11121567595005036
Epoch: 1 [5120/5999 (85%)]	Loss: 10.046144
saving model at:1,0.07526811039447784
====> Epoch: 1 Average loss: 0.154765 
Epoch: 2 [0/5999 (0%)]	Loss: 9.753030
Epoch: 2 [2560/5999 (43%)]	Loss: 8.628828
saving model at:2,0.06107939481735229
Epoch: 2 [5120/5999 (85%)]	Loss: 7.503222
saving model at:2,0.058798106104135515
====> Epoch: 2 Average loss: 0.063888 
Epoch: 3 [0/5999 (0%)]	Loss: 7.550370
Epoch: 3 [2560/5999 (43%)]	Loss: 5.850715
saving model at:3,0.0437056559920311
Epoch: 3 [5120/5999 (85%)]	Loss: 3.476055
saving model at:3,0.02417216704785824
====> Epoch: 3 Average loss: 0.041906 
Epoch: 4 [0/5999 (0%)]	Loss: 3.167887
Epoch: 4 [2560/5999 (43%)]	Loss: 2.196171
saving model at:4,0.017786650478839875
Epoch: 4 [5120/5999 (85%)]	Loss: 2.098046
saving model at:4,0.015384566254913808
====> Epoch: 4 Average loss: 0.018490 
Epoch: 5 [0/5999 (0%)]	Loss: 2.389481
Epoch: 5 [2560/5999 (43%)]	Loss: 1.677078
saving model at:5,0.012918926529586315
Epoch: 5 [5120/5999 (85%)]	Loss: 1.691022
saving model at:5,0.011547216966748238
====> Epoch: 5 Average loss: 0.013521 
Epoch: 6 [0/5999 (0%)]	Loss: 1.312561
Epoch: 6 [2560/5999 (43%)]	Loss: 1.362174
saving model at:6,0.009514920227229596
Epoch: 6 [5120/5999 (85%)]	Loss: 1.105639
saving model at:6,0.008425955146551133
====> Epoch: 6 Average loss: 0.010030 
Epoch: 7 [0/5999 (0%)]	Loss: 1.105583
Epoch: 7 [2560/5999 (43%)]	Loss: 1.064091
saving model at:7,0.007336360715329647
Epoch: 7 [5120/5999 (85%)]	Loss: 0.917387
saving model at:7,0.006993361711502075
====> Epoch: 7 Average loss: 0.008039 
Epoch: 8 [0/5999 (0%)]	Loss: 0.856295
Epoch: 8 [2560/5999 (43%)]	Loss: 0.874708
saving model at:8,0.006391905814409256
Epoch: 8 [5120/5999 (85%)]	Loss: 1.102618
====> Epoch: 8 Average loss: 0.006816 
Epoch: 9 [0/5999 (0%)]	Loss: 1.049363
Epoch: 9 [2560/5999 (43%)]	Loss: 0.779621
saving model at:9,0.005622271098196506
Epoch: 9 [5120/5999 (85%)]	Loss: 0.874701
====> Epoch: 9 Average loss: 0.006110 
Epoch: 10 [0/5999 (0%)]	Loss: 0.781142
Epoch: 10 [2560/5999 (43%)]	Loss: 0.742432
saving model at:10,0.005177353173494339
Epoch: 10 [5120/5999 (85%)]	Loss: 0.684050
saving model at:10,0.0050304972268641
====> Epoch: 10 Average loss: 0.005517 
Epoch: 11 [0/5999 (0%)]	Loss: 0.714379
Epoch: 11 [2560/5999 (43%)]	Loss: 0.773447
saving model at:11,0.0048887253478169445
Epoch: 11 [5120/5999 (85%)]	Loss: 0.770384
saving model at:11,0.004835568934679031
====> Epoch: 11 Average loss: 0.005296 
Epoch: 12 [0/5999 (0%)]	Loss: 0.671106
Epoch: 12 [2560/5999 (43%)]	Loss: 0.648070
saving model at:12,0.004596632573753596
Epoch: 12 [5120/5999 (85%)]	Loss: 0.741684
====> Epoch: 12 Average loss: 0.004993 
Epoch: 13 [0/5999 (0%)]	Loss: 0.837729
Epoch: 13 [2560/5999 (43%)]	Loss: 0.596664
saving model at:13,0.004439575783908367
Epoch: 13 [5120/5999 (85%)]	Loss: 0.586957
====> Epoch: 13 Average loss: 0.005100 
Epoch: 14 [0/5999 (0%)]	Loss: 0.596727
Epoch: 14 [2560/5999 (43%)]	Loss: 0.612010
saving model at:14,0.004370647478848696
Epoch: 14 [5120/5999 (85%)]	Loss: 0.609394
saving model at:14,0.004211835823953152
====> Epoch: 14 Average loss: 0.004588 
Epoch: 15 [0/5999 (0%)]	Loss: 0.561374
Epoch: 15 [2560/5999 (43%)]	Loss: 0.746458
Epoch: 15 [5120/5999 (85%)]	Loss: 0.515416
====> Epoch: 15 Average loss: 0.005178 
Epoch: 16 [0/5999 (0%)]	Loss: 0.598320
Epoch: 16 [2560/5999 (43%)]	Loss: 0.600578
Epoch: 16 [5120/5999 (85%)]	Loss: 0.578971
====> Epoch: 16 Average loss: 0.005082 
Epoch: 17 [0/5999 (0%)]	Loss: 0.539445
Epoch: 17 [2560/5999 (43%)]	Loss: 0.529097
saving model at:17,0.004043567219749093
Epoch: 17 [5120/5999 (85%)]	Loss: 0.539802
saving model at:17,0.0038899315297603607
====> Epoch: 17 Average loss: 0.004361 
Epoch: 18 [0/5999 (0%)]	Loss: 0.647325
Epoch: 18 [2560/5999 (43%)]	Loss: 0.544118
Epoch: 18 [5120/5999 (85%)]	Loss: 0.520739
====> Epoch: 18 Average loss: 0.004386 
Epoch: 19 [0/5999 (0%)]	Loss: 0.673063
Epoch: 19 [2560/5999 (43%)]	Loss: 0.486611
Epoch: 19 [5120/5999 (85%)]	Loss: 0.490653
saving model at:19,0.0038074457608163357
====> Epoch: 19 Average loss: 0.004144 
Epoch: 20 [0/5999 (0%)]	Loss: 0.514191
Epoch: 20 [2560/5999 (43%)]	Loss: 0.488615
Epoch: 20 [5120/5999 (85%)]	Loss: 0.527863
====> Epoch: 20 Average loss: 0.004217 
Epoch: 21 [0/5999 (0%)]	Loss: 0.493343
Epoch: 21 [2560/5999 (43%)]	Loss: 0.595123
saving model at:21,0.003665105681866407
Epoch: 21 [5120/5999 (85%)]	Loss: 0.483852
====> Epoch: 21 Average loss: 0.004087 
Epoch: 22 [0/5999 (0%)]	Loss: 0.476103
Epoch: 22 [2560/5999 (43%)]	Loss: 0.564346
saving model at:22,0.0036371363960206507
Epoch: 22 [5120/5999 (85%)]	Loss: 0.455965
saving model at:22,0.0036157979108393194
====> Epoch: 22 Average loss: 0.004059 
Epoch: 23 [0/5999 (0%)]	Loss: 0.509543
Epoch: 23 [2560/5999 (43%)]	Loss: 0.505048
saving model at:23,0.0035144042894244194
Epoch: 23 [5120/5999 (85%)]	Loss: 0.426662
saving model at:23,0.0034982954561710356
====> Epoch: 23 Average loss: 0.003752 
Epoch: 24 [0/5999 (0%)]	Loss: 0.455514
Epoch: 24 [2560/5999 (43%)]	Loss: 0.515468
Epoch: 24 [5120/5999 (85%)]	Loss: 0.466089
saving model at:24,0.003400489654392004
====> Epoch: 24 Average loss: 0.003665 
Epoch: 25 [0/5999 (0%)]	Loss: 0.476579
Epoch: 25 [2560/5999 (43%)]	Loss: 0.390110
saving model at:25,0.003205218382179737
Epoch: 25 [5120/5999 (85%)]	Loss: 0.482367
====> Epoch: 25 Average loss: 0.003793 
Epoch: 26 [0/5999 (0%)]	Loss: 0.561394
Epoch: 26 [2560/5999 (43%)]	Loss: 0.589016
Epoch: 26 [5120/5999 (85%)]	Loss: 0.457215
saving model at:26,0.0031198486592620613
====> Epoch: 26 Average loss: 0.003894 
Epoch: 27 [0/5999 (0%)]	Loss: 0.403119
Epoch: 27 [2560/5999 (43%)]	Loss: 0.409754
Epoch: 27 [5120/5999 (85%)]	Loss: 0.414248
saving model at:27,0.0030302156433463096
====> Epoch: 27 Average loss: 0.003688 
Epoch: 28 [0/5999 (0%)]	Loss: 0.413310
Epoch: 28 [2560/5999 (43%)]	Loss: 0.397964
saving model at:28,0.002954328067600727
Epoch: 28 [5120/5999 (85%)]	Loss: 0.431569
====> Epoch: 28 Average loss: 0.003281 
Epoch: 29 [0/5999 (0%)]	Loss: 0.375959
Epoch: 29 [2560/5999 (43%)]	Loss: 0.373071
Epoch: 29 [5120/5999 (85%)]	Loss: 0.397241
saving model at:29,0.002824566265568137
====> Epoch: 29 Average loss: 0.003217 
Epoch: 30 [0/5999 (0%)]	Loss: 0.365135
Epoch: 30 [2560/5999 (43%)]	Loss: 0.499773
Epoch: 30 [5120/5999 (85%)]	Loss: 0.384467
saving model at:30,0.002766290104016662
====> Epoch: 30 Average loss: 0.003240 
Epoch: 31 [0/5999 (0%)]	Loss: 0.376986
Epoch: 31 [2560/5999 (43%)]	Loss: 0.356545
saving model at:31,0.002735394436866045
Epoch: 31 [5120/5999 (85%)]	Loss: 0.496056
====> Epoch: 31 Average loss: 0.003066 
Epoch: 32 [0/5999 (0%)]	Loss: 0.335921
Epoch: 32 [2560/5999 (43%)]	Loss: 0.413165
saving model at:32,0.0026110222935676573
Epoch: 32 [5120/5999 (85%)]	Loss: 0.387729
====> Epoch: 32 Average loss: 0.002808 
Epoch: 33 [0/5999 (0%)]	Loss: 0.344932
Epoch: 33 [2560/5999 (43%)]	Loss: 0.435711
Epoch: 33 [5120/5999 (85%)]	Loss: 0.413116
====> Epoch: 33 Average loss: 0.002944 
Epoch: 34 [0/5999 (0%)]	Loss: 0.423703
Epoch: 34 [2560/5999 (43%)]	Loss: 0.359383
Epoch: 34 [5120/5999 (85%)]	Loss: 0.400100
====> Epoch: 34 Average loss: 0.002886 
Epoch: 35 [0/5999 (0%)]	Loss: 0.371270
Epoch: 35 [2560/5999 (43%)]	Loss: 0.351874
Epoch: 35 [5120/5999 (85%)]	Loss: 0.406552
====> Epoch: 35 Average loss: 0.002877 
Epoch: 36 [0/5999 (0%)]	Loss: 0.333050
Epoch: 36 [2560/5999 (43%)]	Loss: 0.360785
saving model at:36,0.00255727631598711
Epoch: 36 [5120/5999 (85%)]	Loss: 0.318278
====> Epoch: 36 Average loss: 0.002890 
Epoch: 37 [0/5999 (0%)]	Loss: 0.313122
Epoch: 37 [2560/5999 (43%)]	Loss: 0.346037
saving model at:37,0.0023583890572190284
Epoch: 37 [5120/5999 (85%)]	Loss: 0.384976
====> Epoch: 37 Average loss: 0.002622 
Epoch: 38 [0/5999 (0%)]	Loss: 0.373098
Epoch: 38 [2560/5999 (43%)]	Loss: 0.337319
Epoch: 38 [5120/5999 (85%)]	Loss: 0.315369
saving model at:38,0.002339560218155384
====> Epoch: 38 Average loss: 0.002617 
Epoch: 39 [0/5999 (0%)]	Loss: 0.411125
Epoch: 39 [2560/5999 (43%)]	Loss: 0.333056
Epoch: 39 [5120/5999 (85%)]	Loss: 0.395675
====> Epoch: 39 Average loss: 0.002693 
Epoch: 40 [0/5999 (0%)]	Loss: 0.328069
Epoch: 40 [2560/5999 (43%)]	Loss: 0.362895
saving model at:40,0.002286680690944195
Epoch: 40 [5120/5999 (85%)]	Loss: 0.352223
saving model at:40,0.002281938187777996
====> Epoch: 40 Average loss: 0.002716 
Epoch: 41 [0/5999 (0%)]	Loss: 0.329865
Epoch: 41 [2560/5999 (43%)]	Loss: 0.287767
saving model at:41,0.002202728819102049
Epoch: 41 [5120/5999 (85%)]	Loss: 0.361052
====> Epoch: 41 Average loss: 0.002675 
Epoch: 42 [0/5999 (0%)]	Loss: 0.334174
Epoch: 42 [2560/5999 (43%)]	Loss: 0.321306
Epoch: 42 [5120/5999 (85%)]	Loss: 0.430796
====> Epoch: 42 Average loss: 0.003057 
Epoch: 43 [0/5999 (0%)]	Loss: 0.357147
Epoch: 43 [2560/5999 (43%)]	Loss: 0.353175
Epoch: 43 [5120/5999 (85%)]	Loss: 0.331947
====> Epoch: 43 Average loss: 0.002883 
Epoch: 44 [0/5999 (0%)]	Loss: 0.339910
Epoch: 44 [2560/5999 (43%)]	Loss: 0.317035
Epoch: 44 [5120/5999 (85%)]	Loss: 0.301816
====> Epoch: 44 Average loss: 0.002530 
Epoch: 45 [0/5999 (0%)]	Loss: 0.342997
Epoch: 45 [2560/5999 (43%)]	Loss: 0.311748
Epoch: 45 [5120/5999 (85%)]	Loss: 0.312632
saving model at:45,0.0021506110224872826
====> Epoch: 45 Average loss: 0.002629 
Epoch: 46 [0/5999 (0%)]	Loss: 0.270209
Epoch: 46 [2560/5999 (43%)]	Loss: 0.433191
Epoch: 46 [5120/5999 (85%)]	Loss: 0.300572
====> Epoch: 46 Average loss: 0.002543 
Epoch: 47 [0/5999 (0%)]	Loss: 0.331651
Epoch: 47 [2560/5999 (43%)]	Loss: 0.419554
Epoch: 47 [5120/5999 (85%)]	Loss: 0.260975
====> Epoch: 47 Average loss: 0.002539 
Epoch: 48 [0/5999 (0%)]	Loss: 0.267521
Epoch: 48 [2560/5999 (43%)]	Loss: 0.325106
Epoch: 48 [5120/5999 (85%)]	Loss: 0.307702
====> Epoch: 48 Average loss: 0.002536 
Epoch: 49 [0/5999 (0%)]	Loss: 0.313257
Epoch: 49 [2560/5999 (43%)]	Loss: 0.288269
saving model at:49,0.0021337834000587463
Epoch: 49 [5120/5999 (85%)]	Loss: 0.389141
====> Epoch: 49 Average loss: 0.002645 
Epoch: 50 [0/5999 (0%)]	Loss: 0.369775
Epoch: 50 [2560/5999 (43%)]	Loss: 0.275324
saving model at:50,0.00208552735671401
Epoch: 50 [5120/5999 (85%)]	Loss: 0.278571
saving model at:50,0.00205664418451488
====> Epoch: 50 Average loss: 0.002504 
Epoch: 51 [0/5999 (0%)]	Loss: 0.406186
Epoch: 51 [2560/5999 (43%)]	Loss: 0.273107
Epoch: 51 [5120/5999 (85%)]	Loss: 0.377630
====> Epoch: 51 Average loss: 0.002519 
Epoch: 52 [0/5999 (0%)]	Loss: 0.336869
Epoch: 52 [2560/5999 (43%)]	Loss: 0.308723
Epoch: 52 [5120/5999 (85%)]	Loss: 0.383751
====> Epoch: 52 Average loss: 0.002665 
Epoch: 53 [0/5999 (0%)]	Loss: 0.346972
Epoch: 53 [2560/5999 (43%)]	Loss: 0.259122
Epoch: 53 [5120/5999 (85%)]	Loss: 0.261277
saving model at:53,0.001991983186453581
====> Epoch: 53 Average loss: 0.002410 
Epoch: 54 [0/5999 (0%)]	Loss: 0.275236
Epoch: 54 [2560/5999 (43%)]	Loss: 0.269315
Epoch: 54 [5120/5999 (85%)]	Loss: 0.314891
====> Epoch: 54 Average loss: 0.002355 
Epoch: 55 [0/5999 (0%)]	Loss: 0.266620
Epoch: 55 [2560/5999 (43%)]	Loss: 0.442245
Epoch: 55 [5120/5999 (85%)]	Loss: 0.346589
====> Epoch: 55 Average loss: 0.002485 
Epoch: 56 [0/5999 (0%)]	Loss: 0.305594
Epoch: 56 [2560/5999 (43%)]	Loss: 0.290149
saving model at:56,0.0019721180945634843
Epoch: 56 [5120/5999 (85%)]	Loss: 0.265749
====> Epoch: 56 Average loss: 0.002260 
Epoch: 57 [0/5999 (0%)]	Loss: 0.268017
Epoch: 57 [2560/5999 (43%)]	Loss: 0.269742
Epoch: 57 [5120/5999 (85%)]	Loss: 0.401325
====> Epoch: 57 Average loss: 0.002424 
Epoch: 58 [0/5999 (0%)]	Loss: 0.376052
Epoch: 58 [2560/5999 (43%)]	Loss: 0.314702
Epoch: 58 [5120/5999 (85%)]	Loss: 0.248750
saving model at:58,0.0019377858564257622
====> Epoch: 58 Average loss: 0.002327 
Epoch: 59 [0/5999 (0%)]	Loss: 0.280599
Epoch: 59 [2560/5999 (43%)]	Loss: 0.250732
Epoch: 59 [5120/5999 (85%)]	Loss: 0.733189
====> Epoch: 59 Average loss: 0.002842 
Epoch: 60 [0/5999 (0%)]	Loss: 0.572870
Epoch: 60 [2560/5999 (43%)]	Loss: 0.334906
Epoch: 60 [5120/5999 (85%)]	Loss: 0.265571
====> Epoch: 60 Average loss: 0.002562 
Epoch: 61 [0/5999 (0%)]	Loss: 0.296286
Epoch: 61 [2560/5999 (43%)]	Loss: 0.473119
Epoch: 61 [5120/5999 (85%)]	Loss: 0.374585
====> Epoch: 61 Average loss: 0.002433 
Epoch: 62 [0/5999 (0%)]	Loss: 0.382351
Epoch: 62 [2560/5999 (43%)]	Loss: 0.308898
Epoch: 62 [5120/5999 (85%)]	Loss: 0.271202
====> Epoch: 62 Average loss: 0.002551 
Epoch: 63 [0/5999 (0%)]	Loss: 0.277318
Epoch: 63 [2560/5999 (43%)]	Loss: 0.266191
Epoch: 63 [5120/5999 (85%)]	Loss: 0.275174
saving model at:63,0.0018727256618440151
====> Epoch: 63 Average loss: 0.002257 
Epoch: 64 [0/5999 (0%)]	Loss: 0.277879
Epoch: 64 [2560/5999 (43%)]	Loss: 0.232848
Epoch: 64 [5120/5999 (85%)]	Loss: 0.290003
====> Epoch: 64 Average loss: 0.002162 
Epoch: 65 [0/5999 (0%)]	Loss: 0.270143
Epoch: 65 [2560/5999 (43%)]	Loss: 0.463162
Epoch: 65 [5120/5999 (85%)]	Loss: 0.291808
====> Epoch: 65 Average loss: 0.002445 
Epoch: 66 [0/5999 (0%)]	Loss: 0.248777
Epoch: 66 [2560/5999 (43%)]	Loss: 0.250657
Epoch: 66 [5120/5999 (85%)]	Loss: 0.226235
saving model at:66,0.0018206980358809232
====> Epoch: 66 Average loss: 0.002264 
Epoch: 67 [0/5999 (0%)]	Loss: 0.243137
Epoch: 67 [2560/5999 (43%)]	Loss: 0.290214
Epoch: 67 [5120/5999 (85%)]	Loss: 0.224966
====> Epoch: 67 Average loss: 0.002218 
Epoch: 68 [0/5999 (0%)]	Loss: 0.248839
Epoch: 68 [2560/5999 (43%)]	Loss: 0.430091
Epoch: 68 [5120/5999 (85%)]	Loss: 0.321979
====> Epoch: 68 Average loss: 0.002558 
Epoch: 69 [0/5999 (0%)]	Loss: 0.275422
Epoch: 69 [2560/5999 (43%)]	Loss: 0.234708
Epoch: 69 [5120/5999 (85%)]	Loss: 0.268903
====> Epoch: 69 Average loss: 0.002333 
Epoch: 70 [0/5999 (0%)]	Loss: 0.249900
Epoch: 70 [2560/5999 (43%)]	Loss: 0.273935
saving model at:70,0.0018127087261527778
Epoch: 70 [5120/5999 (85%)]	Loss: 0.290592
====> Epoch: 70 Average loss: 0.002591 
Epoch: 71 [0/5999 (0%)]	Loss: 0.275696
Epoch: 71 [2560/5999 (43%)]	Loss: 0.245665
Epoch: 71 [5120/5999 (85%)]	Loss: 0.252973
====> Epoch: 71 Average loss: 0.002139 
Epoch: 72 [0/5999 (0%)]	Loss: 0.228350
Epoch: 72 [2560/5999 (43%)]	Loss: 0.259442
Epoch: 72 [5120/5999 (85%)]	Loss: 0.247749
====> Epoch: 72 Average loss: 0.002128 
Epoch: 73 [0/5999 (0%)]	Loss: 0.265237
Epoch: 73 [2560/5999 (43%)]	Loss: 0.266193
Epoch: 73 [5120/5999 (85%)]	Loss: 0.355511
====> Epoch: 73 Average loss: 0.002537 
Epoch: 74 [0/5999 (0%)]	Loss: 0.269627
Epoch: 74 [2560/5999 (43%)]	Loss: 0.262491
saving model at:74,0.0018017980493605138
Epoch: 74 [5120/5999 (85%)]	Loss: 0.277341
====> Epoch: 74 Average loss: 0.002024 
Epoch: 75 [0/5999 (0%)]	Loss: 0.276820
Epoch: 75 [2560/5999 (43%)]	Loss: 0.256700
Epoch: 75 [5120/5999 (85%)]	Loss: 0.288009
====> Epoch: 75 Average loss: 0.002220 
Epoch: 76 [0/5999 (0%)]	Loss: 0.474567
Epoch: 76 [2560/5999 (43%)]	Loss: 0.260782
Epoch: 76 [5120/5999 (85%)]	Loss: 0.294464
====> Epoch: 76 Average loss: 0.002222 
Epoch: 77 [0/5999 (0%)]	Loss: 0.243258
Epoch: 77 [2560/5999 (43%)]	Loss: 0.264877
Epoch: 77 [5120/5999 (85%)]	Loss: 0.296368
saving model at:77,0.0017616873160004615
====> Epoch: 77 Average loss: 0.002135 
Epoch: 78 [0/5999 (0%)]	Loss: 0.239669
Epoch: 78 [2560/5999 (43%)]	Loss: 0.271021
Epoch: 78 [5120/5999 (85%)]	Loss: 0.337611
====> Epoch: 78 Average loss: 0.002293 
Epoch: 79 [0/5999 (0%)]	Loss: 0.401704
Epoch: 79 [2560/5999 (43%)]	Loss: 0.358426
Epoch: 79 [5120/5999 (85%)]	Loss: 0.256257
====> Epoch: 79 Average loss: 0.002137 
Epoch: 80 [0/5999 (0%)]	Loss: 0.309415
Epoch: 80 [2560/5999 (43%)]	Loss: 0.251120
saving model at:80,0.0017580721620470286
Epoch: 80 [5120/5999 (85%)]	Loss: 0.290908
====> Epoch: 80 Average loss: 0.002101 
Epoch: 81 [0/5999 (0%)]	Loss: 0.373636
Epoch: 81 [2560/5999 (43%)]	Loss: 0.257742
Epoch: 81 [5120/5999 (85%)]	Loss: 0.328146
====> Epoch: 81 Average loss: 0.002154 
Epoch: 82 [0/5999 (0%)]	Loss: 0.248597
Epoch: 82 [2560/5999 (43%)]	Loss: 0.290670
Epoch: 82 [5120/5999 (85%)]	Loss: 0.255018
====> Epoch: 82 Average loss: 0.002071 
Epoch: 83 [0/5999 (0%)]	Loss: 0.251427
Epoch: 83 [2560/5999 (43%)]	Loss: 0.276811
Epoch: 83 [5120/5999 (85%)]	Loss: 0.254991
saving model at:83,0.0017355439029633998
====> Epoch: 83 Average loss: 0.002071 
Epoch: 84 [0/5999 (0%)]	Loss: 0.289499
Epoch: 84 [2560/5999 (43%)]	Loss: 0.235491
Epoch: 84 [5120/5999 (85%)]	Loss: 0.339592
====> Epoch: 84 Average loss: 0.002299 
Epoch: 85 [0/5999 (0%)]	Loss: 0.404517
Epoch: 85 [2560/5999 (43%)]	Loss: 0.274844
Epoch: 85 [5120/5999 (85%)]	Loss: 0.224910
====> Epoch: 85 Average loss: 0.002207 
Epoch: 86 [0/5999 (0%)]	Loss: 0.274098
Epoch: 86 [2560/5999 (43%)]	Loss: 0.293725
Epoch: 86 [5120/5999 (85%)]	Loss: 0.357466
====> Epoch: 86 Average loss: 0.002370 
Epoch: 87 [0/5999 (0%)]	Loss: 0.247397
Epoch: 87 [2560/5999 (43%)]	Loss: 0.253555
Epoch: 87 [5120/5999 (85%)]	Loss: 0.245727
====> Epoch: 87 Average loss: 0.002038 
Epoch: 88 [0/5999 (0%)]	Loss: 0.233626
Epoch: 88 [2560/5999 (43%)]	Loss: 0.286578
Epoch: 88 [5120/5999 (85%)]	Loss: 0.283027
====> Epoch: 88 Average loss: 0.002139 
Epoch: 89 [0/5999 (0%)]	Loss: 0.261660
Epoch: 89 [2560/5999 (43%)]	Loss: 0.233535
Epoch: 89 [5120/5999 (85%)]	Loss: 0.403733
====> Epoch: 89 Average loss: 0.002333 
Epoch: 90 [0/5999 (0%)]	Loss: 0.340693
Epoch: 90 [2560/5999 (43%)]	Loss: 0.253597
Epoch: 90 [5120/5999 (85%)]	Loss: 0.294921
====> Epoch: 90 Average loss: 0.002161 
Epoch: 91 [0/5999 (0%)]	Loss: 0.295296
Epoch: 91 [2560/5999 (43%)]	Loss: 0.259861
Epoch: 91 [5120/5999 (85%)]	Loss: 0.265593
====> Epoch: 91 Average loss: 0.002161 
Epoch: 92 [0/5999 (0%)]	Loss: 0.334728
Epoch: 92 [2560/5999 (43%)]	Loss: 0.247370
Epoch: 92 [5120/5999 (85%)]	Loss: 0.227342
====> Epoch: 92 Average loss: 0.002190 
Epoch: 93 [0/5999 (0%)]	Loss: 0.261001
Epoch: 93 [2560/5999 (43%)]	Loss: 0.272264
Epoch: 93 [5120/5999 (85%)]	Loss: 0.358469
====> Epoch: 93 Average loss: 0.002206 
Epoch: 94 [0/5999 (0%)]	Loss: 0.377170
Epoch: 94 [2560/5999 (43%)]	Loss: 0.263131
Epoch: 94 [5120/5999 (85%)]	Loss: 0.256184
====> Epoch: 94 Average loss: 0.002201 
Epoch: 95 [0/5999 (0%)]	Loss: 0.264207
Epoch: 95 [2560/5999 (43%)]	Loss: 0.267894
Epoch: 95 [5120/5999 (85%)]	Loss: 0.229820
====> Epoch: 95 Average loss: 0.002130 
Epoch: 96 [0/5999 (0%)]	Loss: 0.406982
Epoch: 96 [2560/5999 (43%)]	Loss: 0.217024
Epoch: 96 [5120/5999 (85%)]	Loss: 0.245603
====> Epoch: 96 Average loss: 0.002002 
Epoch: 97 [0/5999 (0%)]	Loss: 0.218677
Epoch: 97 [2560/5999 (43%)]	Loss: 0.256326
Epoch: 97 [5120/5999 (85%)]	Loss: 0.211670
saving model at:97,0.0016873897109180688
====> Epoch: 97 Average loss: 0.001997 
Epoch: 98 [0/5999 (0%)]	Loss: 0.275369
Epoch: 98 [2560/5999 (43%)]	Loss: 0.653693
Epoch: 98 [5120/5999 (85%)]	Loss: 0.418524
====> Epoch: 98 Average loss: 0.002788 
Epoch: 99 [0/5999 (0%)]	Loss: 0.326961
Epoch: 99 [2560/5999 (43%)]	Loss: 0.248250
Epoch: 99 [5120/5999 (85%)]	Loss: 0.431124
====> Epoch: 99 Average loss: 0.002471 
Epoch: 100 [0/5999 (0%)]	Loss: 0.243027
Epoch: 100 [2560/5999 (43%)]	Loss: 0.316488
Epoch: 100 [5120/5999 (85%)]	Loss: 0.241190
====> Epoch: 100 Average loss: 0.002153 
Epoch: 101 [0/5999 (0%)]	Loss: 0.271827
Epoch: 101 [2560/5999 (43%)]	Loss: 0.231612
Epoch: 101 [5120/5999 (85%)]	Loss: 0.289366
====> Epoch: 101 Average loss: 0.002007 
Epoch: 102 [0/5999 (0%)]	Loss: 0.240259
Epoch: 102 [2560/5999 (43%)]	Loss: 0.318054
Epoch: 102 [5120/5999 (85%)]	Loss: 0.217174
====> Epoch: 102 Average loss: 0.001997 
Epoch: 103 [0/5999 (0%)]	Loss: 0.242231
Epoch: 103 [2560/5999 (43%)]	Loss: 0.327189
Epoch: 103 [5120/5999 (85%)]	Loss: 0.273234
====> Epoch: 103 Average loss: 0.001981 
Epoch: 104 [0/5999 (0%)]	Loss: 0.284884
Epoch: 104 [2560/5999 (43%)]	Loss: 0.257827
Epoch: 104 [5120/5999 (85%)]	Loss: 0.417325
====> Epoch: 104 Average loss: 0.002368 
Epoch: 105 [0/5999 (0%)]	Loss: 0.314960
Epoch: 105 [2560/5999 (43%)]	Loss: 0.360776
Epoch: 105 [5120/5999 (85%)]	Loss: 0.247903
====> Epoch: 105 Average loss: 0.002219 
Epoch: 106 [0/5999 (0%)]	Loss: 0.241175
Epoch: 106 [2560/5999 (43%)]	Loss: 0.246809
saving model at:106,0.0016623487388715149
Epoch: 106 [5120/5999 (85%)]	Loss: 0.374521
====> Epoch: 106 Average loss: 0.002101 
Epoch: 107 [0/5999 (0%)]	Loss: 0.337363
Epoch: 107 [2560/5999 (43%)]	Loss: 0.221411
Epoch: 107 [5120/5999 (85%)]	Loss: 0.240811
====> Epoch: 107 Average loss: 0.002270 
Epoch: 108 [0/5999 (0%)]	Loss: 0.295453
Epoch: 108 [2560/5999 (43%)]	Loss: 0.229472
Epoch: 108 [5120/5999 (85%)]	Loss: 0.379673
====> Epoch: 108 Average loss: 0.002118 
Epoch: 109 [0/5999 (0%)]	Loss: 0.298146
Epoch: 109 [2560/5999 (43%)]	Loss: 0.247024
Epoch: 109 [5120/5999 (85%)]	Loss: 0.263078
====> Epoch: 109 Average loss: 0.002017 
Epoch: 110 [0/5999 (0%)]	Loss: 0.251870
Epoch: 110 [2560/5999 (43%)]	Loss: 0.255042
Epoch: 110 [5120/5999 (85%)]	Loss: 0.266677
====> Epoch: 110 Average loss: 0.002095 
Epoch: 111 [0/5999 (0%)]	Loss: 0.282574
Epoch: 111 [2560/5999 (43%)]	Loss: 0.241901
Epoch: 111 [5120/5999 (85%)]	Loss: 0.221558
====> Epoch: 111 Average loss: 0.002271 
Epoch: 112 [0/5999 (0%)]	Loss: 0.274145
Epoch: 112 [2560/5999 (43%)]	Loss: 0.264935
Epoch: 112 [5120/5999 (85%)]	Loss: 0.268982
saving model at:112,0.001650107329711318
====> Epoch: 112 Average loss: 0.001961 
Epoch: 113 [0/5999 (0%)]	Loss: 0.315180
Epoch: 113 [2560/5999 (43%)]	Loss: 0.260250
Epoch: 113 [5120/5999 (85%)]	Loss: 0.224887
====> Epoch: 113 Average loss: 0.002016 
Epoch: 114 [0/5999 (0%)]	Loss: 0.257252
Epoch: 114 [2560/5999 (43%)]	Loss: 0.231754
Epoch: 114 [5120/5999 (85%)]	Loss: 0.252287
====> Epoch: 114 Average loss: 0.002065 
Epoch: 115 [0/5999 (0%)]	Loss: 0.212704
Epoch: 115 [2560/5999 (43%)]	Loss: 0.309166
Epoch: 115 [5120/5999 (85%)]	Loss: 0.258885
====> Epoch: 115 Average loss: 0.002028 
Epoch: 116 [0/5999 (0%)]	Loss: 0.250950
Epoch: 116 [2560/5999 (43%)]	Loss: 0.261693
Epoch: 116 [5120/5999 (85%)]	Loss: 0.233039
====> Epoch: 116 Average loss: 0.001980 
Epoch: 117 [0/5999 (0%)]	Loss: 0.219245
Epoch: 117 [2560/5999 (43%)]	Loss: 0.212006
Epoch: 117 [5120/5999 (85%)]	Loss: 0.237234
====> Epoch: 117 Average loss: 0.002011 
Epoch: 118 [0/5999 (0%)]	Loss: 0.213153
Epoch: 118 [2560/5999 (43%)]	Loss: 0.242829
Epoch: 118 [5120/5999 (85%)]	Loss: 0.206011
====> Epoch: 118 Average loss: 0.002085 
Epoch: 119 [0/5999 (0%)]	Loss: 0.257887
Epoch: 119 [2560/5999 (43%)]	Loss: 0.263306
Epoch: 119 [5120/5999 (85%)]	Loss: 0.240747
====> Epoch: 119 Average loss: 0.001943 
Epoch: 120 [0/5999 (0%)]	Loss: 0.241400
Epoch: 120 [2560/5999 (43%)]	Loss: 0.264184
Epoch: 120 [5120/5999 (85%)]	Loss: 0.394740
====> Epoch: 120 Average loss: 0.002148 
Epoch: 121 [0/5999 (0%)]	Loss: 0.239435
Epoch: 121 [2560/5999 (43%)]	Loss: 0.286456
Epoch: 121 [5120/5999 (85%)]	Loss: 0.239714
saving model at:121,0.0016283199386671185
====> Epoch: 121 Average loss: 0.001947 
Epoch: 122 [0/5999 (0%)]	Loss: 0.354370
Epoch: 122 [2560/5999 (43%)]	Loss: 0.219822
Epoch: 122 [5120/5999 (85%)]	Loss: 0.250556
====> Epoch: 122 Average loss: 0.002010 
Epoch: 123 [0/5999 (0%)]	Loss: 0.245995
Epoch: 123 [2560/5999 (43%)]	Loss: 0.336466
Epoch: 123 [5120/5999 (85%)]	Loss: 0.203114
saving model at:123,0.0014712902419269085
====> Epoch: 123 Average loss: 0.002049 
Epoch: 124 [0/5999 (0%)]	Loss: 0.190349
Epoch: 124 [2560/5999 (43%)]	Loss: 0.204287
saving model at:124,0.0014599760659039021
Epoch: 124 [5120/5999 (85%)]	Loss: 0.177721
saving model at:124,0.001356519928202033
====> Epoch: 124 Average loss: 0.001709 
Epoch: 125 [0/5999 (0%)]	Loss: 0.162558
Epoch: 125 [2560/5999 (43%)]	Loss: 0.158915
saving model at:125,0.0011940401829779149
Epoch: 125 [5120/5999 (85%)]	Loss: 0.158680
saving model at:125,0.0011550706084817648
====> Epoch: 125 Average loss: 0.001417 
Epoch: 126 [0/5999 (0%)]	Loss: 0.147636
Epoch: 126 [2560/5999 (43%)]	Loss: 0.253729
Epoch: 126 [5120/5999 (85%)]	Loss: 0.127309
saving model at:126,0.0008774910094216466
====> Epoch: 126 Average loss: 0.001248 
Epoch: 127 [0/5999 (0%)]	Loss: 0.105855
Epoch: 127 [2560/5999 (43%)]	Loss: 0.098694
saving model at:127,0.000770240151323378
Epoch: 127 [5120/5999 (85%)]	Loss: 0.161690
====> Epoch: 127 Average loss: 0.001176 
Epoch: 128 [0/5999 (0%)]	Loss: 0.129396
Epoch: 128 [2560/5999 (43%)]	Loss: 0.105792
Epoch: 128 [5120/5999 (85%)]	Loss: 0.142788
====> Epoch: 128 Average loss: 0.001052 
Epoch: 129 [0/5999 (0%)]	Loss: 0.146629
Epoch: 129 [2560/5999 (43%)]	Loss: 0.088823
saving model at:129,0.0006002426659688354
Epoch: 129 [5120/5999 (85%)]	Loss: 0.111722
====> Epoch: 129 Average loss: 0.000847 
Epoch: 130 [0/5999 (0%)]	Loss: 0.093621
Epoch: 130 [2560/5999 (43%)]	Loss: 0.102269
Epoch: 130 [5120/5999 (85%)]	Loss: 0.070295
====> Epoch: 130 Average loss: 0.000851 
Epoch: 131 [0/5999 (0%)]	Loss: 0.110532
Epoch: 131 [2560/5999 (43%)]	Loss: 0.086714
Epoch: 131 [5120/5999 (85%)]	Loss: 0.142230
====> Epoch: 131 Average loss: 0.000915 
Epoch: 132 [0/5999 (0%)]	Loss: 0.093458
Epoch: 132 [2560/5999 (43%)]	Loss: 0.086686
Epoch: 132 [5120/5999 (85%)]	Loss: 0.285594
====> Epoch: 132 Average loss: 0.000892 
Epoch: 133 [0/5999 (0%)]	Loss: 0.099422
Epoch: 133 [2560/5999 (43%)]	Loss: 0.078105
Epoch: 133 [5120/5999 (85%)]	Loss: 0.138303
====> Epoch: 133 Average loss: 0.000933 
Epoch: 134 [0/5999 (0%)]	Loss: 0.175782
Epoch: 134 [2560/5999 (43%)]	Loss: 0.078942
saving model at:134,0.0005203820506576449
Epoch: 134 [5120/5999 (85%)]	Loss: 0.106584
====> Epoch: 134 Average loss: 0.000753 
Epoch: 135 [0/5999 (0%)]	Loss: 0.078431
Epoch: 135 [2560/5999 (43%)]	Loss: 0.110578
Epoch: 135 [5120/5999 (85%)]	Loss: 0.104368
====> Epoch: 135 Average loss: 0.000945 
Epoch: 136 [0/5999 (0%)]	Loss: 0.088676
Epoch: 136 [2560/5999 (43%)]	Loss: 0.063442
saving model at:136,0.00042258581519126893
Epoch: 136 [5120/5999 (85%)]	Loss: 0.086724
====> Epoch: 136 Average loss: 0.000643 
Epoch: 137 [0/5999 (0%)]	Loss: 0.093151
Epoch: 137 [2560/5999 (43%)]	Loss: 0.098097
saving model at:137,0.00040277341892942786
Epoch: 137 [5120/5999 (85%)]	Loss: 0.098513
====> Epoch: 137 Average loss: 0.000704 
Epoch: 138 [0/5999 (0%)]	Loss: 0.106831
Epoch: 138 [2560/5999 (43%)]	Loss: 0.074083
Epoch: 138 [5120/5999 (85%)]	Loss: 0.057512
saving model at:138,0.0003920488222502172
====> Epoch: 138 Average loss: 0.000713 
Epoch: 139 [0/5999 (0%)]	Loss: 0.050329
Epoch: 139 [2560/5999 (43%)]	Loss: 0.047348
saving model at:139,0.00035163247399032117
Epoch: 139 [5120/5999 (85%)]	Loss: 0.051840
====> Epoch: 139 Average loss: 0.000591 
Epoch: 140 [0/5999 (0%)]	Loss: 0.248001
Epoch: 140 [2560/5999 (43%)]	Loss: 0.218323
Epoch: 140 [5120/5999 (85%)]	Loss: 0.070475
====> Epoch: 140 Average loss: 0.000993 
Epoch: 141 [0/5999 (0%)]	Loss: 0.081088
Epoch: 141 [2560/5999 (43%)]	Loss: 0.104879
Epoch: 141 [5120/5999 (85%)]	Loss: 0.078576
====> Epoch: 141 Average loss: 0.000726 
Epoch: 142 [0/5999 (0%)]	Loss: 0.062216
Epoch: 142 [2560/5999 (43%)]	Loss: 0.047929
saving model at:142,0.0003270676671527326
Epoch: 142 [5120/5999 (85%)]	Loss: 0.086801
====> Epoch: 142 Average loss: 0.000556 
Epoch: 143 [0/5999 (0%)]	Loss: 0.114208
Epoch: 143 [2560/5999 (43%)]	Loss: 0.124400
Epoch: 143 [5120/5999 (85%)]	Loss: 0.050260
====> Epoch: 143 Average loss: 0.000718 
Epoch: 144 [0/5999 (0%)]	Loss: 0.070524
Epoch: 144 [2560/5999 (43%)]	Loss: 0.277999
Epoch: 144 [5120/5999 (85%)]	Loss: 0.048349
====> Epoch: 144 Average loss: 0.000843 
Epoch: 145 [0/5999 (0%)]	Loss: 0.060090
Epoch: 145 [2560/5999 (43%)]	Loss: 0.054400
Epoch: 145 [5120/5999 (85%)]	Loss: 0.047482
====> Epoch: 145 Average loss: 0.000567 
Epoch: 146 [0/5999 (0%)]	Loss: 0.050024
Epoch: 146 [2560/5999 (43%)]	Loss: 0.103016
Epoch: 146 [5120/5999 (85%)]	Loss: 0.130417
====> Epoch: 146 Average loss: 0.000652 
Epoch: 147 [0/5999 (0%)]	Loss: 0.117768
Epoch: 147 [2560/5999 (43%)]	Loss: 0.067660
Epoch: 147 [5120/5999 (85%)]	Loss: 0.075019
====> Epoch: 147 Average loss: 0.000779 
Epoch: 148 [0/5999 (0%)]	Loss: 0.052319
Epoch: 148 [2560/5999 (43%)]	Loss: 0.058275
Epoch: 148 [5120/5999 (85%)]	Loss: 0.062273
saving model at:148,0.00032058629975654184
====> Epoch: 148 Average loss: 0.000545 
Epoch: 149 [0/5999 (0%)]	Loss: 0.043594
Epoch: 149 [2560/5999 (43%)]	Loss: 0.046411
saving model at:149,0.0002665845073061064
Epoch: 149 [5120/5999 (85%)]	Loss: 0.098227
====> Epoch: 149 Average loss: 0.000601 
Epoch: 150 [0/5999 (0%)]	Loss: 0.080167
Epoch: 150 [2560/5999 (43%)]	Loss: 0.040198
Epoch: 150 [5120/5999 (85%)]	Loss: 0.086612
====> Epoch: 150 Average loss: 0.000631 
Epoch: 151 [0/5999 (0%)]	Loss: 0.037492
Epoch: 151 [2560/5999 (43%)]	Loss: 0.047533
Epoch: 151 [5120/5999 (85%)]	Loss: 0.095804
====> Epoch: 151 Average loss: 0.000529 
Epoch: 152 [0/5999 (0%)]	Loss: 0.091234
Epoch: 152 [2560/5999 (43%)]	Loss: 0.041498
Epoch: 152 [5120/5999 (85%)]	Loss: 0.061664
====> Epoch: 152 Average loss: 0.000643 
Epoch: 153 [0/5999 (0%)]	Loss: 0.050843
Epoch: 153 [2560/5999 (43%)]	Loss: 0.052249
Epoch: 153 [5120/5999 (85%)]	Loss: 0.055381
saving model at:153,0.0002412223282735795
====> Epoch: 153 Average loss: 0.000463 
Epoch: 154 [0/5999 (0%)]	Loss: 0.047369
Epoch: 154 [2560/5999 (43%)]	Loss: 0.092927
Epoch: 154 [5120/5999 (85%)]	Loss: 0.113900
====> Epoch: 154 Average loss: 0.000794 
Epoch: 155 [0/5999 (0%)]	Loss: 0.097734
Epoch: 155 [2560/5999 (43%)]	Loss: 0.053976
Epoch: 155 [5120/5999 (85%)]	Loss: 0.043005
====> Epoch: 155 Average loss: 0.000550 
Epoch: 156 [0/5999 (0%)]	Loss: 0.073516
Epoch: 156 [2560/5999 (43%)]	Loss: 0.042512
Epoch: 156 [5120/5999 (85%)]	Loss: 0.073719
====> Epoch: 156 Average loss: 0.000535 
Epoch: 157 [0/5999 (0%)]	Loss: 0.041264
Epoch: 157 [2560/5999 (43%)]	Loss: 0.044121
Epoch: 157 [5120/5999 (85%)]	Loss: 0.051180
====> Epoch: 157 Average loss: 0.000493 
Epoch: 158 [0/5999 (0%)]	Loss: 0.057726
Epoch: 158 [2560/5999 (43%)]	Loss: 0.130087
Epoch: 158 [5120/5999 (85%)]	Loss: 0.050597
====> Epoch: 158 Average loss: 0.000730 
Epoch: 159 [0/5999 (0%)]	Loss: 0.097917
Epoch: 159 [2560/5999 (43%)]	Loss: 0.071158
Epoch: 159 [5120/5999 (85%)]	Loss: 0.150476
====> Epoch: 159 Average loss: 0.000785 
Epoch: 160 [0/5999 (0%)]	Loss: 0.059039
Epoch: 160 [2560/5999 (43%)]	Loss: 0.035023
Epoch: 160 [5120/5999 (85%)]	Loss: 0.035593
====> Epoch: 160 Average loss: 0.000485 
Epoch: 161 [0/5999 (0%)]	Loss: 0.037282
Epoch: 161 [2560/5999 (43%)]	Loss: 0.044493
Epoch: 161 [5120/5999 (85%)]	Loss: 0.105906
====> Epoch: 161 Average loss: 0.000418 
Epoch: 162 [0/5999 (0%)]	Loss: 0.046057
Epoch: 162 [2560/5999 (43%)]	Loss: 0.031702
Epoch: 162 [5120/5999 (85%)]	Loss: 0.038926
saving model at:162,0.00022016903932671995
====> Epoch: 162 Average loss: 0.000513 
Epoch: 163 [0/5999 (0%)]	Loss: 0.069337
Epoch: 163 [2560/5999 (43%)]	Loss: 0.040843
Epoch: 163 [5120/5999 (85%)]	Loss: 0.033124
====> Epoch: 163 Average loss: 0.000424 
Epoch: 164 [0/5999 (0%)]	Loss: 0.180226
Epoch: 164 [2560/5999 (43%)]	Loss: 0.042770
Epoch: 164 [5120/5999 (85%)]	Loss: 0.058941
====> Epoch: 164 Average loss: 0.000497 
Epoch: 165 [0/5999 (0%)]	Loss: 0.030392
Epoch: 165 [2560/5999 (43%)]	Loss: 0.039041
Epoch: 165 [5120/5999 (85%)]	Loss: 0.094269
====> Epoch: 165 Average loss: 0.000756 
Epoch: 166 [0/5999 (0%)]	Loss: 0.056736
Epoch: 166 [2560/5999 (43%)]	Loss: 0.127603
Epoch: 166 [5120/5999 (85%)]	Loss: 0.041009
====> Epoch: 166 Average loss: 0.000674 
Epoch: 167 [0/5999 (0%)]	Loss: 0.070174
Epoch: 167 [2560/5999 (43%)]	Loss: 0.057372
Epoch: 167 [5120/5999 (85%)]	Loss: 0.177845
====> Epoch: 167 Average loss: 0.000766 
Epoch: 168 [0/5999 (0%)]	Loss: 0.130529
Epoch: 168 [2560/5999 (43%)]	Loss: 0.050423
Epoch: 168 [5120/5999 (85%)]	Loss: 0.049172
====> Epoch: 168 Average loss: 0.000683 
Epoch: 169 [0/5999 (0%)]	Loss: 0.102829
Epoch: 169 [2560/5999 (43%)]	Loss: 0.036973
Epoch: 169 [5120/5999 (85%)]	Loss: 0.086779
====> Epoch: 169 Average loss: 0.000719 
Epoch: 170 [0/5999 (0%)]	Loss: 0.049245
Epoch: 170 [2560/5999 (43%)]	Loss: 0.072070
Epoch: 170 [5120/5999 (85%)]	Loss: 0.112870
====> Epoch: 170 Average loss: 0.000640 
Epoch: 171 [0/5999 (0%)]	Loss: 0.051367
Epoch: 171 [2560/5999 (43%)]	Loss: 0.195897
Epoch: 171 [5120/5999 (85%)]	Loss: 0.041785
====> Epoch: 171 Average loss: 0.000554 
Epoch: 172 [0/5999 (0%)]	Loss: 0.043519
Epoch: 172 [2560/5999 (43%)]	Loss: 0.067451
Epoch: 172 [5120/5999 (85%)]	Loss: 0.055949
saving model at:172,0.00020724954875186085
====> Epoch: 172 Average loss: 0.000462 
Epoch: 173 [0/5999 (0%)]	Loss: 0.051520
Epoch: 173 [2560/5999 (43%)]	Loss: 0.056574
Epoch: 173 [5120/5999 (85%)]	Loss: 0.151492
====> Epoch: 173 Average loss: 0.000858 
Epoch: 174 [0/5999 (0%)]	Loss: 0.082757
Epoch: 174 [2560/5999 (43%)]	Loss: 0.044625
Epoch: 174 [5120/5999 (85%)]	Loss: 0.045721
saving model at:174,0.00020709810871630908
====> Epoch: 174 Average loss: 0.000422 
Epoch: 175 [0/5999 (0%)]	Loss: 0.070299
Epoch: 175 [2560/5999 (43%)]	Loss: 0.169537
Epoch: 175 [5120/5999 (85%)]	Loss: 0.065801
====> Epoch: 175 Average loss: 0.000618 
Epoch: 176 [0/5999 (0%)]	Loss: 0.037303
Epoch: 176 [2560/5999 (43%)]	Loss: 0.063697
Epoch: 176 [5120/5999 (85%)]	Loss: 0.055946
====> Epoch: 176 Average loss: 0.000602 
Epoch: 177 [0/5999 (0%)]	Loss: 0.118672
Epoch: 177 [2560/5999 (43%)]	Loss: 0.060169
Epoch: 177 [5120/5999 (85%)]	Loss: 0.091903
====> Epoch: 177 Average loss: 0.000487 
Epoch: 178 [0/5999 (0%)]	Loss: 0.037281
Epoch: 178 [2560/5999 (43%)]	Loss: 0.090256
Epoch: 178 [5120/5999 (85%)]	Loss: 0.056926
====> Epoch: 178 Average loss: 0.000641 
Epoch: 179 [0/5999 (0%)]	Loss: 0.043764
Epoch: 179 [2560/5999 (43%)]	Loss: 0.050796
Epoch: 179 [5120/5999 (85%)]	Loss: 0.169935
====> Epoch: 179 Average loss: 0.000639 
Epoch: 180 [0/5999 (0%)]	Loss: 0.080451
Epoch: 180 [2560/5999 (43%)]	Loss: 0.042848
saving model at:180,0.00017642391065601258
Epoch: 180 [5120/5999 (85%)]	Loss: 0.087120
====> Epoch: 180 Average loss: 0.000501 
Epoch: 181 [0/5999 (0%)]	Loss: 0.116955
Epoch: 181 [2560/5999 (43%)]	Loss: 0.051394
Epoch: 181 [5120/5999 (85%)]	Loss: 0.037749
====> Epoch: 181 Average loss: 0.000444 
Epoch: 182 [0/5999 (0%)]	Loss: 0.068847
Epoch: 182 [2560/5999 (43%)]	Loss: 0.026804
Epoch: 182 [5120/5999 (85%)]	Loss: 0.057066
====> Epoch: 182 Average loss: 0.000426 
Epoch: 183 [0/5999 (0%)]	Loss: 0.045003
Epoch: 183 [2560/5999 (43%)]	Loss: 0.029479
saving model at:183,0.00017246969742700457
Epoch: 183 [5120/5999 (85%)]	Loss: 0.038774
====> Epoch: 183 Average loss: 0.000351 
Epoch: 184 [0/5999 (0%)]	Loss: 0.037436
Epoch: 184 [2560/5999 (43%)]	Loss: 0.064019
Epoch: 184 [5120/5999 (85%)]	Loss: 0.069917
====> Epoch: 184 Average loss: 0.000486 
Epoch: 185 [0/5999 (0%)]	Loss: 0.026833
Epoch: 185 [2560/5999 (43%)]	Loss: 0.047950
Epoch: 185 [5120/5999 (85%)]	Loss: 0.025750
====> Epoch: 185 Average loss: 0.000526 
Epoch: 186 [0/5999 (0%)]	Loss: 0.040742
Epoch: 186 [2560/5999 (43%)]	Loss: 0.114357
Epoch: 186 [5120/5999 (85%)]	Loss: 0.040397
====> Epoch: 186 Average loss: 0.000449 
Epoch: 187 [0/5999 (0%)]	Loss: 0.064646
Epoch: 187 [2560/5999 (43%)]	Loss: 0.047220
Epoch: 187 [5120/5999 (85%)]	Loss: 0.073319
====> Epoch: 187 Average loss: 0.000708 
Epoch: 188 [0/5999 (0%)]	Loss: 0.091915
Epoch: 188 [2560/5999 (43%)]	Loss: 0.078869
Epoch: 188 [5120/5999 (85%)]	Loss: 0.163307
====> Epoch: 188 Average loss: 0.000646 
Epoch: 189 [0/5999 (0%)]	Loss: 0.092912
Epoch: 189 [2560/5999 (43%)]	Loss: 0.039005
Epoch: 189 [5120/5999 (85%)]	Loss: 0.035208
====> Epoch: 189 Average loss: 0.000527 
Epoch: 190 [0/5999 (0%)]	Loss: 0.051466
Epoch: 190 [2560/5999 (43%)]	Loss: 0.032616
Epoch: 190 [5120/5999 (85%)]	Loss: 0.066751
====> Epoch: 190 Average loss: 0.000467 
Epoch: 191 [0/5999 (0%)]	Loss: 0.050663
Epoch: 191 [2560/5999 (43%)]	Loss: 0.154869
Epoch: 191 [5120/5999 (85%)]	Loss: 0.064560
====> Epoch: 191 Average loss: 0.000722 
Epoch: 192 [0/5999 (0%)]	Loss: 0.026522
Epoch: 192 [2560/5999 (43%)]	Loss: 0.046622
Epoch: 192 [5120/5999 (85%)]	Loss: 0.052195
====> Epoch: 192 Average loss: 0.000484 
Epoch: 193 [0/5999 (0%)]	Loss: 0.023255
Epoch: 193 [2560/5999 (43%)]	Loss: 0.052043
Epoch: 193 [5120/5999 (85%)]	Loss: 0.038258
====> Epoch: 193 Average loss: 0.000402 
Epoch: 194 [0/5999 (0%)]	Loss: 0.024976
Epoch: 194 [2560/5999 (43%)]	Loss: 0.033135
Epoch: 194 [5120/5999 (85%)]	Loss: 0.107079
====> Epoch: 194 Average loss: 0.000565 
Epoch: 195 [0/5999 (0%)]	Loss: 0.041078
Epoch: 195 [2560/5999 (43%)]	Loss: 0.059502
Epoch: 195 [5120/5999 (85%)]	Loss: 0.077214
====> Epoch: 195 Average loss: 0.000505 
Epoch: 196 [0/5999 (0%)]	Loss: 0.028459
Epoch: 196 [2560/5999 (43%)]	Loss: 0.032773
Epoch: 196 [5120/5999 (85%)]	Loss: 0.032519
saving model at:196,0.00016729242401197552
====> Epoch: 196 Average loss: 0.000371 
Epoch: 197 [0/5999 (0%)]	Loss: 0.028788
Epoch: 197 [2560/5999 (43%)]	Loss: 0.202396
Epoch: 197 [5120/5999 (85%)]	Loss: 0.041590
====> Epoch: 197 Average loss: 0.000414 
Epoch: 198 [0/5999 (0%)]	Loss: 0.028622
Epoch: 198 [2560/5999 (43%)]	Loss: 0.030288
Epoch: 198 [5120/5999 (85%)]	Loss: 0.103246
====> Epoch: 198 Average loss: 0.000670 
Epoch: 199 [0/5999 (0%)]	Loss: 0.039803
Epoch: 199 [2560/5999 (43%)]	Loss: 0.037956
Epoch: 199 [5120/5999 (85%)]	Loss: 0.024068
====> Epoch: 199 Average loss: 0.000362 
Epoch: 200 [0/5999 (0%)]	Loss: 0.049446
Epoch: 200 [2560/5999 (43%)]	Loss: 0.030044
Epoch: 200 [5120/5999 (85%)]	Loss: 0.029094
====> Epoch: 200 Average loss: 0.000432 
Epoch: 201 [0/5999 (0%)]	Loss: 0.027448
Epoch: 201 [2560/5999 (43%)]	Loss: 0.064241
Epoch: 201 [5120/5999 (85%)]	Loss: 0.029894
====> Epoch: 201 Average loss: 0.000400 
Epoch: 202 [0/5999 (0%)]	Loss: 0.027497
Epoch: 202 [2560/5999 (43%)]	Loss: 0.142873
Epoch: 202 [5120/5999 (85%)]	Loss: 0.041955
====> Epoch: 202 Average loss: 0.000594 
Epoch: 203 [0/5999 (0%)]	Loss: 0.033380
Epoch: 203 [2560/5999 (43%)]	Loss: 0.096536
Epoch: 203 [5120/5999 (85%)]	Loss: 0.049057
====> Epoch: 203 Average loss: 0.000608 
Epoch: 204 [0/5999 (0%)]	Loss: 0.034990
Epoch: 204 [2560/5999 (43%)]	Loss: 0.022219
saving model at:204,0.00016548091114964336
Epoch: 204 [5120/5999 (85%)]	Loss: 0.123362
====> Epoch: 204 Average loss: 0.000353 
Epoch: 205 [0/5999 (0%)]	Loss: 0.025728
Epoch: 205 [2560/5999 (43%)]	Loss: 0.034825
Epoch: 205 [5120/5999 (85%)]	Loss: 0.033678
saving model at:205,0.00016486834431998432
====> Epoch: 205 Average loss: 0.000351 
Epoch: 206 [0/5999 (0%)]	Loss: 0.056381
Epoch: 206 [2560/5999 (43%)]	Loss: 0.030154
saving model at:206,0.0001472562256967649
Epoch: 206 [5120/5999 (85%)]	Loss: 0.028060
====> Epoch: 206 Average loss: 0.000455 
Epoch: 207 [0/5999 (0%)]	Loss: 0.143735
Epoch: 207 [2560/5999 (43%)]	Loss: 0.042857
Epoch: 207 [5120/5999 (85%)]	Loss: 0.027571
====> Epoch: 207 Average loss: 0.000322 
Epoch: 208 [0/5999 (0%)]	Loss: 0.068217
Epoch: 208 [2560/5999 (43%)]	Loss: 0.096925
Epoch: 208 [5120/5999 (85%)]	Loss: 0.068179
====> Epoch: 208 Average loss: 0.000595 
Epoch: 209 [0/5999 (0%)]	Loss: 0.044222
Epoch: 209 [2560/5999 (43%)]	Loss: 0.059010
Epoch: 209 [5120/5999 (85%)]	Loss: 0.050720
====> Epoch: 209 Average loss: 0.000364 
Epoch: 210 [0/5999 (0%)]	Loss: 0.078929
Epoch: 210 [2560/5999 (43%)]	Loss: 0.072596
Epoch: 210 [5120/5999 (85%)]	Loss: 0.028853
====> Epoch: 210 Average loss: 0.000391 
Epoch: 211 [0/5999 (0%)]	Loss: 0.187555
Epoch: 211 [2560/5999 (43%)]	Loss: 0.024010
Epoch: 211 [5120/5999 (85%)]	Loss: 0.055734
====> Epoch: 211 Average loss: 0.000479 
Epoch: 212 [0/5999 (0%)]	Loss: 0.182114
Epoch: 212 [2560/5999 (43%)]	Loss: 0.038878
Epoch: 212 [5120/5999 (85%)]	Loss: 0.029981
====> Epoch: 212 Average loss: 0.000507 
Epoch: 213 [0/5999 (0%)]	Loss: 0.028621
Epoch: 213 [2560/5999 (43%)]	Loss: 0.055320
Epoch: 213 [5120/5999 (85%)]	Loss: 0.033186
====> Epoch: 213 Average loss: 0.000616 
Epoch: 214 [0/5999 (0%)]	Loss: 0.038559
Epoch: 214 [2560/5999 (43%)]	Loss: 0.037176
Epoch: 214 [5120/5999 (85%)]	Loss: 0.093504
====> Epoch: 214 Average loss: 0.000454 
Epoch: 215 [0/5999 (0%)]	Loss: 0.060507
Epoch: 215 [2560/5999 (43%)]	Loss: 0.062181
Epoch: 215 [5120/5999 (85%)]	Loss: 0.033996
====> Epoch: 215 Average loss: 0.000611 
Epoch: 216 [0/5999 (0%)]	Loss: 0.044861
Epoch: 216 [2560/5999 (43%)]	Loss: 0.034731
saving model at:216,0.00014494726224802436
Epoch: 216 [5120/5999 (85%)]	Loss: 0.051545
====> Epoch: 216 Average loss: 0.000280 
Epoch: 217 [0/5999 (0%)]	Loss: 0.036999
Epoch: 217 [2560/5999 (43%)]	Loss: 0.112508
Epoch: 217 [5120/5999 (85%)]	Loss: 0.027793
====> Epoch: 217 Average loss: 0.000460 
Epoch: 218 [0/5999 (0%)]	Loss: 0.055927
Epoch: 218 [2560/5999 (43%)]	Loss: 0.074156
Epoch: 218 [5120/5999 (85%)]	Loss: 0.033745
====> Epoch: 218 Average loss: 0.000495 
Epoch: 219 [0/5999 (0%)]	Loss: 0.040477
Epoch: 219 [2560/5999 (43%)]	Loss: 0.049375
Epoch: 219 [5120/5999 (85%)]	Loss: 0.026294
====> Epoch: 219 Average loss: 0.000317 
Epoch: 220 [0/5999 (0%)]	Loss: 0.083194
Epoch: 220 [2560/5999 (43%)]	Loss: 0.039370
Epoch: 220 [5120/5999 (85%)]	Loss: 0.025936
====> Epoch: 220 Average loss: 0.000391 
Epoch: 221 [0/5999 (0%)]	Loss: 0.028533
Epoch: 221 [2560/5999 (43%)]	Loss: 0.038918
Epoch: 221 [5120/5999 (85%)]	Loss: 0.026515
saving model at:221,0.00013801275985315443
====> Epoch: 221 Average loss: 0.000397 
Epoch: 222 [0/5999 (0%)]	Loss: 0.080777
Epoch: 222 [2560/5999 (43%)]	Loss: 0.065717
Epoch: 222 [5120/5999 (85%)]	Loss: 0.050621
====> Epoch: 222 Average loss: 0.000427 
Epoch: 223 [0/5999 (0%)]	Loss: 0.053475
Epoch: 223 [2560/5999 (43%)]	Loss: 0.096564
Epoch: 223 [5120/5999 (85%)]	Loss: 0.031232
====> Epoch: 223 Average loss: 0.000498 
Epoch: 224 [0/5999 (0%)]	Loss: 0.037674
Epoch: 224 [2560/5999 (43%)]	Loss: 0.066745
Epoch: 224 [5120/5999 (85%)]	Loss: 0.035815
====> Epoch: 224 Average loss: 0.000345 
Epoch: 225 [0/5999 (0%)]	Loss: 0.072745
Epoch: 225 [2560/5999 (43%)]	Loss: 0.091532
Epoch: 225 [5120/5999 (85%)]	Loss: 0.047219
====> Epoch: 225 Average loss: 0.000577 
Epoch: 226 [0/5999 (0%)]	Loss: 0.066083
Epoch: 226 [2560/5999 (43%)]	Loss: 0.034548
Epoch: 226 [5120/5999 (85%)]	Loss: 0.102457
====> Epoch: 226 Average loss: 0.000415 
Epoch: 227 [0/5999 (0%)]	Loss: 0.158558
Epoch: 227 [2560/5999 (43%)]	Loss: 0.037952
Epoch: 227 [5120/5999 (85%)]	Loss: 0.034106
====> Epoch: 227 Average loss: 0.000462 
Epoch: 228 [0/5999 (0%)]	Loss: 0.062428
Epoch: 228 [2560/5999 (43%)]	Loss: 0.142260
Epoch: 228 [5120/5999 (85%)]	Loss: 0.043158
====> Epoch: 228 Average loss: 0.000443 
Epoch: 229 [0/5999 (0%)]	Loss: 0.113190
Epoch: 229 [2560/5999 (43%)]	Loss: 0.071693
Epoch: 229 [5120/5999 (85%)]	Loss: 0.052900
====> Epoch: 229 Average loss: 0.000415 
Epoch: 230 [0/5999 (0%)]	Loss: 0.023924
Epoch: 230 [2560/5999 (43%)]	Loss: 0.021232
Epoch: 230 [5120/5999 (85%)]	Loss: 0.065218
====> Epoch: 230 Average loss: 0.000350 
Epoch: 231 [0/5999 (0%)]	Loss: 0.043893
Epoch: 231 [2560/5999 (43%)]	Loss: 0.044256
Epoch: 231 [5120/5999 (85%)]	Loss: 0.057132
====> Epoch: 231 Average loss: 0.000591 
Epoch: 232 [0/5999 (0%)]	Loss: 0.115900
Epoch: 232 [2560/5999 (43%)]	Loss: 0.022320
Epoch: 232 [5120/5999 (85%)]	Loss: 0.021577
saving model at:232,0.0001352890975540504
====> Epoch: 232 Average loss: 0.000359 
Epoch: 233 [0/5999 (0%)]	Loss: 0.022802
Epoch: 233 [2560/5999 (43%)]	Loss: 0.095927
Epoch: 233 [5120/5999 (85%)]	Loss: 0.027090
====> Epoch: 233 Average loss: 0.000380 
Epoch: 234 [0/5999 (0%)]	Loss: 0.024084
Epoch: 234 [2560/5999 (43%)]	Loss: 0.031608
Epoch: 234 [5120/5999 (85%)]	Loss: 0.031900
====> Epoch: 234 Average loss: 0.000476 
Epoch: 235 [0/5999 (0%)]	Loss: 0.079246
Epoch: 235 [2560/5999 (43%)]	Loss: 0.051215
Epoch: 235 [5120/5999 (85%)]	Loss: 0.142786
====> Epoch: 235 Average loss: 0.000432 
Epoch: 236 [0/5999 (0%)]	Loss: 0.042511
Epoch: 236 [2560/5999 (43%)]	Loss: 0.034776
Epoch: 236 [5120/5999 (85%)]	Loss: 0.059737
====> Epoch: 236 Average loss: 0.000326 
Epoch: 237 [0/5999 (0%)]	Loss: 0.057267
Epoch: 237 [2560/5999 (43%)]	Loss: 0.028135
Epoch: 237 [5120/5999 (85%)]	Loss: 0.056866
====> Epoch: 237 Average loss: 0.000336 
Epoch: 238 [0/5999 (0%)]	Loss: 0.025235
Epoch: 238 [2560/5999 (43%)]	Loss: 0.039887
Epoch: 238 [5120/5999 (85%)]	Loss: 0.040502
====> Epoch: 238 Average loss: 0.000430 
Epoch: 239 [0/5999 (0%)]	Loss: 0.062215
Epoch: 239 [2560/5999 (43%)]	Loss: 0.066661
Epoch: 239 [5120/5999 (85%)]	Loss: 0.030873
====> Epoch: 239 Average loss: 0.000478 
Epoch: 240 [0/5999 (0%)]	Loss: 0.039885
Epoch: 240 [2560/5999 (43%)]	Loss: 0.053379
Epoch: 240 [5120/5999 (85%)]	Loss: 0.048376
====> Epoch: 240 Average loss: 0.000570 
Epoch: 241 [0/5999 (0%)]	Loss: 0.049870
Epoch: 241 [2560/5999 (43%)]	Loss: 0.033780
Epoch: 241 [5120/5999 (85%)]	Loss: 0.033470
====> Epoch: 241 Average loss: 0.000425 
Epoch: 242 [0/5999 (0%)]	Loss: 0.044901
Epoch: 242 [2560/5999 (43%)]	Loss: 0.022355
saving model at:242,0.00012004571344004944
Epoch: 242 [5120/5999 (85%)]	Loss: 0.058900
====> Epoch: 242 Average loss: 0.000414 
Epoch: 243 [0/5999 (0%)]	Loss: 0.045048
Epoch: 243 [2560/5999 (43%)]	Loss: 0.022357
Epoch: 243 [5120/5999 (85%)]	Loss: 0.056806
====> Epoch: 243 Average loss: 0.000466 
Epoch: 244 [0/5999 (0%)]	Loss: 0.033401
Epoch: 244 [2560/5999 (43%)]	Loss: 0.019768
Epoch: 244 [5120/5999 (85%)]	Loss: 0.043887
====> Epoch: 244 Average loss: 0.000360 
Epoch: 245 [0/5999 (0%)]	Loss: 0.025429
Epoch: 245 [2560/5999 (43%)]	Loss: 0.099993
Epoch: 245 [5120/5999 (85%)]	Loss: 0.055194
====> Epoch: 245 Average loss: 0.000351 
Epoch: 246 [0/5999 (0%)]	Loss: 0.019198
Epoch: 246 [2560/5999 (43%)]	Loss: 0.032328
Epoch: 246 [5120/5999 (85%)]	Loss: 0.055635
====> Epoch: 246 Average loss: 0.000356 
Epoch: 247 [0/5999 (0%)]	Loss: 0.053172
Epoch: 247 [2560/5999 (43%)]	Loss: 0.024644
Epoch: 247 [5120/5999 (85%)]	Loss: 0.034042
====> Epoch: 247 Average loss: 0.000396 
Epoch: 248 [0/5999 (0%)]	Loss: 0.067009
Epoch: 248 [2560/5999 (43%)]	Loss: 0.124138
Epoch: 248 [5120/5999 (85%)]	Loss: 0.039380
====> Epoch: 248 Average loss: 0.000418 
Epoch: 249 [0/5999 (0%)]	Loss: 0.057271
Epoch: 249 [2560/5999 (43%)]	Loss: 0.031607
saving model at:249,0.00011994070373475552
Epoch: 249 [5120/5999 (85%)]	Loss: 0.070265
====> Epoch: 249 Average loss: 0.000370 
Epoch: 250 [0/5999 (0%)]	Loss: 0.025195
Epoch: 250 [2560/5999 (43%)]	Loss: 0.024906
Epoch: 250 [5120/5999 (85%)]	Loss: 0.043948
====> Epoch: 250 Average loss: 0.000420 
Epoch: 251 [0/5999 (0%)]	Loss: 0.045701
Epoch: 251 [2560/5999 (43%)]	Loss: 0.020686
Epoch: 251 [5120/5999 (85%)]	Loss: 0.032273
====> Epoch: 251 Average loss: 0.000427 
Epoch: 252 [0/5999 (0%)]	Loss: 0.021673
Epoch: 252 [2560/5999 (43%)]	Loss: 0.059620
Epoch: 252 [5120/5999 (85%)]	Loss: 0.097115
====> Epoch: 252 Average loss: 0.000529 
Epoch: 253 [0/5999 (0%)]	Loss: 0.026710
Epoch: 253 [2560/5999 (43%)]	Loss: 0.048384
Epoch: 253 [5120/5999 (85%)]	Loss: 0.048650
====> Epoch: 253 Average loss: 0.000399 
Epoch: 254 [0/5999 (0%)]	Loss: 0.029699
Epoch: 254 [2560/5999 (43%)]	Loss: 0.054090
Epoch: 254 [5120/5999 (85%)]	Loss: 0.082416
====> Epoch: 254 Average loss: 0.000362 
Epoch: 255 [0/5999 (0%)]	Loss: 0.064842
Epoch: 255 [2560/5999 (43%)]	Loss: 0.039154
Epoch: 255 [5120/5999 (85%)]	Loss: 0.064226
====> Epoch: 255 Average loss: 0.000353 
Epoch: 256 [0/5999 (0%)]	Loss: 0.015944
Epoch: 256 [2560/5999 (43%)]	Loss: 0.031154
saving model at:256,0.00010153876501135528
Epoch: 256 [5120/5999 (85%)]	Loss: 0.019793
saving model at:256,9.611863864120096e-05
====> Epoch: 256 Average loss: 0.000286 
Epoch: 257 [0/5999 (0%)]	Loss: 0.017638
Epoch: 257 [2560/5999 (43%)]	Loss: 0.020417
Epoch: 257 [5120/5999 (85%)]	Loss: 0.023257
====> Epoch: 257 Average loss: 0.000269 
Epoch: 258 [0/5999 (0%)]	Loss: 0.035865
Epoch: 258 [2560/5999 (43%)]	Loss: 0.025839
Epoch: 258 [5120/5999 (85%)]	Loss: 0.027227
====> Epoch: 258 Average loss: 0.000427 
Epoch: 259 [0/5999 (0%)]	Loss: 0.037402
Epoch: 259 [2560/5999 (43%)]	Loss: 0.015971
Epoch: 259 [5120/5999 (85%)]	Loss: 0.031687
====> Epoch: 259 Average loss: 0.000383 
Epoch: 260 [0/5999 (0%)]	Loss: 0.048889
Epoch: 260 [2560/5999 (43%)]	Loss: 0.033482
Epoch: 260 [5120/5999 (85%)]	Loss: 0.131817
====> Epoch: 260 Average loss: 0.000434 
Epoch: 261 [0/5999 (0%)]	Loss: 0.039386
Epoch: 261 [2560/5999 (43%)]	Loss: 0.031846
Epoch: 261 [5120/5999 (85%)]	Loss: 0.122454
====> Epoch: 261 Average loss: 0.000418 
Epoch: 262 [0/5999 (0%)]	Loss: 0.041100
Epoch: 262 [2560/5999 (43%)]	Loss: 0.034924
Epoch: 262 [5120/5999 (85%)]	Loss: 0.093030
====> Epoch: 262 Average loss: 0.000327 
Epoch: 263 [0/5999 (0%)]	Loss: 0.026073
Epoch: 263 [2560/5999 (43%)]	Loss: 0.040759
Epoch: 263 [5120/5999 (85%)]	Loss: 0.036288
====> Epoch: 263 Average loss: 0.000355 
Epoch: 264 [0/5999 (0%)]	Loss: 0.029328
Epoch: 264 [2560/5999 (43%)]	Loss: 0.028227
Epoch: 264 [5120/5999 (85%)]	Loss: 0.045113
====> Epoch: 264 Average loss: 0.000347 
Epoch: 265 [0/5999 (0%)]	Loss: 0.023070
Epoch: 265 [2560/5999 (43%)]	Loss: 0.023236
Epoch: 265 [5120/5999 (85%)]	Loss: 0.031690
====> Epoch: 265 Average loss: 0.000299 
Epoch: 266 [0/5999 (0%)]	Loss: 0.039423
Epoch: 266 [2560/5999 (43%)]	Loss: 0.098639
Epoch: 266 [5120/5999 (85%)]	Loss: 0.030002
====> Epoch: 266 Average loss: 0.000349 
Epoch: 267 [0/5999 (0%)]	Loss: 0.076849
Epoch: 267 [2560/5999 (43%)]	Loss: 0.026048
Epoch: 267 [5120/5999 (85%)]	Loss: 0.079033
====> Epoch: 267 Average loss: 0.000585 
Epoch: 268 [0/5999 (0%)]	Loss: 0.030184
Epoch: 268 [2560/5999 (43%)]	Loss: 0.033975
Epoch: 268 [5120/5999 (85%)]	Loss: 0.040856
====> Epoch: 268 Average loss: 0.000417 
Epoch: 269 [0/5999 (0%)]	Loss: 0.033329
Epoch: 269 [2560/5999 (43%)]	Loss: 0.018036
Epoch: 269 [5120/5999 (85%)]	Loss: 0.021435
====> Epoch: 269 Average loss: 0.000263 
Epoch: 270 [0/5999 (0%)]	Loss: 0.016480
Epoch: 270 [2560/5999 (43%)]	Loss: 0.038942
Epoch: 270 [5120/5999 (85%)]	Loss: 0.035090
====> Epoch: 270 Average loss: 0.000249 
Epoch: 271 [0/5999 (0%)]	Loss: 0.015975
Epoch: 271 [2560/5999 (43%)]	Loss: 0.064159
Epoch: 271 [5120/5999 (85%)]	Loss: 0.033480
====> Epoch: 271 Average loss: 0.000343 
Epoch: 272 [0/5999 (0%)]	Loss: 0.048256
Epoch: 272 [2560/5999 (43%)]	Loss: 0.030455
Epoch: 272 [5120/5999 (85%)]	Loss: 0.025376
====> Epoch: 272 Average loss: 0.000326 
Epoch: 273 [0/5999 (0%)]	Loss: 0.042484
Epoch: 273 [2560/5999 (43%)]	Loss: 0.025729
Epoch: 273 [5120/5999 (85%)]	Loss: 0.051874
====> Epoch: 273 Average loss: 0.000505 
Epoch: 274 [0/5999 (0%)]	Loss: 0.030972
Epoch: 274 [2560/5999 (43%)]	Loss: 0.071639
Epoch: 274 [5120/5999 (85%)]	Loss: 0.030338
====> Epoch: 274 Average loss: 0.000491 
Epoch: 275 [0/5999 (0%)]	Loss: 0.071911
Epoch: 275 [2560/5999 (43%)]	Loss: 0.024236
Epoch: 275 [5120/5999 (85%)]	Loss: 0.018978
====> Epoch: 275 Average loss: 0.000364 
Epoch: 276 [0/5999 (0%)]	Loss: 0.034731
Epoch: 276 [2560/5999 (43%)]	Loss: 0.036031
Epoch: 276 [5120/5999 (85%)]	Loss: 0.029964
====> Epoch: 276 Average loss: 0.000432 
Epoch: 277 [0/5999 (0%)]	Loss: 0.031253
Epoch: 277 [2560/5999 (43%)]	Loss: 0.028569
Epoch: 277 [5120/5999 (85%)]	Loss: 0.062876
====> Epoch: 277 Average loss: 0.000392 
Epoch: 278 [0/5999 (0%)]	Loss: 0.216410
Epoch: 278 [2560/5999 (43%)]	Loss: 0.219279
Epoch: 278 [5120/5999 (85%)]	Loss: 0.042075
====> Epoch: 278 Average loss: 0.000480 
Epoch: 279 [0/5999 (0%)]	Loss: 0.063056
Epoch: 279 [2560/5999 (43%)]	Loss: 0.018689
Epoch: 279 [5120/5999 (85%)]	Loss: 0.015860
====> Epoch: 279 Average loss: 0.000310 
Epoch: 280 [0/5999 (0%)]	Loss: 0.078426
Epoch: 280 [2560/5999 (43%)]	Loss: 0.026811
Epoch: 280 [5120/5999 (85%)]	Loss: 0.045981
====> Epoch: 280 Average loss: 0.000516 
Epoch: 281 [0/5999 (0%)]	Loss: 0.033903
Epoch: 281 [2560/5999 (43%)]	Loss: 0.030839
Epoch: 281 [5120/5999 (85%)]	Loss: 0.018456
====> Epoch: 281 Average loss: 0.000387 
Epoch: 282 [0/5999 (0%)]	Loss: 0.103562
Epoch: 282 [2560/5999 (43%)]	Loss: 0.062972
Epoch: 282 [5120/5999 (85%)]	Loss: 0.061339
====> Epoch: 282 Average loss: 0.000551 
Epoch: 283 [0/5999 (0%)]	Loss: 0.046275
Epoch: 283 [2560/5999 (43%)]	Loss: 0.030747
Epoch: 283 [5120/5999 (85%)]	Loss: 0.056068
====> Epoch: 283 Average loss: 0.000386 
Epoch: 284 [0/5999 (0%)]	Loss: 0.024373
Epoch: 284 [2560/5999 (43%)]	Loss: 0.070338
Epoch: 284 [5120/5999 (85%)]	Loss: 0.043509
====> Epoch: 284 Average loss: 0.000294 
Epoch: 285 [0/5999 (0%)]	Loss: 0.041911
Epoch: 285 [2560/5999 (43%)]	Loss: 0.037927
Epoch: 285 [5120/5999 (85%)]	Loss: 0.039458
====> Epoch: 285 Average loss: 0.000421 
Epoch: 286 [0/5999 (0%)]	Loss: 0.032222
Epoch: 286 [2560/5999 (43%)]	Loss: 0.092101
Epoch: 286 [5120/5999 (85%)]	Loss: 0.039436
====> Epoch: 286 Average loss: 0.000360 
Epoch: 287 [0/5999 (0%)]	Loss: 0.043989
Epoch: 287 [2560/5999 (43%)]	Loss: 0.028397
Epoch: 287 [5120/5999 (85%)]	Loss: 0.169042
====> Epoch: 287 Average loss: 0.000420 
Epoch: 288 [0/5999 (0%)]	Loss: 0.076436
Epoch: 288 [2560/5999 (43%)]	Loss: 0.061348
Epoch: 288 [5120/5999 (85%)]	Loss: 0.022083
====> Epoch: 288 Average loss: 0.000337 
Epoch: 289 [0/5999 (0%)]	Loss: 0.036267
Epoch: 289 [2560/5999 (43%)]	Loss: 0.024305
Epoch: 289 [5120/5999 (85%)]	Loss: 0.081905
====> Epoch: 289 Average loss: 0.000298 
Epoch: 290 [0/5999 (0%)]	Loss: 0.019456
Epoch: 290 [2560/5999 (43%)]	Loss: 0.038717
Epoch: 290 [5120/5999 (85%)]	Loss: 0.019884
====> Epoch: 290 Average loss: 0.000503 
Epoch: 291 [0/5999 (0%)]	Loss: 0.033115
Epoch: 291 [2560/5999 (43%)]	Loss: 0.111083
Epoch: 291 [5120/5999 (85%)]	Loss: 0.075068
====> Epoch: 291 Average loss: 0.000498 
Epoch: 292 [0/5999 (0%)]	Loss: 0.021599
Epoch: 292 [2560/5999 (43%)]	Loss: 0.043455
Epoch: 292 [5120/5999 (85%)]	Loss: 0.020257
====> Epoch: 292 Average loss: 0.000418 
Epoch: 293 [0/5999 (0%)]	Loss: 0.042701
Epoch: 293 [2560/5999 (43%)]	Loss: 0.057740
Epoch: 293 [5120/5999 (85%)]	Loss: 0.021111
====> Epoch: 293 Average loss: 0.000437 
Epoch: 294 [0/5999 (0%)]	Loss: 0.073141
Epoch: 294 [2560/5999 (43%)]	Loss: 0.022836
Epoch: 294 [5120/5999 (85%)]	Loss: 0.092724
====> Epoch: 294 Average loss: 0.000370 
Epoch: 295 [0/5999 (0%)]	Loss: 0.024929
Epoch: 295 [2560/5999 (43%)]	Loss: 0.063680
Epoch: 295 [5120/5999 (85%)]	Loss: 0.120108
====> Epoch: 295 Average loss: 0.000636 
Epoch: 296 [0/5999 (0%)]	Loss: 0.180538
Epoch: 296 [2560/5999 (43%)]	Loss: 0.023560
Epoch: 296 [5120/5999 (85%)]	Loss: 0.031289
====> Epoch: 296 Average loss: 0.000442 
Epoch: 297 [0/5999 (0%)]	Loss: 0.015758
Epoch: 297 [2560/5999 (43%)]	Loss: 0.017310
Epoch: 297 [5120/5999 (85%)]	Loss: 0.104776
====> Epoch: 297 Average loss: 0.000285 
Epoch: 298 [0/5999 (0%)]	Loss: 0.038627
Epoch: 298 [2560/5999 (43%)]	Loss: 0.020561
Epoch: 298 [5120/5999 (85%)]	Loss: 0.022553
====> Epoch: 298 Average loss: 0.000344 
Epoch: 299 [0/5999 (0%)]	Loss: 0.044614
Epoch: 299 [2560/5999 (43%)]	Loss: 0.029041
Epoch: 299 [5120/5999 (85%)]	Loss: 0.062246
====> Epoch: 299 Average loss: 0.000308 
Epoch: 300 [0/5999 (0%)]	Loss: 0.027712
Epoch: 300 [2560/5999 (43%)]	Loss: 0.034529
Epoch: 300 [5120/5999 (85%)]	Loss: 0.066149
====> Epoch: 300 Average loss: 0.000411 
Epoch: 301 [0/5999 (0%)]	Loss: 0.066236
Epoch: 301 [2560/5999 (43%)]	Loss: 0.064743
Epoch: 301 [5120/5999 (85%)]	Loss: 0.080198
====> Epoch: 301 Average loss: 0.000469 
Epoch: 302 [0/5999 (0%)]	Loss: 0.092533
Epoch: 302 [2560/5999 (43%)]	Loss: 0.055810
Epoch: 302 [5120/5999 (85%)]	Loss: 0.033108
====> Epoch: 302 Average loss: 0.000451 
Epoch: 303 [0/5999 (0%)]	Loss: 0.029975
Epoch: 303 [2560/5999 (43%)]	Loss: 0.026864
Epoch: 303 [5120/5999 (85%)]	Loss: 0.029085
====> Epoch: 303 Average loss: 0.000458 
Epoch: 304 [0/5999 (0%)]	Loss: 0.047307
Epoch: 304 [2560/5999 (43%)]	Loss: 0.017956
Epoch: 304 [5120/5999 (85%)]	Loss: 0.053231
====> Epoch: 304 Average loss: 0.000345 
Epoch: 305 [0/5999 (0%)]	Loss: 0.030747
Epoch: 305 [2560/5999 (43%)]	Loss: 0.022452
Epoch: 305 [5120/5999 (85%)]	Loss: 0.019425
====> Epoch: 305 Average loss: 0.000295 
Epoch: 306 [0/5999 (0%)]	Loss: 0.035249
Epoch: 306 [2560/5999 (43%)]	Loss: 0.040763
Epoch: 306 [5120/5999 (85%)]	Loss: 0.047815
====> Epoch: 306 Average loss: 0.000321 
Epoch: 307 [0/5999 (0%)]	Loss: 0.020902
Epoch: 307 [2560/5999 (43%)]	Loss: 0.122904
Epoch: 307 [5120/5999 (85%)]	Loss: 0.040979
====> Epoch: 307 Average loss: 0.000361 
Epoch: 308 [0/5999 (0%)]	Loss: 0.059074
Epoch: 308 [2560/5999 (43%)]	Loss: 0.023248
Epoch: 308 [5120/5999 (85%)]	Loss: 0.028562
====> Epoch: 308 Average loss: 0.000399 
Epoch: 309 [0/5999 (0%)]	Loss: 0.033479
Epoch: 309 [2560/5999 (43%)]	Loss: 0.053688
Epoch: 309 [5120/5999 (85%)]	Loss: 0.090297
====> Epoch: 309 Average loss: 0.000289 
Epoch: 310 [0/5999 (0%)]	Loss: 0.034911
Epoch: 310 [2560/5999 (43%)]	Loss: 0.032121
Epoch: 310 [5120/5999 (85%)]	Loss: 0.020498
====> Epoch: 310 Average loss: 0.000301 
Epoch: 311 [0/5999 (0%)]	Loss: 0.116616
Epoch: 311 [2560/5999 (43%)]	Loss: 0.079142
Epoch: 311 [5120/5999 (85%)]	Loss: 0.054436
====> Epoch: 311 Average loss: 0.000392 
Epoch: 312 [0/5999 (0%)]	Loss: 0.023941
Epoch: 312 [2560/5999 (43%)]	Loss: 0.025057
Epoch: 312 [5120/5999 (85%)]	Loss: 0.022545
====> Epoch: 312 Average loss: 0.000277 
Epoch: 313 [0/5999 (0%)]	Loss: 0.021721
Epoch: 313 [2560/5999 (43%)]	Loss: 0.039389
Epoch: 313 [5120/5999 (85%)]	Loss: 0.052900
====> Epoch: 313 Average loss: 0.000324 
Epoch: 314 [0/5999 (0%)]	Loss: 0.018951
Epoch: 314 [2560/5999 (43%)]	Loss: 0.028912
Epoch: 314 [5120/5999 (85%)]	Loss: 0.062860
====> Epoch: 314 Average loss: 0.000287 
Epoch: 315 [0/5999 (0%)]	Loss: 0.055297
Epoch: 315 [2560/5999 (43%)]	Loss: 0.021415
Epoch: 315 [5120/5999 (85%)]	Loss: 0.057204
====> Epoch: 315 Average loss: 0.000289 
Epoch: 316 [0/5999 (0%)]	Loss: 0.036403
Epoch: 316 [2560/5999 (43%)]	Loss: 0.015088
Epoch: 316 [5120/5999 (85%)]	Loss: 0.014034
====> Epoch: 316 Average loss: 0.000263 
Epoch: 317 [0/5999 (0%)]	Loss: 0.061557
Epoch: 317 [2560/5999 (43%)]	Loss: 0.046771
Epoch: 317 [5120/5999 (85%)]	Loss: 0.051137
====> Epoch: 317 Average loss: 0.000496 
Epoch: 318 [0/5999 (0%)]	Loss: 0.075390
Epoch: 318 [2560/5999 (43%)]	Loss: 0.015898
saving model at:318,8.77259987173602e-05
Epoch: 318 [5120/5999 (85%)]	Loss: 0.026599
====> Epoch: 318 Average loss: 0.000339 
Epoch: 319 [0/5999 (0%)]	Loss: 0.051689
Epoch: 319 [2560/5999 (43%)]	Loss: 0.025721
Epoch: 319 [5120/5999 (85%)]	Loss: 0.070522
====> Epoch: 319 Average loss: 0.000322 
Epoch: 320 [0/5999 (0%)]	Loss: 0.032100
Epoch: 320 [2560/5999 (43%)]	Loss: 0.021894
Epoch: 320 [5120/5999 (85%)]	Loss: 0.044656
====> Epoch: 320 Average loss: 0.000322 
Epoch: 321 [0/5999 (0%)]	Loss: 0.046975
Epoch: 321 [2560/5999 (43%)]	Loss: 0.024976
Epoch: 321 [5120/5999 (85%)]	Loss: 0.026710
====> Epoch: 321 Average loss: 0.000347 
Epoch: 322 [0/5999 (0%)]	Loss: 0.020171
Epoch: 322 [2560/5999 (43%)]	Loss: 0.033233
Epoch: 322 [5120/5999 (85%)]	Loss: 0.023051
====> Epoch: 322 Average loss: 0.000356 
Epoch: 323 [0/5999 (0%)]	Loss: 0.050920
Epoch: 323 [2560/5999 (43%)]	Loss: 0.033946
Epoch: 323 [5120/5999 (85%)]	Loss: 0.063137
====> Epoch: 323 Average loss: 0.000286 
Epoch: 324 [0/5999 (0%)]	Loss: 0.016548
Epoch: 324 [2560/5999 (43%)]	Loss: 0.040160
Epoch: 324 [5120/5999 (85%)]	Loss: 0.023432
====> Epoch: 324 Average loss: 0.000358 
Epoch: 325 [0/5999 (0%)]	Loss: 0.043701
Epoch: 325 [2560/5999 (43%)]	Loss: 0.015885
Epoch: 325 [5120/5999 (85%)]	Loss: 0.038327
====> Epoch: 325 Average loss: 0.000423 
Epoch: 326 [0/5999 (0%)]	Loss: 0.030315
Epoch: 326 [2560/5999 (43%)]	Loss: 0.086467
Epoch: 326 [5120/5999 (85%)]	Loss: 0.083188
====> Epoch: 326 Average loss: 0.000573 
Epoch: 327 [0/5999 (0%)]	Loss: 0.048851
Epoch: 327 [2560/5999 (43%)]	Loss: 0.044076
Epoch: 327 [5120/5999 (85%)]	Loss: 0.024563
====> Epoch: 327 Average loss: 0.000330 
Epoch: 328 [0/5999 (0%)]	Loss: 0.018076
Epoch: 328 [2560/5999 (43%)]	Loss: 0.019535
Epoch: 328 [5120/5999 (85%)]	Loss: 0.038005
====> Epoch: 328 Average loss: 0.000343 
Epoch: 329 [0/5999 (0%)]	Loss: 0.153185
Epoch: 329 [2560/5999 (43%)]	Loss: 0.082052
Epoch: 329 [5120/5999 (85%)]	Loss: 0.035101
====> Epoch: 329 Average loss: 0.000555 
Epoch: 330 [0/5999 (0%)]	Loss: 0.037364
Epoch: 330 [2560/5999 (43%)]	Loss: 0.019309
Epoch: 330 [5120/5999 (85%)]	Loss: 0.099285
====> Epoch: 330 Average loss: 0.000393 
Epoch: 331 [0/5999 (0%)]	Loss: 0.036628
Epoch: 331 [2560/5999 (43%)]	Loss: 0.033165
Epoch: 331 [5120/5999 (85%)]	Loss: 0.116033
====> Epoch: 331 Average loss: 0.000276 
Epoch: 332 [0/5999 (0%)]	Loss: 0.024352
Epoch: 332 [2560/5999 (43%)]	Loss: 0.017546
Epoch: 332 [5120/5999 (85%)]	Loss: 0.099040
====> Epoch: 332 Average loss: 0.000441 
Epoch: 333 [0/5999 (0%)]	Loss: 0.091940
Epoch: 333 [2560/5999 (43%)]	Loss: 0.032569
Epoch: 333 [5120/5999 (85%)]	Loss: 0.061575
====> Epoch: 333 Average loss: 0.000329 
Epoch: 334 [0/5999 (0%)]	Loss: 0.127139
Epoch: 334 [2560/5999 (43%)]	Loss: 0.022364
Epoch: 334 [5120/5999 (85%)]	Loss: 0.095485
====> Epoch: 334 Average loss: 0.000345 
Epoch: 335 [0/5999 (0%)]	Loss: 0.123458
Epoch: 335 [2560/5999 (43%)]	Loss: 0.075907
Epoch: 335 [5120/5999 (85%)]	Loss: 0.052031
====> Epoch: 335 Average loss: 0.000618 
Epoch: 336 [0/5999 (0%)]	Loss: 0.033460
Epoch: 336 [2560/5999 (43%)]	Loss: 0.036062
Epoch: 336 [5120/5999 (85%)]	Loss: 0.017839
====> Epoch: 336 Average loss: 0.000390 
Epoch: 337 [0/5999 (0%)]	Loss: 0.047097
Epoch: 337 [2560/5999 (43%)]	Loss: 0.038819
Epoch: 337 [5120/5999 (85%)]	Loss: 0.059317
====> Epoch: 337 Average loss: 0.000299 
Epoch: 338 [0/5999 (0%)]	Loss: 0.030341
Epoch: 338 [2560/5999 (43%)]	Loss: 0.021645
Epoch: 338 [5120/5999 (85%)]	Loss: 0.050904
====> Epoch: 338 Average loss: 0.000321 
Epoch: 339 [0/5999 (0%)]	Loss: 0.053082
Epoch: 339 [2560/5999 (43%)]	Loss: 0.018848
Epoch: 339 [5120/5999 (85%)]	Loss: 0.051437
====> Epoch: 339 Average loss: 0.000401 
Epoch: 340 [0/5999 (0%)]	Loss: 0.060472
Epoch: 340 [2560/5999 (43%)]	Loss: 0.033675
Epoch: 340 [5120/5999 (85%)]	Loss: 0.033465
====> Epoch: 340 Average loss: 0.000338 
Epoch: 341 [0/5999 (0%)]	Loss: 0.049227
Epoch: 341 [2560/5999 (43%)]	Loss: 0.043788
Epoch: 341 [5120/5999 (85%)]	Loss: 0.027362
====> Epoch: 341 Average loss: 0.000274 
Epoch: 342 [0/5999 (0%)]	Loss: 0.114398
Epoch: 342 [2560/5999 (43%)]	Loss: 0.017622
Epoch: 342 [5120/5999 (85%)]	Loss: 0.019295
====> Epoch: 342 Average loss: 0.000275 
Epoch: 343 [0/5999 (0%)]	Loss: 0.055891
Epoch: 343 [2560/5999 (43%)]	Loss: 0.035814
Epoch: 343 [5120/5999 (85%)]	Loss: 0.030181
====> Epoch: 343 Average loss: 0.000471 
Epoch: 344 [0/5999 (0%)]	Loss: 0.029364
Epoch: 344 [2560/5999 (43%)]	Loss: 0.026868
Epoch: 344 [5120/5999 (85%)]	Loss: 0.015719
saving model at:344,8.545257418882102e-05
====> Epoch: 344 Average loss: 0.000275 
Epoch: 345 [0/5999 (0%)]	Loss: 0.019551
Epoch: 345 [2560/5999 (43%)]	Loss: 0.023187
Epoch: 345 [5120/5999 (85%)]	Loss: 0.055734
====> Epoch: 345 Average loss: 0.000282 
Epoch: 346 [0/5999 (0%)]	Loss: 0.081067
Epoch: 346 [2560/5999 (43%)]	Loss: 0.086081
Epoch: 346 [5120/5999 (85%)]	Loss: 0.063843
====> Epoch: 346 Average loss: 0.000314 
Epoch: 347 [0/5999 (0%)]	Loss: 0.031367
Epoch: 347 [2560/5999 (43%)]	Loss: 0.197083
Epoch: 347 [5120/5999 (85%)]	Loss: 0.039024
====> Epoch: 347 Average loss: 0.000420 
Epoch: 348 [0/5999 (0%)]	Loss: 0.035847
Epoch: 348 [2560/5999 (43%)]	Loss: 0.029617
Epoch: 348 [5120/5999 (85%)]	Loss: 0.024254
====> Epoch: 348 Average loss: 0.000387 
Epoch: 349 [0/5999 (0%)]	Loss: 0.025297
Epoch: 349 [2560/5999 (43%)]	Loss: 0.068931
Epoch: 349 [5120/5999 (85%)]	Loss: 0.037110
====> Epoch: 349 Average loss: 0.000287 
Epoch: 350 [0/5999 (0%)]	Loss: 0.018195
Epoch: 350 [2560/5999 (43%)]	Loss: 0.020676
Epoch: 350 [5120/5999 (85%)]	Loss: 0.039032
====> Epoch: 350 Average loss: 0.000371 
Epoch: 351 [0/5999 (0%)]	Loss: 0.105143
Epoch: 351 [2560/5999 (43%)]	Loss: 0.017741
Epoch: 351 [5120/5999 (85%)]	Loss: 0.039795
====> Epoch: 351 Average loss: 0.000268 
Epoch: 352 [0/5999 (0%)]	Loss: 0.016255
Epoch: 352 [2560/5999 (43%)]	Loss: 0.026070
Epoch: 352 [5120/5999 (85%)]	Loss: 0.081245
====> Epoch: 352 Average loss: 0.000355 
Epoch: 353 [0/5999 (0%)]	Loss: 0.027198
Epoch: 353 [2560/5999 (43%)]	Loss: 0.019582
Epoch: 353 [5120/5999 (85%)]	Loss: 0.031697
====> Epoch: 353 Average loss: 0.000295 
Epoch: 354 [0/5999 (0%)]	Loss: 0.043934
Epoch: 354 [2560/5999 (43%)]	Loss: 0.032335
Epoch: 354 [5120/5999 (85%)]	Loss: 0.018184
====> Epoch: 354 Average loss: 0.000308 
Epoch: 355 [0/5999 (0%)]	Loss: 0.083660
Epoch: 355 [2560/5999 (43%)]	Loss: 0.061783
Epoch: 355 [5120/5999 (85%)]	Loss: 0.121767
====> Epoch: 355 Average loss: 0.000350 
Epoch: 356 [0/5999 (0%)]	Loss: 0.039907
Epoch: 356 [2560/5999 (43%)]	Loss: 0.062914
Epoch: 356 [5120/5999 (85%)]	Loss: 0.025668
====> Epoch: 356 Average loss: 0.000368 
Epoch: 357 [0/5999 (0%)]	Loss: 0.018373
Epoch: 357 [2560/5999 (43%)]	Loss: 0.034970
Epoch: 357 [5120/5999 (85%)]	Loss: 0.088881
====> Epoch: 357 Average loss: 0.000400 
Epoch: 358 [0/5999 (0%)]	Loss: 0.031761
Epoch: 358 [2560/5999 (43%)]	Loss: 0.044912
Epoch: 358 [5120/5999 (85%)]	Loss: 0.016048
====> Epoch: 358 Average loss: 0.000249 
Epoch: 359 [0/5999 (0%)]	Loss: 0.044822
Epoch: 359 [2560/5999 (43%)]	Loss: 0.023454
Epoch: 359 [5120/5999 (85%)]	Loss: 0.026471
====> Epoch: 359 Average loss: 0.000265 
Epoch: 360 [0/5999 (0%)]	Loss: 0.064884
Epoch: 360 [2560/5999 (43%)]	Loss: 0.108764
Epoch: 360 [5120/5999 (85%)]	Loss: 0.058069
====> Epoch: 360 Average loss: 0.000413 
Epoch: 361 [0/5999 (0%)]	Loss: 0.031056
Epoch: 361 [2560/5999 (43%)]	Loss: 0.075887
Epoch: 361 [5120/5999 (85%)]	Loss: 0.079038
====> Epoch: 361 Average loss: 0.000307 
Epoch: 362 [0/5999 (0%)]	Loss: 0.122257
Epoch: 362 [2560/5999 (43%)]	Loss: 0.032133
Epoch: 362 [5120/5999 (85%)]	Loss: 0.106148
====> Epoch: 362 Average loss: 0.000411 
Epoch: 363 [0/5999 (0%)]	Loss: 0.030022
Epoch: 363 [2560/5999 (43%)]	Loss: 0.024884
Epoch: 363 [5120/5999 (85%)]	Loss: 0.017595
====> Epoch: 363 Average loss: 0.000327 
Epoch: 364 [0/5999 (0%)]	Loss: 0.021971
Epoch: 364 [2560/5999 (43%)]	Loss: 0.018181
saving model at:364,8.532733749598265e-05
Epoch: 364 [5120/5999 (85%)]	Loss: 0.027232
====> Epoch: 364 Average loss: 0.000294 
Epoch: 365 [0/5999 (0%)]	Loss: 0.081153
Epoch: 365 [2560/5999 (43%)]	Loss: 0.021589
Epoch: 365 [5120/5999 (85%)]	Loss: 0.015649
====> Epoch: 365 Average loss: 0.000323 
Epoch: 366 [0/5999 (0%)]	Loss: 0.032368
Epoch: 366 [2560/5999 (43%)]	Loss: 0.017060
Epoch: 366 [5120/5999 (85%)]	Loss: 0.018523
====> Epoch: 366 Average loss: 0.000265 
Epoch: 367 [0/5999 (0%)]	Loss: 0.046057
Epoch: 367 [2560/5999 (43%)]	Loss: 0.029369
Epoch: 367 [5120/5999 (85%)]	Loss: 0.029992
====> Epoch: 367 Average loss: 0.000380 
Epoch: 368 [0/5999 (0%)]	Loss: 0.173057
Epoch: 368 [2560/5999 (43%)]	Loss: 0.083997
Epoch: 368 [5120/5999 (85%)]	Loss: 0.074669
====> Epoch: 368 Average loss: 0.000513 
Epoch: 369 [0/5999 (0%)]	Loss: 0.028413
Epoch: 369 [2560/5999 (43%)]	Loss: 0.029136
Epoch: 369 [5120/5999 (85%)]	Loss: 0.019365
====> Epoch: 369 Average loss: 0.000301 
Epoch: 370 [0/5999 (0%)]	Loss: 0.025078
Epoch: 370 [2560/5999 (43%)]	Loss: 0.053893
Epoch: 370 [5120/5999 (85%)]	Loss: 0.061542
====> Epoch: 370 Average loss: 0.000366 
Epoch: 371 [0/5999 (0%)]	Loss: 0.060905
Epoch: 371 [2560/5999 (43%)]	Loss: 0.096352
Epoch: 371 [5120/5999 (85%)]	Loss: 0.105668
====> Epoch: 371 Average loss: 0.000549 
Epoch: 372 [0/5999 (0%)]	Loss: 0.050781
Epoch: 372 [2560/5999 (43%)]	Loss: 0.025586
Epoch: 372 [5120/5999 (85%)]	Loss: 0.016530
====> Epoch: 372 Average loss: 0.000300 
Epoch: 373 [0/5999 (0%)]	Loss: 0.028897
Epoch: 373 [2560/5999 (43%)]	Loss: 0.025554
Epoch: 373 [5120/5999 (85%)]	Loss: 0.027486
====> Epoch: 373 Average loss: 0.000294 
Epoch: 374 [0/5999 (0%)]	Loss: 0.018958
Epoch: 374 [2560/5999 (43%)]	Loss: 0.037097
Epoch: 374 [5120/5999 (85%)]	Loss: 0.049763
====> Epoch: 374 Average loss: 0.000448 
Epoch: 375 [0/5999 (0%)]	Loss: 0.094306
Epoch: 375 [2560/5999 (43%)]	Loss: 0.028876
Epoch: 375 [5120/5999 (85%)]	Loss: 0.014655
====> Epoch: 375 Average loss: 0.000307 
Epoch: 376 [0/5999 (0%)]	Loss: 0.032461
Epoch: 376 [2560/5999 (43%)]	Loss: 0.022523
Epoch: 376 [5120/5999 (85%)]	Loss: 0.131850
====> Epoch: 376 Average loss: 0.000250 
Epoch: 377 [0/5999 (0%)]	Loss: 0.024531
Epoch: 377 [2560/5999 (43%)]	Loss: 0.029247
Epoch: 377 [5120/5999 (85%)]	Loss: 0.025230
====> Epoch: 377 Average loss: 0.000329 
Epoch: 378 [0/5999 (0%)]	Loss: 0.094839
Epoch: 378 [2560/5999 (43%)]	Loss: 0.025125
Epoch: 378 [5120/5999 (85%)]	Loss: 0.076209
====> Epoch: 378 Average loss: 0.000310 
Epoch: 379 [0/5999 (0%)]	Loss: 0.029136
Epoch: 379 [2560/5999 (43%)]	Loss: 0.030817
Epoch: 379 [5120/5999 (85%)]	Loss: 0.043107
====> Epoch: 379 Average loss: 0.000265 
Epoch: 380 [0/5999 (0%)]	Loss: 0.023440
Epoch: 380 [2560/5999 (43%)]	Loss: 0.016103
Epoch: 380 [5120/5999 (85%)]	Loss: 0.042851
====> Epoch: 380 Average loss: 0.000337 
Epoch: 381 [0/5999 (0%)]	Loss: 0.077883
Epoch: 381 [2560/5999 (43%)]	Loss: 0.064200
Epoch: 381 [5120/5999 (85%)]	Loss: 0.023364
====> Epoch: 381 Average loss: 0.000363 
Epoch: 382 [0/5999 (0%)]	Loss: 0.042180
Epoch: 382 [2560/5999 (43%)]	Loss: 0.025959
Epoch: 382 [5120/5999 (85%)]	Loss: 0.036319
====> Epoch: 382 Average loss: 0.000306 
Epoch: 383 [0/5999 (0%)]	Loss: 0.031947
Epoch: 383 [2560/5999 (43%)]	Loss: 0.185207
Epoch: 383 [5120/5999 (85%)]	Loss: 0.033155
====> Epoch: 383 Average loss: 0.000330 
Epoch: 384 [0/5999 (0%)]	Loss: 0.055673
Epoch: 384 [2560/5999 (43%)]	Loss: 0.046353
Epoch: 384 [5120/5999 (85%)]	Loss: 0.045273
====> Epoch: 384 Average loss: 0.000371 
Epoch: 385 [0/5999 (0%)]	Loss: 0.023200
Epoch: 385 [2560/5999 (43%)]	Loss: 0.033264
Epoch: 385 [5120/5999 (85%)]	Loss: 0.061218
====> Epoch: 385 Average loss: 0.000275 
Epoch: 386 [0/5999 (0%)]	Loss: 0.022857
Epoch: 386 [2560/5999 (43%)]	Loss: 0.038215
Epoch: 386 [5120/5999 (85%)]	Loss: 0.015120
====> Epoch: 386 Average loss: 0.000315 
Epoch: 387 [0/5999 (0%)]	Loss: 0.022519
Epoch: 387 [2560/5999 (43%)]	Loss: 0.033225
Epoch: 387 [5120/5999 (85%)]	Loss: 0.034067
====> Epoch: 387 Average loss: 0.000432 
Epoch: 388 [0/5999 (0%)]	Loss: 0.061641
Epoch: 388 [2560/5999 (43%)]	Loss: 0.020747
Epoch: 388 [5120/5999 (85%)]	Loss: 0.046216
====> Epoch: 388 Average loss: 0.000353 
Epoch: 389 [0/5999 (0%)]	Loss: 0.067103
Epoch: 389 [2560/5999 (43%)]	Loss: 0.025732
Epoch: 389 [5120/5999 (85%)]	Loss: 0.019558
====> Epoch: 389 Average loss: 0.000323 
Epoch: 390 [0/5999 (0%)]	Loss: 0.014320
Epoch: 390 [2560/5999 (43%)]	Loss: 0.095273
Epoch: 390 [5120/5999 (85%)]	Loss: 0.030794
====> Epoch: 390 Average loss: 0.000333 
Epoch: 391 [0/5999 (0%)]	Loss: 0.016498
Epoch: 391 [2560/5999 (43%)]	Loss: 0.033027
Epoch: 391 [5120/5999 (85%)]	Loss: 0.045432
====> Epoch: 391 Average loss: 0.000349 
Epoch: 392 [0/5999 (0%)]	Loss: 0.065597
Epoch: 392 [2560/5999 (43%)]	Loss: 0.027919
Epoch: 392 [5120/5999 (85%)]	Loss: 0.034058
====> Epoch: 392 Average loss: 0.000298 
Epoch: 393 [0/5999 (0%)]	Loss: 0.031547
Epoch: 393 [2560/5999 (43%)]	Loss: 0.011741
Epoch: 393 [5120/5999 (85%)]	Loss: 0.085192
====> Epoch: 393 Average loss: 0.000281 
Epoch: 394 [0/5999 (0%)]	Loss: 0.034697
Epoch: 394 [2560/5999 (43%)]	Loss: 0.021777
Epoch: 394 [5120/5999 (85%)]	Loss: 0.034335
====> Epoch: 394 Average loss: 0.000352 
Epoch: 395 [0/5999 (0%)]	Loss: 0.021009
Epoch: 395 [2560/5999 (43%)]	Loss: 0.042995
Epoch: 395 [5120/5999 (85%)]	Loss: 0.039099
====> Epoch: 395 Average loss: 0.000245 
Epoch: 396 [0/5999 (0%)]	Loss: 0.038808
Epoch: 396 [2560/5999 (43%)]	Loss: 0.017685
Epoch: 396 [5120/5999 (85%)]	Loss: 0.017718
====> Epoch: 396 Average loss: 0.000276 
Epoch: 397 [0/5999 (0%)]	Loss: 0.025017
Epoch: 397 [2560/5999 (43%)]	Loss: 0.048412
Epoch: 397 [5120/5999 (85%)]	Loss: 0.019941
====> Epoch: 397 Average loss: 0.000299 
Epoch: 398 [0/5999 (0%)]	Loss: 0.074589
Epoch: 398 [2560/5999 (43%)]	Loss: 0.024609
Epoch: 398 [5120/5999 (85%)]	Loss: 0.019428
====> Epoch: 398 Average loss: 0.000317 
Epoch: 399 [0/5999 (0%)]	Loss: 0.025160
Epoch: 399 [2560/5999 (43%)]	Loss: 0.035907
Epoch: 399 [5120/5999 (85%)]	Loss: 0.027008
====> Epoch: 399 Average loss: 0.000314 
Epoch: 400 [0/5999 (0%)]	Loss: 0.059714
Epoch: 400 [2560/5999 (43%)]	Loss: 0.020396
Epoch: 400 [5120/5999 (85%)]	Loss: 0.028521
====> Epoch: 400 Average loss: 0.000273 
Reconstruction Loss 8.532733586616815e-05
per_obj_mse: ['8.94725089892745e-05', '8.118218102026731e-05']
Reconstruction Loss 8.573055682795168e-05
per_obj_mse: ['9.427622717339545e-05', '7.718490087427199e-05']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_3_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_3_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_3_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_3_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0005560114323161543
per_obj_mse: ['0.0002724335063248873', '0.0008395893964916468']
Reconstruction Loss 0.0005491745371024184
per_obj_mse: ['0.00026124005671590567', '0.0008371089934371412']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_3_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_3_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0005560114323161543
per_obj_mse: ['0.0002724335063248873', '0.0008395893964916468']
Reconstruction Loss 0.0005491745371024184
per_obj_mse: ['0.00026124005671590567', '0.0008371089934371412']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_3_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=20, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_3_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7997, 2, 11)
(1997, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0025652714539319277
per_obj_mse: ['0.0016644119750708342', '0.003466131165623665']
Reconstruction Loss 0.002575534173186251
per_obj_mse: ['0.001675293198786676', '0.003475775010883808']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_3_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=60, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_3_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7993, 2, 11)
(1993, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.00792210095591392
per_obj_mse: ['0.005377964116632938', '0.010466239415109158']
Reconstruction Loss 0.007996578958788322
per_obj_mse: ['0.005433522164821625', '0.010559638030827045']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_3_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=100, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_3_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7989, 2, 11)
(1989, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.013294282705859618
per_obj_mse: ['0.009121834300458431', '0.017466725781559944']
Reconstruction Loss 0.013423082934736517
per_obj_mse: ['0.009194282814860344', '0.017651883885264397']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_3_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=140, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_3_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7985, 2, 11)
(1985, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.018627581775606306
per_obj_mse: ['0.012816647998988628', '0.024438519030809402']
Reconstruction Loss 0.018859073964489496
per_obj_mse: ['0.01295872125774622', '0.024759432300925255']
