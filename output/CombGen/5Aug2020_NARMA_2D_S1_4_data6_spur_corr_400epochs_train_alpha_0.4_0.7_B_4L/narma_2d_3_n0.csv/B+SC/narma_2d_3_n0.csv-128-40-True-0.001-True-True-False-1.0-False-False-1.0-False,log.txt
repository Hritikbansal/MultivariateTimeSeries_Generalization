cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_3_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=400, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_3_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Epoch: 1 [0/5999 (0%)]	Loss: 75.579338
Epoch: 1 [2560/5999 (43%)]	Loss: 11.797552
saving model at:1,0.08529433214664459
Epoch: 1 [5120/5999 (85%)]	Loss: 8.331736
saving model at:1,0.0629645602107048
====> Epoch: 1 Average loss: 0.116834 
Epoch: 2 [0/5999 (0%)]	Loss: 7.720952
Epoch: 2 [2560/5999 (43%)]	Loss: 8.065459
saving model at:2,0.05725754886865616
Epoch: 2 [5120/5999 (85%)]	Loss: 7.331690
saving model at:2,0.049504545509815215
====> Epoch: 2 Average loss: 0.057878 
Epoch: 3 [0/5999 (0%)]	Loss: 5.676482
Epoch: 3 [2560/5999 (43%)]	Loss: 3.729366
saving model at:3,0.02784142203629017
Epoch: 3 [5120/5999 (85%)]	Loss: 2.287192
saving model at:3,0.018881980359554292
====> Epoch: 3 Average loss: 0.029491 
Epoch: 4 [0/5999 (0%)]	Loss: 2.435534
Epoch: 4 [2560/5999 (43%)]	Loss: 1.809156
saving model at:4,0.01320232444256544
Epoch: 4 [5120/5999 (85%)]	Loss: 1.390978
saving model at:4,0.0109401411190629
====> Epoch: 4 Average loss: 0.013904 
Epoch: 5 [0/5999 (0%)]	Loss: 1.413075
Epoch: 5 [2560/5999 (43%)]	Loss: 1.185120
saving model at:5,0.009003944382071496
Epoch: 5 [5120/5999 (85%)]	Loss: 1.036295
saving model at:5,0.008175465650856495
====> Epoch: 5 Average loss: 0.009395 
Epoch: 6 [0/5999 (0%)]	Loss: 1.025165
Epoch: 6 [2560/5999 (43%)]	Loss: 1.059169
saving model at:6,0.007576173275709152
Epoch: 6 [5120/5999 (85%)]	Loss: 1.058125
====> Epoch: 6 Average loss: 0.008383 
Epoch: 7 [0/5999 (0%)]	Loss: 0.845140
Epoch: 7 [2560/5999 (43%)]	Loss: 0.773344
saving model at:7,0.00573756168037653
Epoch: 7 [5120/5999 (85%)]	Loss: 0.742261
====> Epoch: 7 Average loss: 0.006350 
Epoch: 8 [0/5999 (0%)]	Loss: 0.696338
Epoch: 8 [2560/5999 (43%)]	Loss: 0.627117
saving model at:8,0.004469789311289787
Epoch: 8 [5120/5999 (85%)]	Loss: 0.528493
saving model at:8,0.004017515243962407
====> Epoch: 8 Average loss: 0.004787 
Epoch: 9 [0/5999 (0%)]	Loss: 0.576988
Epoch: 9 [2560/5999 (43%)]	Loss: 0.623692
saving model at:9,0.0037046465128660203
Epoch: 9 [5120/5999 (85%)]	Loss: 0.464023
saving model at:9,0.0032363288328051566
====> Epoch: 9 Average loss: 0.004005 
Epoch: 10 [0/5999 (0%)]	Loss: 0.437265
Epoch: 10 [2560/5999 (43%)]	Loss: 0.445525
saving model at:10,0.002935548584908247
Epoch: 10 [5120/5999 (85%)]	Loss: 0.379653
====> Epoch: 10 Average loss: 0.003505 
Epoch: 11 [0/5999 (0%)]	Loss: 0.435041
Epoch: 11 [2560/5999 (43%)]	Loss: 0.388045
saving model at:11,0.0025849859658628703
Epoch: 11 [5120/5999 (85%)]	Loss: 0.432329
====> Epoch: 11 Average loss: 0.003158 
Epoch: 12 [0/5999 (0%)]	Loss: 0.303467
Epoch: 12 [2560/5999 (43%)]	Loss: 0.378481
Epoch: 12 [5120/5999 (85%)]	Loss: 0.295243
saving model at:12,0.0025637841168791057
====> Epoch: 12 Average loss: 0.002774 
Epoch: 13 [0/5999 (0%)]	Loss: 0.330713
Epoch: 13 [2560/5999 (43%)]	Loss: 0.290714
saving model at:13,0.002121640034019947
Epoch: 13 [5120/5999 (85%)]	Loss: 0.277830
saving model at:13,0.001916512254625559
====> Epoch: 13 Average loss: 0.002358 
Epoch: 14 [0/5999 (0%)]	Loss: 0.222978
Epoch: 14 [2560/5999 (43%)]	Loss: 0.233836
Epoch: 14 [5120/5999 (85%)]	Loss: 0.249832
saving model at:14,0.0018104185443371535
====> Epoch: 14 Average loss: 0.002233 
Epoch: 15 [0/5999 (0%)]	Loss: 0.495046
Epoch: 15 [2560/5999 (43%)]	Loss: 0.326357
Epoch: 15 [5120/5999 (85%)]	Loss: 0.334176
====> Epoch: 15 Average loss: 0.002596 
Epoch: 16 [0/5999 (0%)]	Loss: 0.258863
Epoch: 16 [2560/5999 (43%)]	Loss: 0.266349
Epoch: 16 [5120/5999 (85%)]	Loss: 0.222774
saving model at:16,0.0015419860295951366
====> Epoch: 16 Average loss: 0.002678 
Epoch: 17 [0/5999 (0%)]	Loss: 0.220032
Epoch: 17 [2560/5999 (43%)]	Loss: 0.207592
saving model at:17,0.0014210146060213446
Epoch: 17 [5120/5999 (85%)]	Loss: 0.211910
====> Epoch: 17 Average loss: 0.001763 
Epoch: 18 [0/5999 (0%)]	Loss: 0.283684
Epoch: 18 [2560/5999 (43%)]	Loss: 0.208887
Epoch: 18 [5120/5999 (85%)]	Loss: 0.212328
====> Epoch: 18 Average loss: 0.001815 
Epoch: 19 [0/5999 (0%)]	Loss: 0.160823
Epoch: 19 [2560/5999 (43%)]	Loss: 0.288707
Epoch: 19 [5120/5999 (85%)]	Loss: 0.169178
====> Epoch: 19 Average loss: 0.001814 
Epoch: 20 [0/5999 (0%)]	Loss: 0.219284
Epoch: 20 [2560/5999 (43%)]	Loss: 0.165289
Epoch: 20 [5120/5999 (85%)]	Loss: 0.246726
====> Epoch: 20 Average loss: 0.001823 
Epoch: 21 [0/5999 (0%)]	Loss: 0.209264
Epoch: 21 [2560/5999 (43%)]	Loss: 0.199482
saving model at:21,0.0012897611446678638
Epoch: 21 [5120/5999 (85%)]	Loss: 0.204652
====> Epoch: 21 Average loss: 0.001852 
Epoch: 22 [0/5999 (0%)]	Loss: 0.190497
Epoch: 22 [2560/5999 (43%)]	Loss: 0.265732
saving model at:22,0.0010927268508821727
Epoch: 22 [5120/5999 (85%)]	Loss: 0.521673
====> Epoch: 22 Average loss: 0.001591 
Epoch: 23 [0/5999 (0%)]	Loss: 0.139622
Epoch: 23 [2560/5999 (43%)]	Loss: 0.115270
saving model at:23,0.0010464735259301961
Epoch: 23 [5120/5999 (85%)]	Loss: 0.152452
saving model at:23,0.0009207035191357136
====> Epoch: 23 Average loss: 0.001217 
Epoch: 24 [0/5999 (0%)]	Loss: 0.159739
Epoch: 24 [2560/5999 (43%)]	Loss: 0.131510
Epoch: 24 [5120/5999 (85%)]	Loss: 0.147314
====> Epoch: 24 Average loss: 0.001241 
Epoch: 25 [0/5999 (0%)]	Loss: 0.229296
Epoch: 25 [2560/5999 (43%)]	Loss: 0.177119
Epoch: 25 [5120/5999 (85%)]	Loss: 0.151147
saving model at:25,0.0008952993573620915
====> Epoch: 25 Average loss: 0.001471 
Epoch: 26 [0/5999 (0%)]	Loss: 0.221403
Epoch: 26 [2560/5999 (43%)]	Loss: 0.224679
Epoch: 26 [5120/5999 (85%)]	Loss: 0.116608
====> Epoch: 26 Average loss: 0.001463 
Epoch: 27 [0/5999 (0%)]	Loss: 0.137769
Epoch: 27 [2560/5999 (43%)]	Loss: 0.128309
Epoch: 27 [5120/5999 (85%)]	Loss: 0.547323
====> Epoch: 27 Average loss: 0.001621 
Epoch: 28 [0/5999 (0%)]	Loss: 0.119480
Epoch: 28 [2560/5999 (43%)]	Loss: 0.285859
Epoch: 28 [5120/5999 (85%)]	Loss: 0.340274
====> Epoch: 28 Average loss: 0.001559 
Epoch: 29 [0/5999 (0%)]	Loss: 0.254177
Epoch: 29 [2560/5999 (43%)]	Loss: 0.128694
Epoch: 29 [5120/5999 (85%)]	Loss: 0.253448
====> Epoch: 29 Average loss: 0.001213 
Epoch: 30 [0/5999 (0%)]	Loss: 0.191590
Epoch: 30 [2560/5999 (43%)]	Loss: 0.097814
saving model at:30,0.0008684091651812196
Epoch: 30 [5120/5999 (85%)]	Loss: 0.247589
====> Epoch: 30 Average loss: 0.001305 
Epoch: 31 [0/5999 (0%)]	Loss: 0.269931
Epoch: 31 [2560/5999 (43%)]	Loss: 0.136759
Epoch: 31 [5120/5999 (85%)]	Loss: 0.106080
saving model at:31,0.0007617454631254077
====> Epoch: 31 Average loss: 0.001289 
Epoch: 32 [0/5999 (0%)]	Loss: 0.127761
Epoch: 32 [2560/5999 (43%)]	Loss: 0.100406
saving model at:32,0.0006791441887617111
Epoch: 32 [5120/5999 (85%)]	Loss: 0.102917
saving model at:32,0.000646616647951305
====> Epoch: 32 Average loss: 0.001022 
Epoch: 33 [0/5999 (0%)]	Loss: 0.082692
Epoch: 33 [2560/5999 (43%)]	Loss: 0.099109
Epoch: 33 [5120/5999 (85%)]	Loss: 0.092006
====> Epoch: 33 Average loss: 0.001080 
Epoch: 34 [0/5999 (0%)]	Loss: 0.094550
Epoch: 34 [2560/5999 (43%)]	Loss: 0.296097
Epoch: 34 [5120/5999 (85%)]	Loss: 0.119672
saving model at:34,0.000643768493551761
====> Epoch: 34 Average loss: 0.000962 
Epoch: 35 [0/5999 (0%)]	Loss: 0.199581
Epoch: 35 [2560/5999 (43%)]	Loss: 0.099668
Epoch: 35 [5120/5999 (85%)]	Loss: 0.083603
saving model at:35,0.0006172121698036789
====> Epoch: 35 Average loss: 0.001266 
Epoch: 36 [0/5999 (0%)]	Loss: 0.268023
Epoch: 36 [2560/5999 (43%)]	Loss: 0.092351
Epoch: 36 [5120/5999 (85%)]	Loss: 0.204973
====> Epoch: 36 Average loss: 0.001247 
Epoch: 37 [0/5999 (0%)]	Loss: 0.089872
Epoch: 37 [2560/5999 (43%)]	Loss: 0.096258
saving model at:37,0.0005996725773438811
Epoch: 37 [5120/5999 (85%)]	Loss: 0.211277
====> Epoch: 37 Average loss: 0.001041 
Epoch: 38 [0/5999 (0%)]	Loss: 0.124176
Epoch: 38 [2560/5999 (43%)]	Loss: 0.151807
Epoch: 38 [5120/5999 (85%)]	Loss: 0.126282
====> Epoch: 38 Average loss: 0.001215 
Epoch: 39 [0/5999 (0%)]	Loss: 0.092308
Epoch: 39 [2560/5999 (43%)]	Loss: 0.098714
saving model at:39,0.0005524319149553776
Epoch: 39 [5120/5999 (85%)]	Loss: 0.186225
====> Epoch: 39 Average loss: 0.000803 
Epoch: 40 [0/5999 (0%)]	Loss: 0.230594
Epoch: 40 [2560/5999 (43%)]	Loss: 0.156968
Epoch: 40 [5120/5999 (85%)]	Loss: 0.091349
saving model at:40,0.0005434595635160804
====> Epoch: 40 Average loss: 0.000872 
Epoch: 41 [0/5999 (0%)]	Loss: 0.091686
Epoch: 41 [2560/5999 (43%)]	Loss: 0.068240
Epoch: 41 [5120/5999 (85%)]	Loss: 0.085043
saving model at:41,0.0004957369705662131
====> Epoch: 41 Average loss: 0.000972 
Epoch: 42 [0/5999 (0%)]	Loss: 0.086032
Epoch: 42 [2560/5999 (43%)]	Loss: 0.069826
saving model at:42,0.00046364186750724913
Epoch: 42 [5120/5999 (85%)]	Loss: 0.064622
====> Epoch: 42 Average loss: 0.000837 
Epoch: 43 [0/5999 (0%)]	Loss: 0.071285
Epoch: 43 [2560/5999 (43%)]	Loss: 0.113224
Epoch: 43 [5120/5999 (85%)]	Loss: 0.145762
====> Epoch: 43 Average loss: 0.000967 
Epoch: 44 [0/5999 (0%)]	Loss: 0.072805
Epoch: 44 [2560/5999 (43%)]	Loss: 0.095774
saving model at:44,0.00046327890641987326
Epoch: 44 [5120/5999 (85%)]	Loss: 0.111205
====> Epoch: 44 Average loss: 0.001036 
Epoch: 45 [0/5999 (0%)]	Loss: 0.120488
Epoch: 45 [2560/5999 (43%)]	Loss: 0.158777
Epoch: 45 [5120/5999 (85%)]	Loss: 0.071362
====> Epoch: 45 Average loss: 0.000944 
Epoch: 46 [0/5999 (0%)]	Loss: 0.113165
Epoch: 46 [2560/5999 (43%)]	Loss: 0.138842
Epoch: 46 [5120/5999 (85%)]	Loss: 0.114587
====> Epoch: 46 Average loss: 0.001019 
Epoch: 47 [0/5999 (0%)]	Loss: 0.163316
Epoch: 47 [2560/5999 (43%)]	Loss: 0.111142
saving model at:47,0.00045676739839836954
Epoch: 47 [5120/5999 (85%)]	Loss: 0.083104
saving model at:47,0.00045321080181747673
====> Epoch: 47 Average loss: 0.000908 
Epoch: 48 [0/5999 (0%)]	Loss: 0.063477
Epoch: 48 [2560/5999 (43%)]	Loss: 0.129228
Epoch: 48 [5120/5999 (85%)]	Loss: 0.109262
====> Epoch: 48 Average loss: 0.001068 
Epoch: 49 [0/5999 (0%)]	Loss: 0.100117
Epoch: 49 [2560/5999 (43%)]	Loss: 0.308872
Epoch: 49 [5120/5999 (85%)]	Loss: 0.197059
====> Epoch: 49 Average loss: 0.000888 
Epoch: 50 [0/5999 (0%)]	Loss: 0.335263
Epoch: 50 [2560/5999 (43%)]	Loss: 0.121613
Epoch: 50 [5120/5999 (85%)]	Loss: 0.086647
====> Epoch: 50 Average loss: 0.001313 
Epoch: 51 [0/5999 (0%)]	Loss: 0.079358
Epoch: 51 [2560/5999 (43%)]	Loss: 0.072958
Epoch: 51 [5120/5999 (85%)]	Loss: 0.157879
====> Epoch: 51 Average loss: 0.000871 
Epoch: 52 [0/5999 (0%)]	Loss: 0.116185
Epoch: 52 [2560/5999 (43%)]	Loss: 0.070410
saving model at:52,0.0004277196384500712
Epoch: 52 [5120/5999 (85%)]	Loss: 0.079438
====> Epoch: 52 Average loss: 0.000842 
Epoch: 53 [0/5999 (0%)]	Loss: 0.061383
Epoch: 53 [2560/5999 (43%)]	Loss: 0.157307
Epoch: 53 [5120/5999 (85%)]	Loss: 0.063726
====> Epoch: 53 Average loss: 0.001049 
Epoch: 54 [0/5999 (0%)]	Loss: 0.076812
Epoch: 54 [2560/5999 (43%)]	Loss: 0.071105
saving model at:54,0.00042578683607280256
Epoch: 54 [5120/5999 (85%)]	Loss: 0.126860
saving model at:54,0.00037082738312892616
====> Epoch: 54 Average loss: 0.000746 
Epoch: 55 [0/5999 (0%)]	Loss: 0.301859
Epoch: 55 [2560/5999 (43%)]	Loss: 0.436928
Epoch: 55 [5120/5999 (85%)]	Loss: 0.294129
====> Epoch: 55 Average loss: 0.001289 
Epoch: 56 [0/5999 (0%)]	Loss: 0.214216
Epoch: 56 [2560/5999 (43%)]	Loss: 0.081400
Epoch: 56 [5120/5999 (85%)]	Loss: 0.092227
====> Epoch: 56 Average loss: 0.000889 
Epoch: 57 [0/5999 (0%)]	Loss: 0.159716
Epoch: 57 [2560/5999 (43%)]	Loss: 0.164995
Epoch: 57 [5120/5999 (85%)]	Loss: 0.231718
====> Epoch: 57 Average loss: 0.000977 
Epoch: 58 [0/5999 (0%)]	Loss: 0.404475
Epoch: 58 [2560/5999 (43%)]	Loss: 0.079103
Epoch: 58 [5120/5999 (85%)]	Loss: 0.067043
====> Epoch: 58 Average loss: 0.000780 
Epoch: 59 [0/5999 (0%)]	Loss: 0.054681
Epoch: 59 [2560/5999 (43%)]	Loss: 0.053160
saving model at:59,0.00034765987982973457
Epoch: 59 [5120/5999 (85%)]	Loss: 0.165602
====> Epoch: 59 Average loss: 0.000813 
Epoch: 60 [0/5999 (0%)]	Loss: 0.052146
Epoch: 60 [2560/5999 (43%)]	Loss: 0.060970
Epoch: 60 [5120/5999 (85%)]	Loss: 0.206753
====> Epoch: 60 Average loss: 0.000818 
Epoch: 61 [0/5999 (0%)]	Loss: 0.127987
Epoch: 61 [2560/5999 (43%)]	Loss: 0.056671
saving model at:61,0.00032672160887159407
Epoch: 61 [5120/5999 (85%)]	Loss: 0.070392
====> Epoch: 61 Average loss: 0.000754 
Epoch: 62 [0/5999 (0%)]	Loss: 0.058867
Epoch: 62 [2560/5999 (43%)]	Loss: 0.053688
Epoch: 62 [5120/5999 (85%)]	Loss: 0.148746
====> Epoch: 62 Average loss: 0.000684 
Epoch: 63 [0/5999 (0%)]	Loss: 0.065848
Epoch: 63 [2560/5999 (43%)]	Loss: 0.048257
Epoch: 63 [5120/5999 (85%)]	Loss: 0.062331
saving model at:63,0.0003132850623223931
====> Epoch: 63 Average loss: 0.000696 
Epoch: 64 [0/5999 (0%)]	Loss: 0.056983
Epoch: 64 [2560/5999 (43%)]	Loss: 0.051805
Epoch: 64 [5120/5999 (85%)]	Loss: 0.153217
====> Epoch: 64 Average loss: 0.000649 
Epoch: 65 [0/5999 (0%)]	Loss: 0.383102
Epoch: 65 [2560/5999 (43%)]	Loss: 0.069012
Epoch: 65 [5120/5999 (85%)]	Loss: 0.055932
====> Epoch: 65 Average loss: 0.000892 
Epoch: 66 [0/5999 (0%)]	Loss: 0.065658
Epoch: 66 [2560/5999 (43%)]	Loss: 0.046319
Epoch: 66 [5120/5999 (85%)]	Loss: 0.128189
====> Epoch: 66 Average loss: 0.000914 
Epoch: 67 [0/5999 (0%)]	Loss: 0.080306
Epoch: 67 [2560/5999 (43%)]	Loss: 0.077765
Epoch: 67 [5120/5999 (85%)]	Loss: 0.125273
====> Epoch: 67 Average loss: 0.000820 
Epoch: 68 [0/5999 (0%)]	Loss: 0.061309
Epoch: 68 [2560/5999 (43%)]	Loss: 0.120209
saving model at:68,0.0003078349612187594
Epoch: 68 [5120/5999 (85%)]	Loss: 0.056669
====> Epoch: 68 Average loss: 0.000738 
Epoch: 69 [0/5999 (0%)]	Loss: 0.042250
Epoch: 69 [2560/5999 (43%)]	Loss: 0.061334
Epoch: 69 [5120/5999 (85%)]	Loss: 0.081017
====> Epoch: 69 Average loss: 0.001074 
Epoch: 70 [0/5999 (0%)]	Loss: 0.374598
Epoch: 70 [2560/5999 (43%)]	Loss: 0.081013
Epoch: 70 [5120/5999 (85%)]	Loss: 0.053209
====> Epoch: 70 Average loss: 0.000907 
Epoch: 71 [0/5999 (0%)]	Loss: 0.076869
Epoch: 71 [2560/5999 (43%)]	Loss: 0.058410
saving model at:71,0.00028430246701464056
Epoch: 71 [5120/5999 (85%)]	Loss: 0.058347
====> Epoch: 71 Average loss: 0.000506 
Epoch: 72 [0/5999 (0%)]	Loss: 0.047178
Epoch: 72 [2560/5999 (43%)]	Loss: 0.117671
Epoch: 72 [5120/5999 (85%)]	Loss: 0.051384
====> Epoch: 72 Average loss: 0.000715 
Epoch: 73 [0/5999 (0%)]	Loss: 0.080102
Epoch: 73 [2560/5999 (43%)]	Loss: 0.103692
Epoch: 73 [5120/5999 (85%)]	Loss: 0.047311
====> Epoch: 73 Average loss: 0.000656 
Epoch: 74 [0/5999 (0%)]	Loss: 0.056146
Epoch: 74 [2560/5999 (43%)]	Loss: 0.208376
Epoch: 74 [5120/5999 (85%)]	Loss: 0.060093
====> Epoch: 74 Average loss: 0.000938 
Epoch: 75 [0/5999 (0%)]	Loss: 0.041604
Epoch: 75 [2560/5999 (43%)]	Loss: 0.045707
saving model at:75,0.00026166597555857154
Epoch: 75 [5120/5999 (85%)]	Loss: 0.046905
====> Epoch: 75 Average loss: 0.000626 
Epoch: 76 [0/5999 (0%)]	Loss: 0.050631
Epoch: 76 [2560/5999 (43%)]	Loss: 0.117190
Epoch: 76 [5120/5999 (85%)]	Loss: 0.112339
====> Epoch: 76 Average loss: 0.000780 
Epoch: 77 [0/5999 (0%)]	Loss: 0.171986
Epoch: 77 [2560/5999 (43%)]	Loss: 0.082309
Epoch: 77 [5120/5999 (85%)]	Loss: 0.075394
====> Epoch: 77 Average loss: 0.000587 
Epoch: 78 [0/5999 (0%)]	Loss: 0.082264
Epoch: 78 [2560/5999 (43%)]	Loss: 0.082851
Epoch: 78 [5120/5999 (85%)]	Loss: 0.066918
====> Epoch: 78 Average loss: 0.000917 
Epoch: 79 [0/5999 (0%)]	Loss: 0.038665
Epoch: 79 [2560/5999 (43%)]	Loss: 0.060350
Epoch: 79 [5120/5999 (85%)]	Loss: 0.071414
====> Epoch: 79 Average loss: 0.000628 
Epoch: 80 [0/5999 (0%)]	Loss: 0.033028
Epoch: 80 [2560/5999 (43%)]	Loss: 0.052353
Epoch: 80 [5120/5999 (85%)]	Loss: 0.152313
====> Epoch: 80 Average loss: 0.000653 
Epoch: 81 [0/5999 (0%)]	Loss: 0.089399
Epoch: 81 [2560/5999 (43%)]	Loss: 0.043800
saving model at:81,0.00026137385226320474
Epoch: 81 [5120/5999 (85%)]	Loss: 0.273789
====> Epoch: 81 Average loss: 0.000633 
Epoch: 82 [0/5999 (0%)]	Loss: 0.048760
Epoch: 82 [2560/5999 (43%)]	Loss: 0.073439
saving model at:82,0.00025145359197631476
Epoch: 82 [5120/5999 (85%)]	Loss: 0.115543
====> Epoch: 82 Average loss: 0.000632 
Epoch: 83 [0/5999 (0%)]	Loss: 0.078766
Epoch: 83 [2560/5999 (43%)]	Loss: 0.050307
Epoch: 83 [5120/5999 (85%)]	Loss: 0.050004
====> Epoch: 83 Average loss: 0.000556 
Epoch: 84 [0/5999 (0%)]	Loss: 0.255413
Epoch: 84 [2560/5999 (43%)]	Loss: 0.041550
Epoch: 84 [5120/5999 (85%)]	Loss: 0.047112
====> Epoch: 84 Average loss: 0.000564 
Epoch: 85 [0/5999 (0%)]	Loss: 0.037396
Epoch: 85 [2560/5999 (43%)]	Loss: 0.061296
Epoch: 85 [5120/5999 (85%)]	Loss: 0.087127
saving model at:85,0.00023613459430634975
====> Epoch: 85 Average loss: 0.000534 
Epoch: 86 [0/5999 (0%)]	Loss: 0.043238
Epoch: 86 [2560/5999 (43%)]	Loss: 0.032770
saving model at:86,0.00023153201607055963
Epoch: 86 [5120/5999 (85%)]	Loss: 0.100321
====> Epoch: 86 Average loss: 0.000418 
Epoch: 87 [0/5999 (0%)]	Loss: 0.149543
Epoch: 87 [2560/5999 (43%)]	Loss: 0.044941
saving model at:87,0.0002200215628836304
Epoch: 87 [5120/5999 (85%)]	Loss: 0.089694
====> Epoch: 87 Average loss: 0.000706 
Epoch: 88 [0/5999 (0%)]	Loss: 0.322726
Epoch: 88 [2560/5999 (43%)]	Loss: 0.108872
Epoch: 88 [5120/5999 (85%)]	Loss: 0.058692
====> Epoch: 88 Average loss: 0.000854 
Epoch: 89 [0/5999 (0%)]	Loss: 0.051123
Epoch: 89 [2560/5999 (43%)]	Loss: 0.062763
Epoch: 89 [5120/5999 (85%)]	Loss: 0.064128
====> Epoch: 89 Average loss: 0.000544 
Epoch: 90 [0/5999 (0%)]	Loss: 0.127392
Epoch: 90 [2560/5999 (43%)]	Loss: 0.147079
Epoch: 90 [5120/5999 (85%)]	Loss: 0.367963
====> Epoch: 90 Average loss: 0.000729 
Epoch: 91 [0/5999 (0%)]	Loss: 0.141172
Epoch: 91 [2560/5999 (43%)]	Loss: 0.138375
Epoch: 91 [5120/5999 (85%)]	Loss: 0.064286
====> Epoch: 91 Average loss: 0.000591 
Epoch: 92 [0/5999 (0%)]	Loss: 0.178572
Epoch: 92 [2560/5999 (43%)]	Loss: 0.059028
Epoch: 92 [5120/5999 (85%)]	Loss: 0.062476
====> Epoch: 92 Average loss: 0.000453 
Epoch: 93 [0/5999 (0%)]	Loss: 0.044691
Epoch: 93 [2560/5999 (43%)]	Loss: 0.065260
Epoch: 93 [5120/5999 (85%)]	Loss: 0.045964
====> Epoch: 93 Average loss: 0.000548 
Epoch: 94 [0/5999 (0%)]	Loss: 0.133471
Epoch: 94 [2560/5999 (43%)]	Loss: 0.034603
Epoch: 94 [5120/5999 (85%)]	Loss: 0.059293
====> Epoch: 94 Average loss: 0.000757 
Epoch: 95 [0/5999 (0%)]	Loss: 0.034082
Epoch: 95 [2560/5999 (43%)]	Loss: 0.085556
Epoch: 95 [5120/5999 (85%)]	Loss: 0.035232
saving model at:95,0.00021659553237259388
====> Epoch: 95 Average loss: 0.000538 
Epoch: 96 [0/5999 (0%)]	Loss: 0.072859
Epoch: 96 [2560/5999 (43%)]	Loss: 0.039301
Epoch: 96 [5120/5999 (85%)]	Loss: 0.218885
====> Epoch: 96 Average loss: 0.000779 
Epoch: 97 [0/5999 (0%)]	Loss: 0.141633
Epoch: 97 [2560/5999 (43%)]	Loss: 0.126583
Epoch: 97 [5120/5999 (85%)]	Loss: 0.154688
====> Epoch: 97 Average loss: 0.000927 
Epoch: 98 [0/5999 (0%)]	Loss: 0.050578
Epoch: 98 [2560/5999 (43%)]	Loss: 0.043341
Epoch: 98 [5120/5999 (85%)]	Loss: 0.104307
====> Epoch: 98 Average loss: 0.000568 
Epoch: 99 [0/5999 (0%)]	Loss: 0.127487
Epoch: 99 [2560/5999 (43%)]	Loss: 0.053471
Epoch: 99 [5120/5999 (85%)]	Loss: 0.047846
====> Epoch: 99 Average loss: 0.000601 
Epoch: 100 [0/5999 (0%)]	Loss: 0.057848
Epoch: 100 [2560/5999 (43%)]	Loss: 0.057876
Epoch: 100 [5120/5999 (85%)]	Loss: 0.087083
====> Epoch: 100 Average loss: 0.000547 
Epoch: 101 [0/5999 (0%)]	Loss: 0.041754
Epoch: 101 [2560/5999 (43%)]	Loss: 0.045119
Epoch: 101 [5120/5999 (85%)]	Loss: 0.241786
====> Epoch: 101 Average loss: 0.000847 
Epoch: 102 [0/5999 (0%)]	Loss: 0.070693
Epoch: 102 [2560/5999 (43%)]	Loss: 0.194953
Epoch: 102 [5120/5999 (85%)]	Loss: 0.054318
====> Epoch: 102 Average loss: 0.000788 
Epoch: 103 [0/5999 (0%)]	Loss: 0.062820
Epoch: 103 [2560/5999 (43%)]	Loss: 0.049878
Epoch: 103 [5120/5999 (85%)]	Loss: 0.116323
====> Epoch: 103 Average loss: 0.000706 
Epoch: 104 [0/5999 (0%)]	Loss: 0.152087
Epoch: 104 [2560/5999 (43%)]	Loss: 0.100844
Epoch: 104 [5120/5999 (85%)]	Loss: 0.382769
====> Epoch: 104 Average loss: 0.000613 
Epoch: 105 [0/5999 (0%)]	Loss: 0.061259
Epoch: 105 [2560/5999 (43%)]	Loss: 0.065003
Epoch: 105 [5120/5999 (85%)]	Loss: 0.048664
====> Epoch: 105 Average loss: 0.000886 
Epoch: 106 [0/5999 (0%)]	Loss: 0.146446
Epoch: 106 [2560/5999 (43%)]	Loss: 0.057558
Epoch: 106 [5120/5999 (85%)]	Loss: 0.095259
====> Epoch: 106 Average loss: 0.000631 
Epoch: 107 [0/5999 (0%)]	Loss: 0.040645
Epoch: 107 [2560/5999 (43%)]	Loss: 0.111949
Epoch: 107 [5120/5999 (85%)]	Loss: 0.039093
====> Epoch: 107 Average loss: 0.000632 
Epoch: 108 [0/5999 (0%)]	Loss: 0.035545
Epoch: 108 [2560/5999 (43%)]	Loss: 0.051899
saving model at:108,0.00021587650966830552
Epoch: 108 [5120/5999 (85%)]	Loss: 0.035677
saving model at:108,0.00020590443653054535
====> Epoch: 108 Average loss: 0.000414 
Epoch: 109 [0/5999 (0%)]	Loss: 0.151681
Epoch: 109 [2560/5999 (43%)]	Loss: 0.077618
Epoch: 109 [5120/5999 (85%)]	Loss: 0.039404
====> Epoch: 109 Average loss: 0.000747 
Epoch: 110 [0/5999 (0%)]	Loss: 0.056722
Epoch: 110 [2560/5999 (43%)]	Loss: 0.047760
saving model at:110,0.00019773642148356884
Epoch: 110 [5120/5999 (85%)]	Loss: 0.097368
====> Epoch: 110 Average loss: 0.000566 
Epoch: 111 [0/5999 (0%)]	Loss: 0.077240
Epoch: 111 [2560/5999 (43%)]	Loss: 0.070349
Epoch: 111 [5120/5999 (85%)]	Loss: 0.027119
====> Epoch: 111 Average loss: 0.000527 
Epoch: 112 [0/5999 (0%)]	Loss: 0.030236
Epoch: 112 [2560/5999 (43%)]	Loss: 0.044119
Epoch: 112 [5120/5999 (85%)]	Loss: 0.041481
====> Epoch: 112 Average loss: 0.000555 
Epoch: 113 [0/5999 (0%)]	Loss: 0.038391
Epoch: 113 [2560/5999 (43%)]	Loss: 0.077854
Epoch: 113 [5120/5999 (85%)]	Loss: 0.039677
====> Epoch: 113 Average loss: 0.000499 
Epoch: 114 [0/5999 (0%)]	Loss: 0.030224
Epoch: 114 [2560/5999 (43%)]	Loss: 0.065600
Epoch: 114 [5120/5999 (85%)]	Loss: 0.113041
====> Epoch: 114 Average loss: 0.000662 
Epoch: 115 [0/5999 (0%)]	Loss: 0.136833
Epoch: 115 [2560/5999 (43%)]	Loss: 0.116942
Epoch: 115 [5120/5999 (85%)]	Loss: 0.105981
====> Epoch: 115 Average loss: 0.000709 
Epoch: 116 [0/5999 (0%)]	Loss: 0.135265
Epoch: 116 [2560/5999 (43%)]	Loss: 0.055037
Epoch: 116 [5120/5999 (85%)]	Loss: 0.042967
====> Epoch: 116 Average loss: 0.000457 
Epoch: 117 [0/5999 (0%)]	Loss: 0.034181
Epoch: 117 [2560/5999 (43%)]	Loss: 0.076757
Epoch: 117 [5120/5999 (85%)]	Loss: 0.055837
====> Epoch: 117 Average loss: 0.000501 
Epoch: 118 [0/5999 (0%)]	Loss: 0.068570
Epoch: 118 [2560/5999 (43%)]	Loss: 0.063243
Epoch: 118 [5120/5999 (85%)]	Loss: 0.096397
====> Epoch: 118 Average loss: 0.000482 
Epoch: 119 [0/5999 (0%)]	Loss: 0.042822
Epoch: 119 [2560/5999 (43%)]	Loss: 0.127096
Epoch: 119 [5120/5999 (85%)]	Loss: 0.059744
saving model at:119,0.0001829160222550854
====> Epoch: 119 Average loss: 0.000497 
Epoch: 120 [0/5999 (0%)]	Loss: 0.128750
Epoch: 120 [2560/5999 (43%)]	Loss: 0.035794
Epoch: 120 [5120/5999 (85%)]	Loss: 0.121293
====> Epoch: 120 Average loss: 0.000604 
Epoch: 121 [0/5999 (0%)]	Loss: 0.036953
Epoch: 121 [2560/5999 (43%)]	Loss: 0.040505
Epoch: 121 [5120/5999 (85%)]	Loss: 0.027104
====> Epoch: 121 Average loss: 0.000575 
Epoch: 122 [0/5999 (0%)]	Loss: 0.032413
Epoch: 122 [2560/5999 (43%)]	Loss: 0.052408
Epoch: 122 [5120/5999 (85%)]	Loss: 0.295008
====> Epoch: 122 Average loss: 0.000813 
Epoch: 123 [0/5999 (0%)]	Loss: 0.044600
Epoch: 123 [2560/5999 (43%)]	Loss: 0.057733
Epoch: 123 [5120/5999 (85%)]	Loss: 0.066917
====> Epoch: 123 Average loss: 0.000691 
Epoch: 124 [0/5999 (0%)]	Loss: 0.202230
Epoch: 124 [2560/5999 (43%)]	Loss: 0.049013
Epoch: 124 [5120/5999 (85%)]	Loss: 0.037041
====> Epoch: 124 Average loss: 0.000983 
Epoch: 125 [0/5999 (0%)]	Loss: 0.039699
Epoch: 125 [2560/5999 (43%)]	Loss: 0.088754
Epoch: 125 [5120/5999 (85%)]	Loss: 0.135816
====> Epoch: 125 Average loss: 0.000519 
Epoch: 126 [0/5999 (0%)]	Loss: 0.273109
Epoch: 126 [2560/5999 (43%)]	Loss: 0.070884
Epoch: 126 [5120/5999 (85%)]	Loss: 0.032110
====> Epoch: 126 Average loss: 0.000776 
Epoch: 127 [0/5999 (0%)]	Loss: 0.143070
Epoch: 127 [2560/5999 (43%)]	Loss: 0.027920
Epoch: 127 [5120/5999 (85%)]	Loss: 0.032178
====> Epoch: 127 Average loss: 0.000497 
Epoch: 128 [0/5999 (0%)]	Loss: 0.035538
Epoch: 128 [2560/5999 (43%)]	Loss: 0.036179
Epoch: 128 [5120/5999 (85%)]	Loss: 0.059487
====> Epoch: 128 Average loss: 0.000403 
Epoch: 129 [0/5999 (0%)]	Loss: 0.038825
Epoch: 129 [2560/5999 (43%)]	Loss: 0.024409
Epoch: 129 [5120/5999 (85%)]	Loss: 0.043740
====> Epoch: 129 Average loss: 0.000590 
Epoch: 130 [0/5999 (0%)]	Loss: 0.048118
Epoch: 130 [2560/5999 (43%)]	Loss: 0.091388
Epoch: 130 [5120/5999 (85%)]	Loss: 0.091412
====> Epoch: 130 Average loss: 0.000584 
Epoch: 131 [0/5999 (0%)]	Loss: 0.059540
Epoch: 131 [2560/5999 (43%)]	Loss: 0.039069
Epoch: 131 [5120/5999 (85%)]	Loss: 0.051506
====> Epoch: 131 Average loss: 0.000476 
Epoch: 132 [0/5999 (0%)]	Loss: 0.055341
Epoch: 132 [2560/5999 (43%)]	Loss: 0.031546
saving model at:132,0.00017805142153520136
Epoch: 132 [5120/5999 (85%)]	Loss: 0.108589
====> Epoch: 132 Average loss: 0.000527 
Epoch: 133 [0/5999 (0%)]	Loss: 0.020846
Epoch: 133 [2560/5999 (43%)]	Loss: 0.078462
Epoch: 133 [5120/5999 (85%)]	Loss: 0.114717
====> Epoch: 133 Average loss: 0.000604 
Epoch: 134 [0/5999 (0%)]	Loss: 0.142781
Epoch: 134 [2560/5999 (43%)]	Loss: 0.064817
Epoch: 134 [5120/5999 (85%)]	Loss: 0.110064
====> Epoch: 134 Average loss: 0.000510 
Epoch: 135 [0/5999 (0%)]	Loss: 0.052353
Epoch: 135 [2560/5999 (43%)]	Loss: 0.024340
Epoch: 135 [5120/5999 (85%)]	Loss: 0.043170
saving model at:135,0.00014105548639781773
====> Epoch: 135 Average loss: 0.000359 
Epoch: 136 [0/5999 (0%)]	Loss: 0.032162
Epoch: 136 [2560/5999 (43%)]	Loss: 0.093657
Epoch: 136 [5120/5999 (85%)]	Loss: 0.065624
====> Epoch: 136 Average loss: 0.000703 
Epoch: 137 [0/5999 (0%)]	Loss: 0.109458
Epoch: 137 [2560/5999 (43%)]	Loss: 0.074955
Epoch: 137 [5120/5999 (85%)]	Loss: 0.189056
====> Epoch: 137 Average loss: 0.000798 
Epoch: 138 [0/5999 (0%)]	Loss: 0.046100
Epoch: 138 [2560/5999 (43%)]	Loss: 0.048783
Epoch: 138 [5120/5999 (85%)]	Loss: 0.071075
====> Epoch: 138 Average loss: 0.000495 
Epoch: 139 [0/5999 (0%)]	Loss: 0.035897
Epoch: 139 [2560/5999 (43%)]	Loss: 0.083873
Epoch: 139 [5120/5999 (85%)]	Loss: 0.030370
====> Epoch: 139 Average loss: 0.000475 
Epoch: 140 [0/5999 (0%)]	Loss: 0.023703
Epoch: 140 [2560/5999 (43%)]	Loss: 0.031013
Epoch: 140 [5120/5999 (85%)]	Loss: 0.058689
====> Epoch: 140 Average loss: 0.000642 
Epoch: 141 [0/5999 (0%)]	Loss: 0.116458
Epoch: 141 [2560/5999 (43%)]	Loss: 0.031318
Epoch: 141 [5120/5999 (85%)]	Loss: 0.034734
====> Epoch: 141 Average loss: 0.000418 
Epoch: 142 [0/5999 (0%)]	Loss: 0.043918
Epoch: 142 [2560/5999 (43%)]	Loss: 0.028240
Epoch: 142 [5120/5999 (85%)]	Loss: 0.033916
====> Epoch: 142 Average loss: 0.000525 
Epoch: 143 [0/5999 (0%)]	Loss: 0.068486
Epoch: 143 [2560/5999 (43%)]	Loss: 0.069665
Epoch: 143 [5120/5999 (85%)]	Loss: 0.051600
====> Epoch: 143 Average loss: 0.000490 
Epoch: 144 [0/5999 (0%)]	Loss: 0.032884
Epoch: 144 [2560/5999 (43%)]	Loss: 0.038904
Epoch: 144 [5120/5999 (85%)]	Loss: 0.096383
====> Epoch: 144 Average loss: 0.000489 
Epoch: 145 [0/5999 (0%)]	Loss: 0.028823
Epoch: 145 [2560/5999 (43%)]	Loss: 0.135550
Epoch: 145 [5120/5999 (85%)]	Loss: 0.025314
saving model at:145,0.00013766521273646503
====> Epoch: 145 Average loss: 0.000366 
Epoch: 146 [0/5999 (0%)]	Loss: 0.028995
Epoch: 146 [2560/5999 (43%)]	Loss: 0.135273
Epoch: 146 [5120/5999 (85%)]	Loss: 0.045466
====> Epoch: 146 Average loss: 0.000856 
Epoch: 147 [0/5999 (0%)]	Loss: 0.062008
Epoch: 147 [2560/5999 (43%)]	Loss: 0.023213
Epoch: 147 [5120/5999 (85%)]	Loss: 0.040288
====> Epoch: 147 Average loss: 0.000326 
Epoch: 148 [0/5999 (0%)]	Loss: 0.030057
Epoch: 148 [2560/5999 (43%)]	Loss: 0.057432
Epoch: 148 [5120/5999 (85%)]	Loss: 0.100909
====> Epoch: 148 Average loss: 0.000808 
Epoch: 149 [0/5999 (0%)]	Loss: 0.068734
Epoch: 149 [2560/5999 (43%)]	Loss: 0.027988
Epoch: 149 [5120/5999 (85%)]	Loss: 0.052600
====> Epoch: 149 Average loss: 0.000690 
Epoch: 150 [0/5999 (0%)]	Loss: 0.082774
Epoch: 150 [2560/5999 (43%)]	Loss: 0.134333
Epoch: 150 [5120/5999 (85%)]	Loss: 0.062297
====> Epoch: 150 Average loss: 0.000496 
Epoch: 151 [0/5999 (0%)]	Loss: 0.029018
Epoch: 151 [2560/5999 (43%)]	Loss: 0.033842
Epoch: 151 [5120/5999 (85%)]	Loss: 0.046827
====> Epoch: 151 Average loss: 0.000471 
Epoch: 152 [0/5999 (0%)]	Loss: 0.116210
Epoch: 152 [2560/5999 (43%)]	Loss: 0.104375
Epoch: 152 [5120/5999 (85%)]	Loss: 0.036281
====> Epoch: 152 Average loss: 0.000437 
Epoch: 153 [0/5999 (0%)]	Loss: 0.043254
Epoch: 153 [2560/5999 (43%)]	Loss: 0.049066
Epoch: 153 [5120/5999 (85%)]	Loss: 0.052402
====> Epoch: 153 Average loss: 0.000464 
Epoch: 154 [0/5999 (0%)]	Loss: 0.044118
Epoch: 154 [2560/5999 (43%)]	Loss: 0.023156
Epoch: 154 [5120/5999 (85%)]	Loss: 0.094467
====> Epoch: 154 Average loss: 0.000572 
Epoch: 155 [0/5999 (0%)]	Loss: 0.098101
Epoch: 155 [2560/5999 (43%)]	Loss: 0.058121
Epoch: 155 [5120/5999 (85%)]	Loss: 0.050135
====> Epoch: 155 Average loss: 0.000502 
Epoch: 156 [0/5999 (0%)]	Loss: 0.020322
Epoch: 156 [2560/5999 (43%)]	Loss: 0.031341
Epoch: 156 [5120/5999 (85%)]	Loss: 0.068597
====> Epoch: 156 Average loss: 0.000610 
Epoch: 157 [0/5999 (0%)]	Loss: 0.091764
Epoch: 157 [2560/5999 (43%)]	Loss: 0.034495
Epoch: 157 [5120/5999 (85%)]	Loss: 0.021256
====> Epoch: 157 Average loss: 0.000405 
Epoch: 158 [0/5999 (0%)]	Loss: 0.056588
Epoch: 158 [2560/5999 (43%)]	Loss: 0.026083
Epoch: 158 [5120/5999 (85%)]	Loss: 0.041189
====> Epoch: 158 Average loss: 0.000406 
Epoch: 159 [0/5999 (0%)]	Loss: 0.035165
Epoch: 159 [2560/5999 (43%)]	Loss: 0.029785
Epoch: 159 [5120/5999 (85%)]	Loss: 0.080803
====> Epoch: 159 Average loss: 0.000404 
Epoch: 160 [0/5999 (0%)]	Loss: 0.051277
Epoch: 160 [2560/5999 (43%)]	Loss: 0.041585
Epoch: 160 [5120/5999 (85%)]	Loss: 0.049440
====> Epoch: 160 Average loss: 0.000622 
Epoch: 161 [0/5999 (0%)]	Loss: 0.035516
Epoch: 161 [2560/5999 (43%)]	Loss: 0.021560
Epoch: 161 [5120/5999 (85%)]	Loss: 0.069690
====> Epoch: 161 Average loss: 0.000447 
Epoch: 162 [0/5999 (0%)]	Loss: 0.022941
Epoch: 162 [2560/5999 (43%)]	Loss: 0.076022
Epoch: 162 [5120/5999 (85%)]	Loss: 0.023226
====> Epoch: 162 Average loss: 0.000370 
Epoch: 163 [0/5999 (0%)]	Loss: 0.056610
Epoch: 163 [2560/5999 (43%)]	Loss: 0.058531
Epoch: 163 [5120/5999 (85%)]	Loss: 0.036150
====> Epoch: 163 Average loss: 0.000499 
Epoch: 164 [0/5999 (0%)]	Loss: 0.019786
Epoch: 164 [2560/5999 (43%)]	Loss: 0.070076
Epoch: 164 [5120/5999 (85%)]	Loss: 0.041034
====> Epoch: 164 Average loss: 0.000341 
Epoch: 165 [0/5999 (0%)]	Loss: 0.019686
Epoch: 165 [2560/5999 (43%)]	Loss: 0.028032
Epoch: 165 [5120/5999 (85%)]	Loss: 0.032494
====> Epoch: 165 Average loss: 0.000408 
Epoch: 166 [0/5999 (0%)]	Loss: 0.053862
Epoch: 166 [2560/5999 (43%)]	Loss: 0.024260
saving model at:166,0.00013133376510813833
Epoch: 166 [5120/5999 (85%)]	Loss: 0.076822
====> Epoch: 166 Average loss: 0.000523 
Epoch: 167 [0/5999 (0%)]	Loss: 0.036912
Epoch: 167 [2560/5999 (43%)]	Loss: 0.032978
Epoch: 167 [5120/5999 (85%)]	Loss: 0.055188
====> Epoch: 167 Average loss: 0.000626 
Epoch: 168 [0/5999 (0%)]	Loss: 0.073736
Epoch: 168 [2560/5999 (43%)]	Loss: 0.073260
Epoch: 168 [5120/5999 (85%)]	Loss: 0.055371
====> Epoch: 168 Average loss: 0.000412 
Epoch: 169 [0/5999 (0%)]	Loss: 0.022275
Epoch: 169 [2560/5999 (43%)]	Loss: 0.045020
Epoch: 169 [5120/5999 (85%)]	Loss: 0.023589
====> Epoch: 169 Average loss: 0.000439 
Epoch: 170 [0/5999 (0%)]	Loss: 0.103164
Epoch: 170 [2560/5999 (43%)]	Loss: 0.031293
Epoch: 170 [5120/5999 (85%)]	Loss: 0.033144
saving model at:170,0.00012332531390711664
====> Epoch: 170 Average loss: 0.000392 
Epoch: 171 [0/5999 (0%)]	Loss: 0.186633
Epoch: 171 [2560/5999 (43%)]	Loss: 0.034040
Epoch: 171 [5120/5999 (85%)]	Loss: 0.039056
====> Epoch: 171 Average loss: 0.000482 
Epoch: 172 [0/5999 (0%)]	Loss: 0.049206
Epoch: 172 [2560/5999 (43%)]	Loss: 0.028289
Epoch: 172 [5120/5999 (85%)]	Loss: 0.028818
====> Epoch: 172 Average loss: 0.000332 
Epoch: 173 [0/5999 (0%)]	Loss: 0.041477
Epoch: 173 [2560/5999 (43%)]	Loss: 0.020742
Epoch: 173 [5120/5999 (85%)]	Loss: 0.073972
====> Epoch: 173 Average loss: 0.000465 
Epoch: 174 [0/5999 (0%)]	Loss: 0.068987
Epoch: 174 [2560/5999 (43%)]	Loss: 0.037349
Epoch: 174 [5120/5999 (85%)]	Loss: 0.207683
====> Epoch: 174 Average loss: 0.000559 
Epoch: 175 [0/5999 (0%)]	Loss: 0.029575
Epoch: 175 [2560/5999 (43%)]	Loss: 0.072910
Epoch: 175 [5120/5999 (85%)]	Loss: 0.045472
====> Epoch: 175 Average loss: 0.000372 
Epoch: 176 [0/5999 (0%)]	Loss: 0.041304
Epoch: 176 [2560/5999 (43%)]	Loss: 0.075863
Epoch: 176 [5120/5999 (85%)]	Loss: 0.132354
====> Epoch: 176 Average loss: 0.000532 
Epoch: 177 [0/5999 (0%)]	Loss: 0.053381
Epoch: 177 [2560/5999 (43%)]	Loss: 0.064059
Epoch: 177 [5120/5999 (85%)]	Loss: 0.027226
====> Epoch: 177 Average loss: 0.000405 
Epoch: 178 [0/5999 (0%)]	Loss: 0.051787
Epoch: 178 [2560/5999 (43%)]	Loss: 0.078383
Epoch: 178 [5120/5999 (85%)]	Loss: 0.035092
====> Epoch: 178 Average loss: 0.000345 
Epoch: 179 [0/5999 (0%)]	Loss: 0.018467
Epoch: 179 [2560/5999 (43%)]	Loss: 0.085152
Epoch: 179 [5120/5999 (85%)]	Loss: 0.171573
====> Epoch: 179 Average loss: 0.000467 
Epoch: 180 [0/5999 (0%)]	Loss: 0.028396
Epoch: 180 [2560/5999 (43%)]	Loss: 0.025714
Epoch: 180 [5120/5999 (85%)]	Loss: 0.046212
====> Epoch: 180 Average loss: 0.000358 
Epoch: 181 [0/5999 (0%)]	Loss: 0.027421
Epoch: 181 [2560/5999 (43%)]	Loss: 0.075938
Epoch: 181 [5120/5999 (85%)]	Loss: 0.036561
====> Epoch: 181 Average loss: 0.000311 
Epoch: 182 [0/5999 (0%)]	Loss: 0.016661
Epoch: 182 [2560/5999 (43%)]	Loss: 0.041527
saving model at:182,0.00011718153138644993
Epoch: 182 [5120/5999 (85%)]	Loss: 0.156352
====> Epoch: 182 Average loss: 0.000463 
Epoch: 183 [0/5999 (0%)]	Loss: 0.025495
Epoch: 183 [2560/5999 (43%)]	Loss: 0.030306
Epoch: 183 [5120/5999 (85%)]	Loss: 0.020198
====> Epoch: 183 Average loss: 0.000484 
Epoch: 184 [0/5999 (0%)]	Loss: 0.050626
Epoch: 184 [2560/5999 (43%)]	Loss: 0.025828
Epoch: 184 [5120/5999 (85%)]	Loss: 0.045545
====> Epoch: 184 Average loss: 0.000366 
Epoch: 185 [0/5999 (0%)]	Loss: 0.018639
Epoch: 185 [2560/5999 (43%)]	Loss: 0.015033
saving model at:185,0.00010325263219419866
Epoch: 185 [5120/5999 (85%)]	Loss: 0.117045
====> Epoch: 185 Average loss: 0.000431 
Epoch: 186 [0/5999 (0%)]	Loss: 0.026548
Epoch: 186 [2560/5999 (43%)]	Loss: 0.100656
Epoch: 186 [5120/5999 (85%)]	Loss: 0.025737
====> Epoch: 186 Average loss: 0.000509 
Epoch: 187 [0/5999 (0%)]	Loss: 0.090390
Epoch: 187 [2560/5999 (43%)]	Loss: 0.024606
Epoch: 187 [5120/5999 (85%)]	Loss: 0.029016
====> Epoch: 187 Average loss: 0.000418 
Epoch: 188 [0/5999 (0%)]	Loss: 0.073235
Epoch: 188 [2560/5999 (43%)]	Loss: 0.047714
Epoch: 188 [5120/5999 (85%)]	Loss: 0.120855
====> Epoch: 188 Average loss: 0.000305 
Epoch: 189 [0/5999 (0%)]	Loss: 0.061934
Epoch: 189 [2560/5999 (43%)]	Loss: 0.072062
Epoch: 189 [5120/5999 (85%)]	Loss: 0.045842
====> Epoch: 189 Average loss: 0.000464 
Epoch: 190 [0/5999 (0%)]	Loss: 0.025957
Epoch: 190 [2560/5999 (43%)]	Loss: 0.041443
Epoch: 190 [5120/5999 (85%)]	Loss: 0.021355
====> Epoch: 190 Average loss: 0.000373 
Epoch: 191 [0/5999 (0%)]	Loss: 0.048376
Epoch: 191 [2560/5999 (43%)]	Loss: 0.022701
Epoch: 191 [5120/5999 (85%)]	Loss: 0.071359
====> Epoch: 191 Average loss: 0.000317 
Epoch: 192 [0/5999 (0%)]	Loss: 0.042009
Epoch: 192 [2560/5999 (43%)]	Loss: 0.027736
Epoch: 192 [5120/5999 (85%)]	Loss: 0.019926
====> Epoch: 192 Average loss: 0.000341 
Epoch: 193 [0/5999 (0%)]	Loss: 0.130590
Epoch: 193 [2560/5999 (43%)]	Loss: 0.029143
Epoch: 193 [5120/5999 (85%)]	Loss: 0.054923
====> Epoch: 193 Average loss: 0.000388 
Epoch: 194 [0/5999 (0%)]	Loss: 0.032544
Epoch: 194 [2560/5999 (43%)]	Loss: 0.078899
Epoch: 194 [5120/5999 (85%)]	Loss: 0.065097
====> Epoch: 194 Average loss: 0.000475 
Epoch: 195 [0/5999 (0%)]	Loss: 0.068741
Epoch: 195 [2560/5999 (43%)]	Loss: 0.025597
Epoch: 195 [5120/5999 (85%)]	Loss: 0.025931
====> Epoch: 195 Average loss: 0.000379 
Epoch: 196 [0/5999 (0%)]	Loss: 0.070715
Epoch: 196 [2560/5999 (43%)]	Loss: 0.021501
Epoch: 196 [5120/5999 (85%)]	Loss: 0.020731
====> Epoch: 196 Average loss: 0.000358 
Epoch: 197 [0/5999 (0%)]	Loss: 0.100882
Epoch: 197 [2560/5999 (43%)]	Loss: 0.069588
Epoch: 197 [5120/5999 (85%)]	Loss: 0.085762
====> Epoch: 197 Average loss: 0.000363 
Epoch: 198 [0/5999 (0%)]	Loss: 0.051571
Epoch: 198 [2560/5999 (43%)]	Loss: 0.018814
Epoch: 198 [5120/5999 (85%)]	Loss: 0.091656
====> Epoch: 198 Average loss: 0.000285 
Epoch: 199 [0/5999 (0%)]	Loss: 0.043587
Epoch: 199 [2560/5999 (43%)]	Loss: 0.121181
Epoch: 199 [5120/5999 (85%)]	Loss: 0.018082
====> Epoch: 199 Average loss: 0.000502 
Epoch: 200 [0/5999 (0%)]	Loss: 0.118237
Epoch: 200 [2560/5999 (43%)]	Loss: 0.095507
Epoch: 200 [5120/5999 (85%)]	Loss: 0.017663
====> Epoch: 200 Average loss: 0.000418 
Epoch: 201 [0/5999 (0%)]	Loss: 0.056025
Epoch: 201 [2560/5999 (43%)]	Loss: 0.017968
Epoch: 201 [5120/5999 (85%)]	Loss: 0.022939
====> Epoch: 201 Average loss: 0.000341 
Epoch: 202 [0/5999 (0%)]	Loss: 0.023768
Epoch: 202 [2560/5999 (43%)]	Loss: 0.053509
Epoch: 202 [5120/5999 (85%)]	Loss: 0.023785
====> Epoch: 202 Average loss: 0.000350 
Epoch: 203 [0/5999 (0%)]	Loss: 0.022941
Epoch: 203 [2560/5999 (43%)]	Loss: 0.024577
Epoch: 203 [5120/5999 (85%)]	Loss: 0.050855
====> Epoch: 203 Average loss: 0.000276 
Epoch: 204 [0/5999 (0%)]	Loss: 0.049396
Epoch: 204 [2560/5999 (43%)]	Loss: 0.157202
Epoch: 204 [5120/5999 (85%)]	Loss: 0.017359
====> Epoch: 204 Average loss: 0.000235 
Epoch: 205 [0/5999 (0%)]	Loss: 0.015023
Epoch: 205 [2560/5999 (43%)]	Loss: 0.020015
Epoch: 205 [5120/5999 (85%)]	Loss: 0.023671
====> Epoch: 205 Average loss: 0.000351 
Epoch: 206 [0/5999 (0%)]	Loss: 0.082159
Epoch: 206 [2560/5999 (43%)]	Loss: 0.092648
Epoch: 206 [5120/5999 (85%)]	Loss: 0.058206
====> Epoch: 206 Average loss: 0.000354 
Epoch: 207 [0/5999 (0%)]	Loss: 0.016409
Epoch: 207 [2560/5999 (43%)]	Loss: 0.144251
Epoch: 207 [5120/5999 (85%)]	Loss: 0.017206
====> Epoch: 207 Average loss: 0.000403 
Epoch: 208 [0/5999 (0%)]	Loss: 0.030600
Epoch: 208 [2560/5999 (43%)]	Loss: 0.265102
Epoch: 208 [5120/5999 (85%)]	Loss: 0.024424
====> Epoch: 208 Average loss: 0.000418 
Epoch: 209 [0/5999 (0%)]	Loss: 0.042087
Epoch: 209 [2560/5999 (43%)]	Loss: 0.031069
Epoch: 209 [5120/5999 (85%)]	Loss: 0.064951
====> Epoch: 209 Average loss: 0.000315 
Epoch: 210 [0/5999 (0%)]	Loss: 0.017695
Epoch: 210 [2560/5999 (43%)]	Loss: 0.064465
Epoch: 210 [5120/5999 (85%)]	Loss: 0.021420
====> Epoch: 210 Average loss: 0.000360 
Epoch: 211 [0/5999 (0%)]	Loss: 0.066601
Epoch: 211 [2560/5999 (43%)]	Loss: 0.036228
Epoch: 211 [5120/5999 (85%)]	Loss: 0.058632
====> Epoch: 211 Average loss: 0.000356 
Epoch: 212 [0/5999 (0%)]	Loss: 0.025467
Epoch: 212 [2560/5999 (43%)]	Loss: 0.026300
Epoch: 212 [5120/5999 (85%)]	Loss: 0.038308
====> Epoch: 212 Average loss: 0.000234 
Epoch: 213 [0/5999 (0%)]	Loss: 0.016555
Epoch: 213 [2560/5999 (43%)]	Loss: 0.034283
Epoch: 213 [5120/5999 (85%)]	Loss: 0.084767
====> Epoch: 213 Average loss: 0.000336 
Epoch: 214 [0/5999 (0%)]	Loss: 0.023132
Epoch: 214 [2560/5999 (43%)]	Loss: 0.161956
Epoch: 214 [5120/5999 (85%)]	Loss: 0.059928
====> Epoch: 214 Average loss: 0.000515 
Epoch: 215 [0/5999 (0%)]	Loss: 0.020912
Epoch: 215 [2560/5999 (43%)]	Loss: 0.024544
Epoch: 215 [5120/5999 (85%)]	Loss: 0.027845
====> Epoch: 215 Average loss: 0.000539 
Epoch: 216 [0/5999 (0%)]	Loss: 0.125757
Epoch: 216 [2560/5999 (43%)]	Loss: 0.027088
Epoch: 216 [5120/5999 (85%)]	Loss: 0.065504
====> Epoch: 216 Average loss: 0.000396 
Epoch: 217 [0/5999 (0%)]	Loss: 0.068385
Epoch: 217 [2560/5999 (43%)]	Loss: 0.054870
Epoch: 217 [5120/5999 (85%)]	Loss: 0.025668
====> Epoch: 217 Average loss: 0.000295 
Epoch: 218 [0/5999 (0%)]	Loss: 0.038931
Epoch: 218 [2560/5999 (43%)]	Loss: 0.014094
Epoch: 218 [5120/5999 (85%)]	Loss: 0.103138
====> Epoch: 218 Average loss: 0.000441 
Epoch: 219 [0/5999 (0%)]	Loss: 0.079485
Epoch: 219 [2560/5999 (43%)]	Loss: 0.030188
Epoch: 219 [5120/5999 (85%)]	Loss: 0.040956
====> Epoch: 219 Average loss: 0.000502 
Epoch: 220 [0/5999 (0%)]	Loss: 0.024959
Epoch: 220 [2560/5999 (43%)]	Loss: 0.044485
Epoch: 220 [5120/5999 (85%)]	Loss: 0.032783
====> Epoch: 220 Average loss: 0.000355 
Epoch: 221 [0/5999 (0%)]	Loss: 0.022882
Epoch: 221 [2560/5999 (43%)]	Loss: 0.061422
Epoch: 221 [5120/5999 (85%)]	Loss: 0.118901
====> Epoch: 221 Average loss: 0.000581 
Epoch: 222 [0/5999 (0%)]	Loss: 0.040462
Epoch: 222 [2560/5999 (43%)]	Loss: 0.057404
Epoch: 222 [5120/5999 (85%)]	Loss: 0.047284
====> Epoch: 222 Average loss: 0.000388 
Epoch: 223 [0/5999 (0%)]	Loss: 0.019775
Epoch: 223 [2560/5999 (43%)]	Loss: 0.021840
saving model at:223,9.688169555738568e-05
Epoch: 223 [5120/5999 (85%)]	Loss: 0.019950
====> Epoch: 223 Average loss: 0.000306 
Epoch: 224 [0/5999 (0%)]	Loss: 0.024991
Epoch: 224 [2560/5999 (43%)]	Loss: 0.030389
Epoch: 224 [5120/5999 (85%)]	Loss: 0.027457
====> Epoch: 224 Average loss: 0.000341 
Epoch: 225 [0/5999 (0%)]	Loss: 0.049064
Epoch: 225 [2560/5999 (43%)]	Loss: 0.102735
Epoch: 225 [5120/5999 (85%)]	Loss: 0.017225
====> Epoch: 225 Average loss: 0.000406 
Epoch: 226 [0/5999 (0%)]	Loss: 0.046231
Epoch: 226 [2560/5999 (43%)]	Loss: 0.100658
Epoch: 226 [5120/5999 (85%)]	Loss: 0.023695
====> Epoch: 226 Average loss: 0.000363 
Epoch: 227 [0/5999 (0%)]	Loss: 0.024938
Epoch: 227 [2560/5999 (43%)]	Loss: 0.027283
saving model at:227,8.849000261398032e-05
Epoch: 227 [5120/5999 (85%)]	Loss: 0.045820
====> Epoch: 227 Average loss: 0.000281 
Epoch: 228 [0/5999 (0%)]	Loss: 0.026418
Epoch: 228 [2560/5999 (43%)]	Loss: 0.056091
Epoch: 228 [5120/5999 (85%)]	Loss: 0.065988
====> Epoch: 228 Average loss: 0.000330 
Epoch: 229 [0/5999 (0%)]	Loss: 0.024838
Epoch: 229 [2560/5999 (43%)]	Loss: 0.016965
Epoch: 229 [5120/5999 (85%)]	Loss: 0.024333
====> Epoch: 229 Average loss: 0.000275 
Epoch: 230 [0/5999 (0%)]	Loss: 0.070267
Epoch: 230 [2560/5999 (43%)]	Loss: 0.059333
Epoch: 230 [5120/5999 (85%)]	Loss: 0.064759
====> Epoch: 230 Average loss: 0.000497 
Epoch: 231 [0/5999 (0%)]	Loss: 0.070040
Epoch: 231 [2560/5999 (43%)]	Loss: 0.032112
Epoch: 231 [5120/5999 (85%)]	Loss: 0.069652
====> Epoch: 231 Average loss: 0.000404 
Epoch: 232 [0/5999 (0%)]	Loss: 0.035967
Epoch: 232 [2560/5999 (43%)]	Loss: 0.023357
Epoch: 232 [5120/5999 (85%)]	Loss: 0.144945
====> Epoch: 232 Average loss: 0.000347 
Epoch: 233 [0/5999 (0%)]	Loss: 0.031446
Epoch: 233 [2560/5999 (43%)]	Loss: 0.027370
Epoch: 233 [5120/5999 (85%)]	Loss: 0.045488
====> Epoch: 233 Average loss: 0.000289 
Epoch: 234 [0/5999 (0%)]	Loss: 0.019791
Epoch: 234 [2560/5999 (43%)]	Loss: 0.034009
Epoch: 234 [5120/5999 (85%)]	Loss: 0.065475
====> Epoch: 234 Average loss: 0.000324 
Epoch: 235 [0/5999 (0%)]	Loss: 0.046332
Epoch: 235 [2560/5999 (43%)]	Loss: 0.086599
Epoch: 235 [5120/5999 (85%)]	Loss: 0.048542
====> Epoch: 235 Average loss: 0.000559 
Epoch: 236 [0/5999 (0%)]	Loss: 0.015744
Epoch: 236 [2560/5999 (43%)]	Loss: 0.025804
Epoch: 236 [5120/5999 (85%)]	Loss: 0.033721
====> Epoch: 236 Average loss: 0.000285 
Epoch: 237 [0/5999 (0%)]	Loss: 0.049638
Epoch: 237 [2560/5999 (43%)]	Loss: 0.015236
Epoch: 237 [5120/5999 (85%)]	Loss: 0.042368
====> Epoch: 237 Average loss: 0.000371 
Epoch: 238 [0/5999 (0%)]	Loss: 0.079450
Epoch: 238 [2560/5999 (43%)]	Loss: 0.031553
Epoch: 238 [5120/5999 (85%)]	Loss: 0.230868
====> Epoch: 238 Average loss: 0.000375 
Epoch: 239 [0/5999 (0%)]	Loss: 0.026962
Epoch: 239 [2560/5999 (43%)]	Loss: 0.035840
Epoch: 239 [5120/5999 (85%)]	Loss: 0.018944
====> Epoch: 239 Average loss: 0.000258 
Epoch: 240 [0/5999 (0%)]	Loss: 0.052030
Epoch: 240 [2560/5999 (43%)]	Loss: 0.075544
Epoch: 240 [5120/5999 (85%)]	Loss: 0.030506
====> Epoch: 240 Average loss: 0.000560 
Epoch: 241 [0/5999 (0%)]	Loss: 0.062365
Epoch: 241 [2560/5999 (43%)]	Loss: 0.036732
Epoch: 241 [5120/5999 (85%)]	Loss: 0.026684
====> Epoch: 241 Average loss: 0.000222 
Epoch: 242 [0/5999 (0%)]	Loss: 0.049567
Epoch: 242 [2560/5999 (43%)]	Loss: 0.033130
Epoch: 242 [5120/5999 (85%)]	Loss: 0.022079
====> Epoch: 242 Average loss: 0.000241 
Epoch: 243 [0/5999 (0%)]	Loss: 0.071838
Epoch: 243 [2560/5999 (43%)]	Loss: 0.044228
Epoch: 243 [5120/5999 (85%)]	Loss: 0.110154
====> Epoch: 243 Average loss: 0.000322 
Epoch: 244 [0/5999 (0%)]	Loss: 0.073854
Epoch: 244 [2560/5999 (43%)]	Loss: 0.061295
Epoch: 244 [5120/5999 (85%)]	Loss: 0.159305
====> Epoch: 244 Average loss: 0.000402 
Epoch: 245 [0/5999 (0%)]	Loss: 0.076523
Epoch: 245 [2560/5999 (43%)]	Loss: 0.029434
Epoch: 245 [5120/5999 (85%)]	Loss: 0.025549
====> Epoch: 245 Average loss: 0.000385 
Epoch: 246 [0/5999 (0%)]	Loss: 0.045437
Epoch: 246 [2560/5999 (43%)]	Loss: 0.091501
Epoch: 246 [5120/5999 (85%)]	Loss: 0.022075
====> Epoch: 246 Average loss: 0.000366 
Epoch: 247 [0/5999 (0%)]	Loss: 0.019643
Epoch: 247 [2560/5999 (43%)]	Loss: 0.066775
Epoch: 247 [5120/5999 (85%)]	Loss: 0.021074
====> Epoch: 247 Average loss: 0.000263 
Epoch: 248 [0/5999 (0%)]	Loss: 0.034132
Epoch: 248 [2560/5999 (43%)]	Loss: 0.031794
Epoch: 248 [5120/5999 (85%)]	Loss: 0.051212
====> Epoch: 248 Average loss: 0.000299 
Epoch: 249 [0/5999 (0%)]	Loss: 0.063992
Epoch: 249 [2560/5999 (43%)]	Loss: 0.019982
Epoch: 249 [5120/5999 (85%)]	Loss: 0.014076
saving model at:249,7.999813603237272e-05
====> Epoch: 249 Average loss: 0.000286 
Epoch: 250 [0/5999 (0%)]	Loss: 0.013378
Epoch: 250 [2560/5999 (43%)]	Loss: 0.039372
Epoch: 250 [5120/5999 (85%)]	Loss: 0.022287
====> Epoch: 250 Average loss: 0.000268 
Epoch: 251 [0/5999 (0%)]	Loss: 0.032905
Epoch: 251 [2560/5999 (43%)]	Loss: 0.047104
Epoch: 251 [5120/5999 (85%)]	Loss: 0.021956
====> Epoch: 251 Average loss: 0.000272 
Epoch: 252 [0/5999 (0%)]	Loss: 0.016831
Epoch: 252 [2560/5999 (43%)]	Loss: 0.060100
Epoch: 252 [5120/5999 (85%)]	Loss: 0.015384
====> Epoch: 252 Average loss: 0.000331 
Epoch: 253 [0/5999 (0%)]	Loss: 0.064284
Epoch: 253 [2560/5999 (43%)]	Loss: 0.080348
Epoch: 253 [5120/5999 (85%)]	Loss: 0.034948
====> Epoch: 253 Average loss: 0.000289 
Epoch: 254 [0/5999 (0%)]	Loss: 0.017137
Epoch: 254 [2560/5999 (43%)]	Loss: 0.035277
Epoch: 254 [5120/5999 (85%)]	Loss: 0.018997
====> Epoch: 254 Average loss: 0.000316 
Epoch: 255 [0/5999 (0%)]	Loss: 0.023788
Epoch: 255 [2560/5999 (43%)]	Loss: 0.016298
Epoch: 255 [5120/5999 (85%)]	Loss: 0.045251
====> Epoch: 255 Average loss: 0.000279 
Epoch: 256 [0/5999 (0%)]	Loss: 0.021075
Epoch: 256 [2560/5999 (43%)]	Loss: 0.018132
Epoch: 256 [5120/5999 (85%)]	Loss: 0.050821
====> Epoch: 256 Average loss: 0.000290 
Epoch: 257 [0/5999 (0%)]	Loss: 0.112990
Epoch: 257 [2560/5999 (43%)]	Loss: 0.082539
Epoch: 257 [5120/5999 (85%)]	Loss: 0.054715
====> Epoch: 257 Average loss: 0.000430 
Epoch: 258 [0/5999 (0%)]	Loss: 0.128841
Epoch: 258 [2560/5999 (43%)]	Loss: 0.026183
Epoch: 258 [5120/5999 (85%)]	Loss: 0.044615
====> Epoch: 258 Average loss: 0.000352 
Epoch: 259 [0/5999 (0%)]	Loss: 0.024049
Epoch: 259 [2560/5999 (43%)]	Loss: 0.066265
Epoch: 259 [5120/5999 (85%)]	Loss: 0.043037
====> Epoch: 259 Average loss: 0.000335 
Epoch: 260 [0/5999 (0%)]	Loss: 0.016645
Epoch: 260 [2560/5999 (43%)]	Loss: 0.017583
Epoch: 260 [5120/5999 (85%)]	Loss: 0.015793
====> Epoch: 260 Average loss: 0.000279 
Epoch: 261 [0/5999 (0%)]	Loss: 0.041213
Epoch: 261 [2560/5999 (43%)]	Loss: 0.053948
Epoch: 261 [5120/5999 (85%)]	Loss: 0.033797
====> Epoch: 261 Average loss: 0.000239 
Epoch: 262 [0/5999 (0%)]	Loss: 0.022909
Epoch: 262 [2560/5999 (43%)]	Loss: 0.030894
Epoch: 262 [5120/5999 (85%)]	Loss: 0.019258
====> Epoch: 262 Average loss: 0.000263 
Epoch: 263 [0/5999 (0%)]	Loss: 0.020594
Epoch: 263 [2560/5999 (43%)]	Loss: 0.082133
Epoch: 263 [5120/5999 (85%)]	Loss: 0.022229
====> Epoch: 263 Average loss: 0.000246 
Epoch: 264 [0/5999 (0%)]	Loss: 0.021024
Epoch: 264 [2560/5999 (43%)]	Loss: 0.019123
Epoch: 264 [5120/5999 (85%)]	Loss: 0.016331
saving model at:264,7.798708911286667e-05
====> Epoch: 264 Average loss: 0.000237 
Epoch: 265 [0/5999 (0%)]	Loss: 0.055533
Epoch: 265 [2560/5999 (43%)]	Loss: 0.015861
Epoch: 265 [5120/5999 (85%)]	Loss: 0.035903
====> Epoch: 265 Average loss: 0.000342 
Epoch: 266 [0/5999 (0%)]	Loss: 0.021582
Epoch: 266 [2560/5999 (43%)]	Loss: 0.051044
Epoch: 266 [5120/5999 (85%)]	Loss: 0.024774
====> Epoch: 266 Average loss: 0.000300 
Epoch: 267 [0/5999 (0%)]	Loss: 0.038758
Epoch: 267 [2560/5999 (43%)]	Loss: 0.032515
Epoch: 267 [5120/5999 (85%)]	Loss: 0.014881
====> Epoch: 267 Average loss: 0.000304 
Epoch: 268 [0/5999 (0%)]	Loss: 0.018175
Epoch: 268 [2560/5999 (43%)]	Loss: 0.014337
Epoch: 268 [5120/5999 (85%)]	Loss: 0.080192
====> Epoch: 268 Average loss: 0.000210 
Epoch: 269 [0/5999 (0%)]	Loss: 0.038714
Epoch: 269 [2560/5999 (43%)]	Loss: 0.040623
Epoch: 269 [5120/5999 (85%)]	Loss: 0.049994
====> Epoch: 269 Average loss: 0.000275 
Epoch: 270 [0/5999 (0%)]	Loss: 0.012413
Epoch: 270 [2560/5999 (43%)]	Loss: 0.067711
Epoch: 270 [5120/5999 (85%)]	Loss: 0.055439
====> Epoch: 270 Average loss: 0.000344 
Epoch: 271 [0/5999 (0%)]	Loss: 0.053834
Epoch: 271 [2560/5999 (43%)]	Loss: 0.052169
Epoch: 271 [5120/5999 (85%)]	Loss: 0.026792
====> Epoch: 271 Average loss: 0.000261 
Epoch: 272 [0/5999 (0%)]	Loss: 0.042360
Epoch: 272 [2560/5999 (43%)]	Loss: 0.017352
Epoch: 272 [5120/5999 (85%)]	Loss: 0.031882
====> Epoch: 272 Average loss: 0.000233 
Epoch: 273 [0/5999 (0%)]	Loss: 0.037309
Epoch: 273 [2560/5999 (43%)]	Loss: 0.069973
Epoch: 273 [5120/5999 (85%)]	Loss: 0.013600
saving model at:273,7.550381420878694e-05
====> Epoch: 273 Average loss: 0.000255 
Epoch: 274 [0/5999 (0%)]	Loss: 0.022148
Epoch: 274 [2560/5999 (43%)]	Loss: 0.011244
saving model at:274,7.39018086460419e-05
Epoch: 274 [5120/5999 (85%)]	Loss: 0.038229
====> Epoch: 274 Average loss: 0.000250 
Epoch: 275 [0/5999 (0%)]	Loss: 0.037931
Epoch: 275 [2560/5999 (43%)]	Loss: 0.023658
Epoch: 275 [5120/5999 (85%)]	Loss: 0.040508
====> Epoch: 275 Average loss: 0.000263 
Epoch: 276 [0/5999 (0%)]	Loss: 0.045780
Epoch: 276 [2560/5999 (43%)]	Loss: 0.148016
Epoch: 276 [5120/5999 (85%)]	Loss: 0.051605
====> Epoch: 276 Average loss: 0.000339 
Epoch: 277 [0/5999 (0%)]	Loss: 0.023033
Epoch: 277 [2560/5999 (43%)]	Loss: 0.015260
Epoch: 277 [5120/5999 (85%)]	Loss: 0.019193
====> Epoch: 277 Average loss: 0.000314 
Epoch: 278 [0/5999 (0%)]	Loss: 0.035202
Epoch: 278 [2560/5999 (43%)]	Loss: 0.033856
Epoch: 278 [5120/5999 (85%)]	Loss: 0.088989
====> Epoch: 278 Average loss: 0.000352 
Epoch: 279 [0/5999 (0%)]	Loss: 0.024414
Epoch: 279 [2560/5999 (43%)]	Loss: 0.018728
Epoch: 279 [5120/5999 (85%)]	Loss: 0.019771
====> Epoch: 279 Average loss: 0.000243 
Epoch: 280 [0/5999 (0%)]	Loss: 0.020290
Epoch: 280 [2560/5999 (43%)]	Loss: 0.024216
Epoch: 280 [5120/5999 (85%)]	Loss: 0.090536
====> Epoch: 280 Average loss: 0.000276 
Epoch: 281 [0/5999 (0%)]	Loss: 0.036311
Epoch: 281 [2560/5999 (43%)]	Loss: 0.074278
Epoch: 281 [5120/5999 (85%)]	Loss: 0.079983
====> Epoch: 281 Average loss: 0.000459 
Epoch: 282 [0/5999 (0%)]	Loss: 0.020501
Epoch: 282 [2560/5999 (43%)]	Loss: 0.013209
Epoch: 282 [5120/5999 (85%)]	Loss: 0.128833
====> Epoch: 282 Average loss: 0.000318 
Epoch: 283 [0/5999 (0%)]	Loss: 0.029706
Epoch: 283 [2560/5999 (43%)]	Loss: 0.041161
Epoch: 283 [5120/5999 (85%)]	Loss: 0.050800
====> Epoch: 283 Average loss: 0.000308 
Epoch: 284 [0/5999 (0%)]	Loss: 0.029941
Epoch: 284 [2560/5999 (43%)]	Loss: 0.023911
Epoch: 284 [5120/5999 (85%)]	Loss: 0.078411
====> Epoch: 284 Average loss: 0.000343 
Epoch: 285 [0/5999 (0%)]	Loss: 0.064055
Epoch: 285 [2560/5999 (43%)]	Loss: 0.029222
Epoch: 285 [5120/5999 (85%)]	Loss: 0.057855
====> Epoch: 285 Average loss: 0.000318 
Epoch: 286 [0/5999 (0%)]	Loss: 0.016885
Epoch: 286 [2560/5999 (43%)]	Loss: 0.020228
Epoch: 286 [5120/5999 (85%)]	Loss: 0.027244
====> Epoch: 286 Average loss: 0.000307 
Epoch: 287 [0/5999 (0%)]	Loss: 0.080098
Epoch: 287 [2560/5999 (43%)]	Loss: 0.168826
Epoch: 287 [5120/5999 (85%)]	Loss: 0.022583
====> Epoch: 287 Average loss: 0.000456 
Epoch: 288 [0/5999 (0%)]	Loss: 0.045226
Epoch: 288 [2560/5999 (43%)]	Loss: 0.148070
Epoch: 288 [5120/5999 (85%)]	Loss: 0.027741
====> Epoch: 288 Average loss: 0.000415 
Epoch: 289 [0/5999 (0%)]	Loss: 0.019394
Epoch: 289 [2560/5999 (43%)]	Loss: 0.019445
Epoch: 289 [5120/5999 (85%)]	Loss: 0.139490
====> Epoch: 289 Average loss: 0.000329 
Epoch: 290 [0/5999 (0%)]	Loss: 0.033852
Epoch: 290 [2560/5999 (43%)]	Loss: 0.026254
Epoch: 290 [5120/5999 (85%)]	Loss: 0.033846
====> Epoch: 290 Average loss: 0.000444 
Epoch: 291 [0/5999 (0%)]	Loss: 0.022094
Epoch: 291 [2560/5999 (43%)]	Loss: 0.019992
Epoch: 291 [5120/5999 (85%)]	Loss: 0.050988
====> Epoch: 291 Average loss: 0.000365 
Epoch: 292 [0/5999 (0%)]	Loss: 0.021281
Epoch: 292 [2560/5999 (43%)]	Loss: 0.024254
Epoch: 292 [5120/5999 (85%)]	Loss: 0.063918
====> Epoch: 292 Average loss: 0.000311 
Epoch: 293 [0/5999 (0%)]	Loss: 0.030719
Epoch: 293 [2560/5999 (43%)]	Loss: 0.021466
Epoch: 293 [5120/5999 (85%)]	Loss: 0.068248
====> Epoch: 293 Average loss: 0.000305 
Epoch: 294 [0/5999 (0%)]	Loss: 0.071630
Epoch: 294 [2560/5999 (43%)]	Loss: 0.037958
Epoch: 294 [5120/5999 (85%)]	Loss: 0.047480
====> Epoch: 294 Average loss: 0.000350 
Epoch: 295 [0/5999 (0%)]	Loss: 0.031513
Epoch: 295 [2560/5999 (43%)]	Loss: 0.016865
Epoch: 295 [5120/5999 (85%)]	Loss: 0.017739
====> Epoch: 295 Average loss: 0.000262 
Epoch: 296 [0/5999 (0%)]	Loss: 0.024838
Epoch: 296 [2560/5999 (43%)]	Loss: 0.018616
Epoch: 296 [5120/5999 (85%)]	Loss: 0.028545
====> Epoch: 296 Average loss: 0.000269 
Epoch: 297 [0/5999 (0%)]	Loss: 0.112545
Epoch: 297 [2560/5999 (43%)]	Loss: 0.025089
Epoch: 297 [5120/5999 (85%)]	Loss: 0.011886
====> Epoch: 297 Average loss: 0.000231 
Epoch: 298 [0/5999 (0%)]	Loss: 0.015428
Epoch: 298 [2560/5999 (43%)]	Loss: 0.015890
Epoch: 298 [5120/5999 (85%)]	Loss: 0.030511
====> Epoch: 298 Average loss: 0.000195 
Epoch: 299 [0/5999 (0%)]	Loss: 0.013595
Epoch: 299 [2560/5999 (43%)]	Loss: 0.036334
Epoch: 299 [5120/5999 (85%)]	Loss: 0.015202
====> Epoch: 299 Average loss: 0.000233 
Epoch: 300 [0/5999 (0%)]	Loss: 0.032360
Epoch: 300 [2560/5999 (43%)]	Loss: 0.020728
Epoch: 300 [5120/5999 (85%)]	Loss: 0.021192
====> Epoch: 300 Average loss: 0.000412 
Epoch: 301 [0/5999 (0%)]	Loss: 0.015620
Epoch: 301 [2560/5999 (43%)]	Loss: 0.014227
Epoch: 301 [5120/5999 (85%)]	Loss: 0.019935
====> Epoch: 301 Average loss: 0.000227 
Epoch: 302 [0/5999 (0%)]	Loss: 0.026227
Epoch: 302 [2560/5999 (43%)]	Loss: 0.018702
Epoch: 302 [5120/5999 (85%)]	Loss: 0.017510
====> Epoch: 302 Average loss: 0.000255 
Epoch: 303 [0/5999 (0%)]	Loss: 0.022967
Epoch: 303 [2560/5999 (43%)]	Loss: 0.017662
Epoch: 303 [5120/5999 (85%)]	Loss: 0.018583
====> Epoch: 303 Average loss: 0.000226 
Epoch: 304 [0/5999 (0%)]	Loss: 0.050414
Epoch: 304 [2560/5999 (43%)]	Loss: 0.025824
Epoch: 304 [5120/5999 (85%)]	Loss: 0.026272
====> Epoch: 304 Average loss: 0.000252 
Epoch: 305 [0/5999 (0%)]	Loss: 0.022657
Epoch: 305 [2560/5999 (43%)]	Loss: 0.022335
Epoch: 305 [5120/5999 (85%)]	Loss: 0.019243
====> Epoch: 305 Average loss: 0.000241 
Epoch: 306 [0/5999 (0%)]	Loss: 0.015350
Epoch: 306 [2560/5999 (43%)]	Loss: 0.056243
Epoch: 306 [5120/5999 (85%)]	Loss: 0.019921
====> Epoch: 306 Average loss: 0.000211 
Epoch: 307 [0/5999 (0%)]	Loss: 0.019276
Epoch: 307 [2560/5999 (43%)]	Loss: 0.049512
Epoch: 307 [5120/5999 (85%)]	Loss: 0.022718
====> Epoch: 307 Average loss: 0.000250 
Epoch: 308 [0/5999 (0%)]	Loss: 0.082454
Epoch: 308 [2560/5999 (43%)]	Loss: 0.079430
Epoch: 308 [5120/5999 (85%)]	Loss: 0.021374
====> Epoch: 308 Average loss: 0.000290 
Epoch: 309 [0/5999 (0%)]	Loss: 0.012614
Epoch: 309 [2560/5999 (43%)]	Loss: 0.113671
Epoch: 309 [5120/5999 (85%)]	Loss: 0.045939
====> Epoch: 309 Average loss: 0.000305 
Epoch: 310 [0/5999 (0%)]	Loss: 0.025531
Epoch: 310 [2560/5999 (43%)]	Loss: 0.017289
Epoch: 310 [5120/5999 (85%)]	Loss: 0.039435
====> Epoch: 310 Average loss: 0.000250 
Epoch: 311 [0/5999 (0%)]	Loss: 0.025099
Epoch: 311 [2560/5999 (43%)]	Loss: 0.031740
Epoch: 311 [5120/5999 (85%)]	Loss: 0.020762
====> Epoch: 311 Average loss: 0.000231 
Epoch: 312 [0/5999 (0%)]	Loss: 0.047854
Epoch: 312 [2560/5999 (43%)]	Loss: 0.016521
Epoch: 312 [5120/5999 (85%)]	Loss: 0.019548
====> Epoch: 312 Average loss: 0.000274 
Epoch: 313 [0/5999 (0%)]	Loss: 0.014922
Epoch: 313 [2560/5999 (43%)]	Loss: 0.068678
Epoch: 313 [5120/5999 (85%)]	Loss: 0.010190
saving model at:313,7.09207592299208e-05
====> Epoch: 313 Average loss: 0.000188 
Epoch: 314 [0/5999 (0%)]	Loss: 0.024331
Epoch: 314 [2560/5999 (43%)]	Loss: 0.070580
Epoch: 314 [5120/5999 (85%)]	Loss: 0.012930
====> Epoch: 314 Average loss: 0.000240 
Epoch: 315 [0/5999 (0%)]	Loss: 0.030751
Epoch: 315 [2560/5999 (43%)]	Loss: 0.066610
Epoch: 315 [5120/5999 (85%)]	Loss: 0.020195
====> Epoch: 315 Average loss: 0.000256 
Epoch: 316 [0/5999 (0%)]	Loss: 0.030160
Epoch: 316 [2560/5999 (43%)]	Loss: 0.035316
Epoch: 316 [5120/5999 (85%)]	Loss: 0.018125
====> Epoch: 316 Average loss: 0.000277 
Epoch: 317 [0/5999 (0%)]	Loss: 0.017994
Epoch: 317 [2560/5999 (43%)]	Loss: 0.029896
Epoch: 317 [5120/5999 (85%)]	Loss: 0.036504
====> Epoch: 317 Average loss: 0.000251 
Epoch: 318 [0/5999 (0%)]	Loss: 0.025201
Epoch: 318 [2560/5999 (43%)]	Loss: 0.045754
Epoch: 318 [5120/5999 (85%)]	Loss: 0.038217
====> Epoch: 318 Average loss: 0.000278 
Epoch: 319 [0/5999 (0%)]	Loss: 0.014447
Epoch: 319 [2560/5999 (43%)]	Loss: 0.015695
Epoch: 319 [5120/5999 (85%)]	Loss: 0.027875
====> Epoch: 319 Average loss: 0.000329 
Epoch: 320 [0/5999 (0%)]	Loss: 0.035770
Epoch: 320 [2560/5999 (43%)]	Loss: 0.020381
Epoch: 320 [5120/5999 (85%)]	Loss: 0.022142
====> Epoch: 320 Average loss: 0.000240 
Epoch: 321 [0/5999 (0%)]	Loss: 0.093271
Epoch: 321 [2560/5999 (43%)]	Loss: 0.020753
Epoch: 321 [5120/5999 (85%)]	Loss: 0.034937
====> Epoch: 321 Average loss: 0.000234 
Epoch: 322 [0/5999 (0%)]	Loss: 0.021262
Epoch: 322 [2560/5999 (43%)]	Loss: 0.063184
Epoch: 322 [5120/5999 (85%)]	Loss: 0.016728
====> Epoch: 322 Average loss: 0.000292 
Epoch: 323 [0/5999 (0%)]	Loss: 0.059968
Epoch: 323 [2560/5999 (43%)]	Loss: 0.031619
Epoch: 323 [5120/5999 (85%)]	Loss: 0.027689
====> Epoch: 323 Average loss: 0.000301 
Epoch: 324 [0/5999 (0%)]	Loss: 0.046379
Epoch: 324 [2560/5999 (43%)]	Loss: 0.021597
Epoch: 324 [5120/5999 (85%)]	Loss: 0.045685
====> Epoch: 324 Average loss: 0.000222 
Epoch: 325 [0/5999 (0%)]	Loss: 0.058222
Epoch: 325 [2560/5999 (43%)]	Loss: 0.023817
Epoch: 325 [5120/5999 (85%)]	Loss: 0.065848
====> Epoch: 325 Average loss: 0.000262 
Epoch: 326 [0/5999 (0%)]	Loss: 0.033660
Epoch: 326 [2560/5999 (43%)]	Loss: 0.035920
Epoch: 326 [5120/5999 (85%)]	Loss: 0.013472
====> Epoch: 326 Average loss: 0.000256 
Epoch: 327 [0/5999 (0%)]	Loss: 0.024549
Epoch: 327 [2560/5999 (43%)]	Loss: 0.079425
Epoch: 327 [5120/5999 (85%)]	Loss: 0.045786
====> Epoch: 327 Average loss: 0.000436 
Epoch: 328 [0/5999 (0%)]	Loss: 0.117111
Epoch: 328 [2560/5999 (43%)]	Loss: 0.037939
Epoch: 328 [5120/5999 (85%)]	Loss: 0.061115
====> Epoch: 328 Average loss: 0.000304 
Epoch: 329 [0/5999 (0%)]	Loss: 0.108968
Epoch: 329 [2560/5999 (43%)]	Loss: 0.029138
Epoch: 329 [5120/5999 (85%)]	Loss: 0.132035
====> Epoch: 329 Average loss: 0.000275 
Epoch: 330 [0/5999 (0%)]	Loss: 0.011458
Epoch: 330 [2560/5999 (43%)]	Loss: 0.030553
Epoch: 330 [5120/5999 (85%)]	Loss: 0.047776
====> Epoch: 330 Average loss: 0.000309 
Epoch: 331 [0/5999 (0%)]	Loss: 0.107724
Epoch: 331 [2560/5999 (43%)]	Loss: 0.030859
Epoch: 331 [5120/5999 (85%)]	Loss: 0.031836
====> Epoch: 331 Average loss: 0.000307 
Epoch: 332 [0/5999 (0%)]	Loss: 0.039239
Epoch: 332 [2560/5999 (43%)]	Loss: 0.026093
Epoch: 332 [5120/5999 (85%)]	Loss: 0.039830
====> Epoch: 332 Average loss: 0.000300 
Epoch: 333 [0/5999 (0%)]	Loss: 0.012826
Epoch: 333 [2560/5999 (43%)]	Loss: 0.017998
Epoch: 333 [5120/5999 (85%)]	Loss: 0.018482
====> Epoch: 333 Average loss: 0.000234 
Epoch: 334 [0/5999 (0%)]	Loss: 0.056363
Epoch: 334 [2560/5999 (43%)]	Loss: 0.058612
Epoch: 334 [5120/5999 (85%)]	Loss: 0.032685
====> Epoch: 334 Average loss: 0.000269 
Epoch: 335 [0/5999 (0%)]	Loss: 0.035715
Epoch: 335 [2560/5999 (43%)]	Loss: 0.019468
Epoch: 335 [5120/5999 (85%)]	Loss: 0.048609
====> Epoch: 335 Average loss: 0.000215 
Epoch: 336 [0/5999 (0%)]	Loss: 0.014677
Epoch: 336 [2560/5999 (43%)]	Loss: 0.175957
Epoch: 336 [5120/5999 (85%)]	Loss: 0.121600
====> Epoch: 336 Average loss: 0.000414 
Epoch: 337 [0/5999 (0%)]	Loss: 0.036056
Epoch: 337 [2560/5999 (43%)]	Loss: 0.024752
Epoch: 337 [5120/5999 (85%)]	Loss: 0.028817
====> Epoch: 337 Average loss: 0.000367 
Epoch: 338 [0/5999 (0%)]	Loss: 0.053367
Epoch: 338 [2560/5999 (43%)]	Loss: 0.019340
Epoch: 338 [5120/5999 (85%)]	Loss: 0.027560
====> Epoch: 338 Average loss: 0.000279 
Epoch: 339 [0/5999 (0%)]	Loss: 0.031851
Epoch: 339 [2560/5999 (43%)]	Loss: 0.062833
Epoch: 339 [5120/5999 (85%)]	Loss: 0.013902
====> Epoch: 339 Average loss: 0.000232 
Epoch: 340 [0/5999 (0%)]	Loss: 0.039435
Epoch: 340 [2560/5999 (43%)]	Loss: 0.147034
Epoch: 340 [5120/5999 (85%)]	Loss: 0.036403
====> Epoch: 340 Average loss: 0.000321 
Epoch: 341 [0/5999 (0%)]	Loss: 0.027089
Epoch: 341 [2560/5999 (43%)]	Loss: 0.035475
Epoch: 341 [5120/5999 (85%)]	Loss: 0.018921
====> Epoch: 341 Average loss: 0.000272 
Epoch: 342 [0/5999 (0%)]	Loss: 0.051824
Epoch: 342 [2560/5999 (43%)]	Loss: 0.038545
Epoch: 342 [5120/5999 (85%)]	Loss: 0.013995
====> Epoch: 342 Average loss: 0.000263 
Epoch: 343 [0/5999 (0%)]	Loss: 0.029479
Epoch: 343 [2560/5999 (43%)]	Loss: 0.014529
Epoch: 343 [5120/5999 (85%)]	Loss: 0.102554
====> Epoch: 343 Average loss: 0.000270 
Epoch: 344 [0/5999 (0%)]	Loss: 0.021861
Epoch: 344 [2560/5999 (43%)]	Loss: 0.013464
Epoch: 344 [5120/5999 (85%)]	Loss: 0.016004
====> Epoch: 344 Average loss: 0.000189 
Epoch: 345 [0/5999 (0%)]	Loss: 0.018185
Epoch: 345 [2560/5999 (43%)]	Loss: 0.028842
Epoch: 345 [5120/5999 (85%)]	Loss: 0.016251
====> Epoch: 345 Average loss: 0.000237 
Epoch: 346 [0/5999 (0%)]	Loss: 0.042279
Epoch: 346 [2560/5999 (43%)]	Loss: 0.013690
Epoch: 346 [5120/5999 (85%)]	Loss: 0.031791
====> Epoch: 346 Average loss: 0.000187 
Epoch: 347 [0/5999 (0%)]	Loss: 0.045411
Epoch: 347 [2560/5999 (43%)]	Loss: 0.043008
Epoch: 347 [5120/5999 (85%)]	Loss: 0.013405
====> Epoch: 347 Average loss: 0.000212 
Epoch: 348 [0/5999 (0%)]	Loss: 0.014519
Epoch: 348 [2560/5999 (43%)]	Loss: 0.061012
Epoch: 348 [5120/5999 (85%)]	Loss: 0.021206
====> Epoch: 348 Average loss: 0.000205 
Epoch: 349 [0/5999 (0%)]	Loss: 0.036972
Epoch: 349 [2560/5999 (43%)]	Loss: 0.014486
Epoch: 349 [5120/5999 (85%)]	Loss: 0.035633
====> Epoch: 349 Average loss: 0.000260 
Epoch: 350 [0/5999 (0%)]	Loss: 0.090655
Epoch: 350 [2560/5999 (43%)]	Loss: 0.020606
Epoch: 350 [5120/5999 (85%)]	Loss: 0.050416
====> Epoch: 350 Average loss: 0.000393 
Epoch: 351 [0/5999 (0%)]	Loss: 0.064557
Epoch: 351 [2560/5999 (43%)]	Loss: 0.012488
Epoch: 351 [5120/5999 (85%)]	Loss: 0.013424
====> Epoch: 351 Average loss: 0.000199 
Epoch: 352 [0/5999 (0%)]	Loss: 0.016091
Epoch: 352 [2560/5999 (43%)]	Loss: 0.031017
Epoch: 352 [5120/5999 (85%)]	Loss: 0.036834
====> Epoch: 352 Average loss: 0.000280 
Epoch: 353 [0/5999 (0%)]	Loss: 0.028724
Epoch: 353 [2560/5999 (43%)]	Loss: 0.028186
Epoch: 353 [5120/5999 (85%)]	Loss: 0.011843
====> Epoch: 353 Average loss: 0.000251 
Epoch: 354 [0/5999 (0%)]	Loss: 0.034015
Epoch: 354 [2560/5999 (43%)]	Loss: 0.030145
Epoch: 354 [5120/5999 (85%)]	Loss: 0.025115
====> Epoch: 354 Average loss: 0.000229 
Epoch: 355 [0/5999 (0%)]	Loss: 0.042371
Epoch: 355 [2560/5999 (43%)]	Loss: 0.022139
Epoch: 355 [5120/5999 (85%)]	Loss: 0.021540
saving model at:355,6.255476683145388e-05
====> Epoch: 355 Average loss: 0.000201 
Epoch: 356 [0/5999 (0%)]	Loss: 0.037555
Epoch: 356 [2560/5999 (43%)]	Loss: 0.019233
Epoch: 356 [5120/5999 (85%)]	Loss: 0.013208
====> Epoch: 356 Average loss: 0.000186 
Epoch: 357 [0/5999 (0%)]	Loss: 0.019667
Epoch: 357 [2560/5999 (43%)]	Loss: 0.030841
Epoch: 357 [5120/5999 (85%)]	Loss: 0.023189
====> Epoch: 357 Average loss: 0.000231 
Epoch: 358 [0/5999 (0%)]	Loss: 0.013616
Epoch: 358 [2560/5999 (43%)]	Loss: 0.016338
Epoch: 358 [5120/5999 (85%)]	Loss: 0.095768
====> Epoch: 358 Average loss: 0.000275 
Epoch: 359 [0/5999 (0%)]	Loss: 0.075281
Epoch: 359 [2560/5999 (43%)]	Loss: 0.033051
Epoch: 359 [5120/5999 (85%)]	Loss: 0.024972
====> Epoch: 359 Average loss: 0.000249 
Epoch: 360 [0/5999 (0%)]	Loss: 0.022028
Epoch: 360 [2560/5999 (43%)]	Loss: 0.030815
Epoch: 360 [5120/5999 (85%)]	Loss: 0.040427
====> Epoch: 360 Average loss: 0.000271 
Epoch: 361 [0/5999 (0%)]	Loss: 0.016168
Epoch: 361 [2560/5999 (43%)]	Loss: 0.045620
Epoch: 361 [5120/5999 (85%)]	Loss: 0.043824
====> Epoch: 361 Average loss: 0.000435 
Epoch: 362 [0/5999 (0%)]	Loss: 0.026621
Epoch: 362 [2560/5999 (43%)]	Loss: 0.012097
Epoch: 362 [5120/5999 (85%)]	Loss: 0.083280
====> Epoch: 362 Average loss: 0.000314 
Epoch: 363 [0/5999 (0%)]	Loss: 0.017774
Epoch: 363 [2560/5999 (43%)]	Loss: 0.014447
Epoch: 363 [5120/5999 (85%)]	Loss: 0.040599
====> Epoch: 363 Average loss: 0.000276 
Epoch: 364 [0/5999 (0%)]	Loss: 0.017747
Epoch: 364 [2560/5999 (43%)]	Loss: 0.014928
Epoch: 364 [5120/5999 (85%)]	Loss: 0.019548
====> Epoch: 364 Average loss: 0.000221 
Epoch: 365 [0/5999 (0%)]	Loss: 0.014120
Epoch: 365 [2560/5999 (43%)]	Loss: 0.022288
Epoch: 365 [5120/5999 (85%)]	Loss: 0.026032
====> Epoch: 365 Average loss: 0.000279 
Epoch: 366 [0/5999 (0%)]	Loss: 0.045547
Epoch: 366 [2560/5999 (43%)]	Loss: 0.016948
Epoch: 366 [5120/5999 (85%)]	Loss: 0.015962
====> Epoch: 366 Average loss: 0.000216 
Epoch: 367 [0/5999 (0%)]	Loss: 0.014471
Epoch: 367 [2560/5999 (43%)]	Loss: 0.104477
Epoch: 367 [5120/5999 (85%)]	Loss: 0.028571
====> Epoch: 367 Average loss: 0.000251 
Epoch: 368 [0/5999 (0%)]	Loss: 0.035525
Epoch: 368 [2560/5999 (43%)]	Loss: 0.050076
Epoch: 368 [5120/5999 (85%)]	Loss: 0.030187
====> Epoch: 368 Average loss: 0.000234 
Epoch: 369 [0/5999 (0%)]	Loss: 0.023999
Epoch: 369 [2560/5999 (43%)]	Loss: 0.024062
Epoch: 369 [5120/5999 (85%)]	Loss: 0.018870
====> Epoch: 369 Average loss: 0.000206 
Epoch: 370 [0/5999 (0%)]	Loss: 0.021353
Epoch: 370 [2560/5999 (43%)]	Loss: 0.023584
Epoch: 370 [5120/5999 (85%)]	Loss: 0.019288
====> Epoch: 370 Average loss: 0.000317 
Epoch: 371 [0/5999 (0%)]	Loss: 0.019208
Epoch: 371 [2560/5999 (43%)]	Loss: 0.046899
Epoch: 371 [5120/5999 (85%)]	Loss: 0.027224
====> Epoch: 371 Average loss: 0.000212 
Epoch: 372 [0/5999 (0%)]	Loss: 0.021051
Epoch: 372 [2560/5999 (43%)]	Loss: 0.027319
Epoch: 372 [5120/5999 (85%)]	Loss: 0.016080
====> Epoch: 372 Average loss: 0.000213 
Epoch: 373 [0/5999 (0%)]	Loss: 0.038509
Epoch: 373 [2560/5999 (43%)]	Loss: 0.022005
Epoch: 373 [5120/5999 (85%)]	Loss: 0.026530
====> Epoch: 373 Average loss: 0.000198 
Epoch: 374 [0/5999 (0%)]	Loss: 0.016936
Epoch: 374 [2560/5999 (43%)]	Loss: 0.018161
Epoch: 374 [5120/5999 (85%)]	Loss: 0.020734
====> Epoch: 374 Average loss: 0.000212 
Epoch: 375 [0/5999 (0%)]	Loss: 0.044235
Epoch: 375 [2560/5999 (43%)]	Loss: 0.043906
Epoch: 375 [5120/5999 (85%)]	Loss: 0.039780
====> Epoch: 375 Average loss: 0.000254 
Epoch: 376 [0/5999 (0%)]	Loss: 0.064296
Epoch: 376 [2560/5999 (43%)]	Loss: 0.019106
Epoch: 376 [5120/5999 (85%)]	Loss: 0.013792
saving model at:376,5.967825843254104e-05
====> Epoch: 376 Average loss: 0.000207 
Epoch: 377 [0/5999 (0%)]	Loss: 0.014247
Epoch: 377 [2560/5999 (43%)]	Loss: 0.024454
Epoch: 377 [5120/5999 (85%)]	Loss: 0.024850
====> Epoch: 377 Average loss: 0.000177 
Epoch: 378 [0/5999 (0%)]	Loss: 0.029631
Epoch: 378 [2560/5999 (43%)]	Loss: 0.026478
Epoch: 378 [5120/5999 (85%)]	Loss: 0.070884
====> Epoch: 378 Average loss: 0.000329 
Epoch: 379 [0/5999 (0%)]	Loss: 0.016836
Epoch: 379 [2560/5999 (43%)]	Loss: 0.012191
Epoch: 379 [5120/5999 (85%)]	Loss: 0.027968
====> Epoch: 379 Average loss: 0.000196 
Epoch: 380 [0/5999 (0%)]	Loss: 0.015037
Epoch: 380 [2560/5999 (43%)]	Loss: 0.021695
Epoch: 380 [5120/5999 (85%)]	Loss: 0.021687
====> Epoch: 380 Average loss: 0.000225 
Epoch: 381 [0/5999 (0%)]	Loss: 0.023773
Epoch: 381 [2560/5999 (43%)]	Loss: 0.027169
Epoch: 381 [5120/5999 (85%)]	Loss: 0.023386
====> Epoch: 381 Average loss: 0.000233 
Epoch: 382 [0/5999 (0%)]	Loss: 0.015851
Epoch: 382 [2560/5999 (43%)]	Loss: 0.039396
Epoch: 382 [5120/5999 (85%)]	Loss: 0.026109
====> Epoch: 382 Average loss: 0.000259 
Epoch: 383 [0/5999 (0%)]	Loss: 0.019507
Epoch: 383 [2560/5999 (43%)]	Loss: 0.036657
Epoch: 383 [5120/5999 (85%)]	Loss: 0.018646
====> Epoch: 383 Average loss: 0.000244 
Epoch: 384 [0/5999 (0%)]	Loss: 0.011777
Epoch: 384 [2560/5999 (43%)]	Loss: 0.026283
Epoch: 384 [5120/5999 (85%)]	Loss: 0.019107
====> Epoch: 384 Average loss: 0.000191 
Epoch: 385 [0/5999 (0%)]	Loss: 0.038340
Epoch: 385 [2560/5999 (43%)]	Loss: 0.016389
Epoch: 385 [5120/5999 (85%)]	Loss: 0.027758
====> Epoch: 385 Average loss: 0.000241 
Epoch: 386 [0/5999 (0%)]	Loss: 0.018721
Epoch: 386 [2560/5999 (43%)]	Loss: 0.070269
Epoch: 386 [5120/5999 (85%)]	Loss: 0.014985
====> Epoch: 386 Average loss: 0.000190 
Epoch: 387 [0/5999 (0%)]	Loss: 0.050531
Epoch: 387 [2560/5999 (43%)]	Loss: 0.024832
Epoch: 387 [5120/5999 (85%)]	Loss: 0.019762
====> Epoch: 387 Average loss: 0.000248 
Epoch: 388 [0/5999 (0%)]	Loss: 0.028868
Epoch: 388 [2560/5999 (43%)]	Loss: 0.034854
Epoch: 388 [5120/5999 (85%)]	Loss: 0.037552
====> Epoch: 388 Average loss: 0.000348 
Epoch: 389 [0/5999 (0%)]	Loss: 0.029617
Epoch: 389 [2560/5999 (43%)]	Loss: 0.056000
Epoch: 389 [5120/5999 (85%)]	Loss: 0.048760
====> Epoch: 389 Average loss: 0.000287 
Epoch: 390 [0/5999 (0%)]	Loss: 0.028360
Epoch: 390 [2560/5999 (43%)]	Loss: 0.059920
Epoch: 390 [5120/5999 (85%)]	Loss: 0.017550
====> Epoch: 390 Average loss: 0.000211 
Epoch: 391 [0/5999 (0%)]	Loss: 0.009655
Epoch: 391 [2560/5999 (43%)]	Loss: 0.019170
Epoch: 391 [5120/5999 (85%)]	Loss: 0.016341
====> Epoch: 391 Average loss: 0.000207 
Epoch: 392 [0/5999 (0%)]	Loss: 0.026668
Epoch: 392 [2560/5999 (43%)]	Loss: 0.019538
Epoch: 392 [5120/5999 (85%)]	Loss: 0.057615
====> Epoch: 392 Average loss: 0.000288 
Epoch: 393 [0/5999 (0%)]	Loss: 0.031292
Epoch: 393 [2560/5999 (43%)]	Loss: 0.018590
Epoch: 393 [5120/5999 (85%)]	Loss: 0.013511
====> Epoch: 393 Average loss: 0.000287 
Epoch: 394 [0/5999 (0%)]	Loss: 0.023703
Epoch: 394 [2560/5999 (43%)]	Loss: 0.028297
Epoch: 394 [5120/5999 (85%)]	Loss: 0.016729
====> Epoch: 394 Average loss: 0.000227 
Epoch: 395 [0/5999 (0%)]	Loss: 0.037056
Epoch: 395 [2560/5999 (43%)]	Loss: 0.047583
Epoch: 395 [5120/5999 (85%)]	Loss: 0.029549
====> Epoch: 395 Average loss: 0.000179 
Epoch: 396 [0/5999 (0%)]	Loss: 0.021432
Epoch: 396 [2560/5999 (43%)]	Loss: 0.091088
Epoch: 396 [5120/5999 (85%)]	Loss: 0.028288
====> Epoch: 396 Average loss: 0.000295 
Epoch: 397 [0/5999 (0%)]	Loss: 0.102928
Epoch: 397 [2560/5999 (43%)]	Loss: 0.054843
Epoch: 397 [5120/5999 (85%)]	Loss: 0.041175
====> Epoch: 397 Average loss: 0.000264 
Epoch: 398 [0/5999 (0%)]	Loss: 0.036146
Epoch: 398 [2560/5999 (43%)]	Loss: 0.016261
Epoch: 398 [5120/5999 (85%)]	Loss: 0.056085
====> Epoch: 398 Average loss: 0.000194 
Epoch: 399 [0/5999 (0%)]	Loss: 0.015257
Epoch: 399 [2560/5999 (43%)]	Loss: 0.012272
Epoch: 399 [5120/5999 (85%)]	Loss: 0.021338
====> Epoch: 399 Average loss: 0.000221 
Epoch: 400 [0/5999 (0%)]	Loss: 0.026442
Epoch: 400 [2560/5999 (43%)]	Loss: 0.044912
Epoch: 400 [5120/5999 (85%)]	Loss: 0.012996
====> Epoch: 400 Average loss: 0.000301 
Reconstruction Loss 5.967825854895637e-05
per_obj_mse: ['5.85215566388797e-05', '6.083497282816097e-05']
Reconstruction Loss 5.861034970485366e-05
per_obj_mse: ['5.925587174715474e-05', '5.796482946607284e-05']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_3_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_3_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_3_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_3_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0001831598897697404
per_obj_mse: ['9.644131205277517e-05', '0.0002698784810490906']
Reconstruction Loss 0.00017274758906516918
per_obj_mse: ['9.125987708102912e-05', '0.0002542352885939181']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_3_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_3_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0001831598897697404
per_obj_mse: ['9.644131205277517e-05', '0.0002698784810490906']
Reconstruction Loss 0.00017274758906516918
per_obj_mse: ['9.125987708102912e-05', '0.0002542352885939181']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_3_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=20, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_3_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7997, 2, 11)
(1997, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0010923248641192913
per_obj_mse: ['0.001008390448987484', '0.0011762594804167747']
Reconstruction Loss 0.0011423213914448082
per_obj_mse: ['0.001000296208076179', '0.0012843464501202106']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_3_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=60, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_3_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7993, 2, 11)
(1993, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0036429411094782947
per_obj_mse: ['0.0035157809033989906', '0.003770101349800825']
Reconstruction Loss 0.0037575552751236804
per_obj_mse: ['0.003472082782536745', '0.004043028224259615']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_3_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=100, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_3_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7989, 2, 11)
(1989, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.006182298970383567
per_obj_mse: ['0.006046433933079243', '0.006318164989352226']
Reconstruction Loss 0.006372670064993528
per_obj_mse: ['0.00594358192756772', '0.006801759358495474']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_3_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=140, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B+SC', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_3_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=True, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7985, 2, 11)
(1985, 2, 11)
Data loaded!
MVTS(
  (control_CNN_0): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (control_CNN_1): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 1, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM_0): LSTM(11, 40)
  (control_LSTM_1): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.008723084434296079
per_obj_mse: ['0.008534024469554424', '0.008912146091461182']
Reconstruction Loss 0.008991457763074027
per_obj_mse: ['0.008417369797825813', '0.009565542452037334']
