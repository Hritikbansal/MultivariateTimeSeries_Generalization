cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_2_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=400, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=False, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/train_test_iid_alpha_0.4_0.7/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_2_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Epoch: 1 [0/5999 (0%)]	Loss: 100.491600
Epoch: 1 [2560/5999 (43%)]	Loss: 15.101169
saving model at:1,0.11037358272075654
Epoch: 1 [5120/5999 (85%)]	Loss: 10.478895
saving model at:1,0.0791343013048172
====> Epoch: 1 Average loss: 0.148940 
Epoch: 2 [0/5999 (0%)]	Loss: 9.701443
Epoch: 2 [2560/5999 (43%)]	Loss: 8.610843
saving model at:2,0.06038431960344315
Epoch: 2 [5120/5999 (85%)]	Loss: 7.298461
saving model at:2,0.05659472942352295
====> Epoch: 2 Average loss: 0.062354 
Epoch: 3 [0/5999 (0%)]	Loss: 6.944038
Epoch: 3 [2560/5999 (43%)]	Loss: 4.352105
saving model at:3,0.032715067863464356
Epoch: 3 [5120/5999 (85%)]	Loss: 3.126160
saving model at:3,0.022620346218347548
====> Epoch: 3 Average loss: 0.035260 
Epoch: 4 [0/5999 (0%)]	Loss: 2.821434
Epoch: 4 [2560/5999 (43%)]	Loss: 2.020730
saving model at:4,0.016590267464518547
Epoch: 4 [5120/5999 (85%)]	Loss: 2.004468
saving model at:4,0.014545384630560874
====> Epoch: 4 Average loss: 0.017110 
Epoch: 5 [0/5999 (0%)]	Loss: 2.260816
Epoch: 5 [2560/5999 (43%)]	Loss: 1.588748
saving model at:5,0.012455193161964416
Epoch: 5 [5120/5999 (85%)]	Loss: 1.641224
saving model at:5,0.011336458541452885
====> Epoch: 5 Average loss: 0.012998 
Epoch: 6 [0/5999 (0%)]	Loss: 1.297452
Epoch: 6 [2560/5999 (43%)]	Loss: 1.356328
saving model at:6,0.009594480983912945
Epoch: 6 [5120/5999 (85%)]	Loss: 1.097478
saving model at:6,0.008631404496729373
====> Epoch: 6 Average loss: 0.010019 
Epoch: 7 [0/5999 (0%)]	Loss: 1.125158
Epoch: 7 [2560/5999 (43%)]	Loss: 1.046481
saving model at:7,0.007731382071971893
Epoch: 7 [5120/5999 (85%)]	Loss: 1.046007
saving model at:7,0.0070567218773067
====> Epoch: 7 Average loss: 0.008386 
Epoch: 8 [0/5999 (0%)]	Loss: 0.897635
Epoch: 8 [2560/5999 (43%)]	Loss: 0.904519
saving model at:8,0.006567258182913065
Epoch: 8 [5120/5999 (85%)]	Loss: 1.058866
====> Epoch: 8 Average loss: 0.007108 
Epoch: 9 [0/5999 (0%)]	Loss: 1.099319
Epoch: 9 [2560/5999 (43%)]	Loss: 0.845446
saving model at:9,0.006184819929301739
Epoch: 9 [5120/5999 (85%)]	Loss: 0.850951
saving model at:9,0.005772695638239384
====> Epoch: 9 Average loss: 0.006389 
Epoch: 10 [0/5999 (0%)]	Loss: 0.773767
Epoch: 10 [2560/5999 (43%)]	Loss: 0.754902
saving model at:10,0.005483812797814608
Epoch: 10 [5120/5999 (85%)]	Loss: 0.703444
saving model at:10,0.005316478617489338
====> Epoch: 10 Average loss: 0.005831 
Epoch: 11 [0/5999 (0%)]	Loss: 0.679750
Epoch: 11 [2560/5999 (43%)]	Loss: 0.755118
saving model at:11,0.005140617944300175
Epoch: 11 [5120/5999 (85%)]	Loss: 0.828773
====> Epoch: 11 Average loss: 0.005570 
Epoch: 12 [0/5999 (0%)]	Loss: 0.692368
Epoch: 12 [2560/5999 (43%)]	Loss: 0.703975
saving model at:12,0.004946250803768635
Epoch: 12 [5120/5999 (85%)]	Loss: 0.771405
====> Epoch: 12 Average loss: 0.005320 
Epoch: 13 [0/5999 (0%)]	Loss: 0.870174
Epoch: 13 [2560/5999 (43%)]	Loss: 0.639039
saving model at:13,0.004851538203656674
Epoch: 13 [5120/5999 (85%)]	Loss: 0.579892
saving model at:13,0.004764775797724724
====> Epoch: 13 Average loss: 0.005387 
Epoch: 14 [0/5999 (0%)]	Loss: 0.679185
Epoch: 14 [2560/5999 (43%)]	Loss: 0.637575
saving model at:14,0.004632709667086601
Epoch: 14 [5120/5999 (85%)]	Loss: 0.598023
saving model at:14,0.004531162198632956
====> Epoch: 14 Average loss: 0.004917 
Epoch: 15 [0/5999 (0%)]	Loss: 0.592651
Epoch: 15 [2560/5999 (43%)]	Loss: 0.727480
Epoch: 15 [5120/5999 (85%)]	Loss: 0.575215
====> Epoch: 15 Average loss: 0.005357 
Epoch: 16 [0/5999 (0%)]	Loss: 0.649624
Epoch: 16 [2560/5999 (43%)]	Loss: 0.649735
saving model at:16,0.0045175299867987635
Epoch: 16 [5120/5999 (85%)]	Loss: 0.524239
saving model at:16,0.004351387921720743
====> Epoch: 16 Average loss: 0.005254 
Epoch: 17 [0/5999 (0%)]	Loss: 0.579394
Epoch: 17 [2560/5999 (43%)]	Loss: 0.560808
Epoch: 17 [5120/5999 (85%)]	Loss: 0.591220
saving model at:17,0.004180758781731128
====> Epoch: 17 Average loss: 0.004708 
Epoch: 18 [0/5999 (0%)]	Loss: 0.709717
Epoch: 18 [2560/5999 (43%)]	Loss: 0.566583
Epoch: 18 [5120/5999 (85%)]	Loss: 0.545297
====> Epoch: 18 Average loss: 0.004603 
Epoch: 19 [0/5999 (0%)]	Loss: 0.697408
Epoch: 19 [2560/5999 (43%)]	Loss: 0.479666
saving model at:19,0.0041175057291984556
Epoch: 19 [5120/5999 (85%)]	Loss: 0.532895
saving model at:19,0.0039959755167365075
====> Epoch: 19 Average loss: 0.004380 
Epoch: 20 [0/5999 (0%)]	Loss: 0.531625
Epoch: 20 [2560/5999 (43%)]	Loss: 0.536721
Epoch: 20 [5120/5999 (85%)]	Loss: 0.530518
====> Epoch: 20 Average loss: 0.004452 
Epoch: 21 [0/5999 (0%)]	Loss: 0.503419
Epoch: 21 [2560/5999 (43%)]	Loss: 0.635894
saving model at:21,0.0038634254671633243
Epoch: 21 [5120/5999 (85%)]	Loss: 0.535802
====> Epoch: 21 Average loss: 0.004341 
Epoch: 22 [0/5999 (0%)]	Loss: 0.499196
Epoch: 22 [2560/5999 (43%)]	Loss: 0.587780
saving model at:22,0.003855583921074867
Epoch: 22 [5120/5999 (85%)]	Loss: 0.492593
saving model at:22,0.00371240209788084
====> Epoch: 22 Average loss: 0.004259 
Epoch: 23 [0/5999 (0%)]	Loss: 0.512141
Epoch: 23 [2560/5999 (43%)]	Loss: 0.535526
Epoch: 23 [5120/5999 (85%)]	Loss: 0.444956
saving model at:23,0.0035504847895354032
====> Epoch: 23 Average loss: 0.003861 
Epoch: 24 [0/5999 (0%)]	Loss: 0.472126
Epoch: 24 [2560/5999 (43%)]	Loss: 0.518706
Epoch: 24 [5120/5999 (85%)]	Loss: 0.476308
====> Epoch: 24 Average loss: 0.003754 
Epoch: 25 [0/5999 (0%)]	Loss: 0.513875
Epoch: 25 [2560/5999 (43%)]	Loss: 0.428052
saving model at:25,0.003274462830275297
Epoch: 25 [5120/5999 (85%)]	Loss: 0.481291
====> Epoch: 25 Average loss: 0.003788 
Epoch: 26 [0/5999 (0%)]	Loss: 0.550223
Epoch: 26 [2560/5999 (43%)]	Loss: 0.524737
saving model at:26,0.003172258988022804
Epoch: 26 [5120/5999 (85%)]	Loss: 0.461913
====> Epoch: 26 Average loss: 0.003833 
Epoch: 27 [0/5999 (0%)]	Loss: 0.422317
Epoch: 27 [2560/5999 (43%)]	Loss: 0.391490
Epoch: 27 [5120/5999 (85%)]	Loss: 0.429710
====> Epoch: 27 Average loss: 0.003699 
Epoch: 28 [0/5999 (0%)]	Loss: 0.405953
Epoch: 28 [2560/5999 (43%)]	Loss: 0.394683
saving model at:28,0.0028542272355407475
Epoch: 28 [5120/5999 (85%)]	Loss: 0.409896
====> Epoch: 28 Average loss: 0.003273 
Epoch: 29 [0/5999 (0%)]	Loss: 0.381003
Epoch: 29 [2560/5999 (43%)]	Loss: 0.381864
Epoch: 29 [5120/5999 (85%)]	Loss: 0.397171
saving model at:29,0.0027454498279839753
====> Epoch: 29 Average loss: 0.003196 
Epoch: 30 [0/5999 (0%)]	Loss: 0.390910
Epoch: 30 [2560/5999 (43%)]	Loss: 0.504775
Epoch: 30 [5120/5999 (85%)]	Loss: 0.365628
====> Epoch: 30 Average loss: 0.003203 
Epoch: 31 [0/5999 (0%)]	Loss: 0.387784
Epoch: 31 [2560/5999 (43%)]	Loss: 0.339031
saving model at:31,0.0026443465948104857
Epoch: 31 [5120/5999 (85%)]	Loss: 0.451807
====> Epoch: 31 Average loss: 0.003002 
Epoch: 32 [0/5999 (0%)]	Loss: 0.332418
Epoch: 32 [2560/5999 (43%)]	Loss: 0.422218
Epoch: 32 [5120/5999 (85%)]	Loss: 0.385516
====> Epoch: 32 Average loss: 0.002804 
Epoch: 33 [0/5999 (0%)]	Loss: 0.341554
Epoch: 33 [2560/5999 (43%)]	Loss: 0.422878
Epoch: 33 [5120/5999 (85%)]	Loss: 0.462425
====> Epoch: 33 Average loss: 0.002978 
Epoch: 34 [0/5999 (0%)]	Loss: 0.468833
Epoch: 34 [2560/5999 (43%)]	Loss: 0.341680
Epoch: 34 [5120/5999 (85%)]	Loss: 0.341659
saving model at:34,0.002510000703856349
====> Epoch: 34 Average loss: 0.002892 
Epoch: 35 [0/5999 (0%)]	Loss: 0.359351
Epoch: 35 [2560/5999 (43%)]	Loss: 0.332732
Epoch: 35 [5120/5999 (85%)]	Loss: 0.413851
====> Epoch: 35 Average loss: 0.002792 
Epoch: 36 [0/5999 (0%)]	Loss: 0.357224
Epoch: 36 [2560/5999 (43%)]	Loss: 0.332200
saving model at:36,0.0024627608731389045
Epoch: 36 [5120/5999 (85%)]	Loss: 0.320816
====> Epoch: 36 Average loss: 0.002824 
Epoch: 37 [0/5999 (0%)]	Loss: 0.321135
Epoch: 37 [2560/5999 (43%)]	Loss: 0.319454
saving model at:37,0.002306065721437335
Epoch: 37 [5120/5999 (85%)]	Loss: 0.393859
====> Epoch: 37 Average loss: 0.002626 
Epoch: 38 [0/5999 (0%)]	Loss: 0.385419
Epoch: 38 [2560/5999 (43%)]	Loss: 0.352586
Epoch: 38 [5120/5999 (85%)]	Loss: 0.306319
====> Epoch: 38 Average loss: 0.002599 
Epoch: 39 [0/5999 (0%)]	Loss: 0.383610
Epoch: 39 [2560/5999 (43%)]	Loss: 0.337465
Epoch: 39 [5120/5999 (85%)]	Loss: 0.368327
====> Epoch: 39 Average loss: 0.002644 
Epoch: 40 [0/5999 (0%)]	Loss: 0.335824
Epoch: 40 [2560/5999 (43%)]	Loss: 0.387205
Epoch: 40 [5120/5999 (85%)]	Loss: 0.345641
saving model at:40,0.0022002363372594117
====> Epoch: 40 Average loss: 0.002667 
Epoch: 41 [0/5999 (0%)]	Loss: 0.336513
Epoch: 41 [2560/5999 (43%)]	Loss: 0.280249
saving model at:41,0.0021892195418477057
Epoch: 41 [5120/5999 (85%)]	Loss: 0.338694
saving model at:41,0.002167044445872307
====> Epoch: 41 Average loss: 0.002591 
Epoch: 42 [0/5999 (0%)]	Loss: 0.325805
Epoch: 42 [2560/5999 (43%)]	Loss: 0.314463
Epoch: 42 [5120/5999 (85%)]	Loss: 0.397418
====> Epoch: 42 Average loss: 0.002894 
Epoch: 43 [0/5999 (0%)]	Loss: 0.311767
Epoch: 43 [2560/5999 (43%)]	Loss: 0.355990
Epoch: 43 [5120/5999 (85%)]	Loss: 0.315966
====> Epoch: 43 Average loss: 0.002681 
Epoch: 44 [0/5999 (0%)]	Loss: 0.324762
Epoch: 44 [2560/5999 (43%)]	Loss: 0.296578
Epoch: 44 [5120/5999 (85%)]	Loss: 0.307946
====> Epoch: 44 Average loss: 0.002476 
Epoch: 45 [0/5999 (0%)]	Loss: 0.401710
Epoch: 45 [2560/5999 (43%)]	Loss: 0.313689
Epoch: 45 [5120/5999 (85%)]	Loss: 0.299457
saving model at:45,0.0020225888602435588
====> Epoch: 45 Average loss: 0.002500 
Epoch: 46 [0/5999 (0%)]	Loss: 0.257764
Epoch: 46 [2560/5999 (43%)]	Loss: 0.381609
Epoch: 46 [5120/5999 (85%)]	Loss: 0.289755
====> Epoch: 46 Average loss: 0.002380 
Epoch: 47 [0/5999 (0%)]	Loss: 0.282288
Epoch: 47 [2560/5999 (43%)]	Loss: 0.414030
Epoch: 47 [5120/5999 (85%)]	Loss: 0.246614
saving model at:47,0.002006966977380216
====> Epoch: 47 Average loss: 0.002378 
Epoch: 48 [0/5999 (0%)]	Loss: 0.258236
Epoch: 48 [2560/5999 (43%)]	Loss: 0.295133
Epoch: 48 [5120/5999 (85%)]	Loss: 0.279962
saving model at:48,0.00197129837423563
====> Epoch: 48 Average loss: 0.002377 
Epoch: 49 [0/5999 (0%)]	Loss: 0.297882
Epoch: 49 [2560/5999 (43%)]	Loss: 0.273804
Epoch: 49 [5120/5999 (85%)]	Loss: 0.328175
====> Epoch: 49 Average loss: 0.002415 
Epoch: 50 [0/5999 (0%)]	Loss: 0.311273
Epoch: 50 [2560/5999 (43%)]	Loss: 0.241610
saving model at:50,0.0018139261901378631
Epoch: 50 [5120/5999 (85%)]	Loss: 0.243552
saving model at:50,0.0017331277057528495
====> Epoch: 50 Average loss: 0.002255 
Epoch: 51 [0/5999 (0%)]	Loss: 0.392806
Epoch: 51 [2560/5999 (43%)]	Loss: 0.228511
Epoch: 51 [5120/5999 (85%)]	Loss: 0.276812
saving model at:51,0.0016613646456971765
====> Epoch: 51 Average loss: 0.002090 
Epoch: 52 [0/5999 (0%)]	Loss: 0.269087
Epoch: 52 [2560/5999 (43%)]	Loss: 0.262187
Epoch: 52 [5120/5999 (85%)]	Loss: 0.278019
saving model at:52,0.0015584360044449568
====> Epoch: 52 Average loss: 0.002076 
Epoch: 53 [0/5999 (0%)]	Loss: 0.234046
Epoch: 53 [2560/5999 (43%)]	Loss: 0.166607
saving model at:53,0.001454288711771369
Epoch: 53 [5120/5999 (85%)]	Loss: 0.178307
saving model at:53,0.0013355927681550383
====> Epoch: 53 Average loss: 0.001766 
Epoch: 54 [0/5999 (0%)]	Loss: 0.187746
Epoch: 54 [2560/5999 (43%)]	Loss: 0.206314
Epoch: 54 [5120/5999 (85%)]	Loss: 0.222112
====> Epoch: 54 Average loss: 0.001697 
Epoch: 55 [0/5999 (0%)]	Loss: 0.186534
Epoch: 55 [2560/5999 (43%)]	Loss: 0.376030
Epoch: 55 [5120/5999 (85%)]	Loss: 0.208029
====> Epoch: 55 Average loss: 0.001727 
Epoch: 56 [0/5999 (0%)]	Loss: 0.207336
Epoch: 56 [2560/5999 (43%)]	Loss: 0.191387
saving model at:56,0.0011968347784131765
Epoch: 56 [5120/5999 (85%)]	Loss: 0.166681
====> Epoch: 56 Average loss: 0.001471 
Epoch: 57 [0/5999 (0%)]	Loss: 0.162393
Epoch: 57 [2560/5999 (43%)]	Loss: 0.172614
saving model at:57,0.0011529241474345327
Epoch: 57 [5120/5999 (85%)]	Loss: 0.249292
====> Epoch: 57 Average loss: 0.001470 
Epoch: 58 [0/5999 (0%)]	Loss: 0.222929
Epoch: 58 [2560/5999 (43%)]	Loss: 0.255875
Epoch: 58 [5120/5999 (85%)]	Loss: 0.130533
saving model at:58,0.0010572130028158426
====> Epoch: 58 Average loss: 0.001477 
Epoch: 59 [0/5999 (0%)]	Loss: 0.156217
Epoch: 59 [2560/5999 (43%)]	Loss: 0.156146
Epoch: 59 [5120/5999 (85%)]	Loss: 0.527579
====> Epoch: 59 Average loss: 0.001910 
Epoch: 60 [0/5999 (0%)]	Loss: 0.358458
Epoch: 60 [2560/5999 (43%)]	Loss: 0.212656
Epoch: 60 [5120/5999 (85%)]	Loss: 0.150951
====> Epoch: 60 Average loss: 0.001482 
Epoch: 61 [0/5999 (0%)]	Loss: 0.150456
Epoch: 61 [2560/5999 (43%)]	Loss: 0.311182
Epoch: 61 [5120/5999 (85%)]	Loss: 0.213182
====> Epoch: 61 Average loss: 0.001383 
Epoch: 62 [0/5999 (0%)]	Loss: 0.223582
Epoch: 62 [2560/5999 (43%)]	Loss: 0.197714
saving model at:62,0.0010154138756915928
Epoch: 62 [5120/5999 (85%)]	Loss: 0.137954
====> Epoch: 62 Average loss: 0.001435 
Epoch: 63 [0/5999 (0%)]	Loss: 0.132500
Epoch: 63 [2560/5999 (43%)]	Loss: 0.115518
saving model at:63,0.0009873796803876757
Epoch: 63 [5120/5999 (85%)]	Loss: 0.116864
saving model at:63,0.0008644427512772381
====> Epoch: 63 Average loss: 0.001187 
Epoch: 64 [0/5999 (0%)]	Loss: 0.125982
Epoch: 64 [2560/5999 (43%)]	Loss: 0.112010
Epoch: 64 [5120/5999 (85%)]	Loss: 0.161308
====> Epoch: 64 Average loss: 0.001103 
Epoch: 65 [0/5999 (0%)]	Loss: 0.128290
Epoch: 65 [2560/5999 (43%)]	Loss: 0.307088
Epoch: 65 [5120/5999 (85%)]	Loss: 0.147449
====> Epoch: 65 Average loss: 0.001381 
Epoch: 66 [0/5999 (0%)]	Loss: 0.110287
Epoch: 66 [2560/5999 (43%)]	Loss: 0.124866
saving model at:66,0.0008138146977871657
Epoch: 66 [5120/5999 (85%)]	Loss: 0.112531
saving model at:66,0.0007838249611668289
====> Epoch: 66 Average loss: 0.001187 
Epoch: 67 [0/5999 (0%)]	Loss: 0.111405
Epoch: 67 [2560/5999 (43%)]	Loss: 0.146074
Epoch: 67 [5120/5999 (85%)]	Loss: 0.098645
====> Epoch: 67 Average loss: 0.001124 
Epoch: 68 [0/5999 (0%)]	Loss: 0.113842
Epoch: 68 [2560/5999 (43%)]	Loss: 0.276192
Epoch: 68 [5120/5999 (85%)]	Loss: 0.190814
====> Epoch: 68 Average loss: 0.001457 
Epoch: 69 [0/5999 (0%)]	Loss: 0.138055
Epoch: 69 [2560/5999 (43%)]	Loss: 0.107523
Epoch: 69 [5120/5999 (85%)]	Loss: 0.124692
====> Epoch: 69 Average loss: 0.001183 
Epoch: 70 [0/5999 (0%)]	Loss: 0.108980
Epoch: 70 [2560/5999 (43%)]	Loss: 0.144067
saving model at:70,0.0006991765722632408
Epoch: 70 [5120/5999 (85%)]	Loss: 0.178451
====> Epoch: 70 Average loss: 0.001426 
Epoch: 71 [0/5999 (0%)]	Loss: 0.129641
Epoch: 71 [2560/5999 (43%)]	Loss: 0.108842
Epoch: 71 [5120/5999 (85%)]	Loss: 0.120226
====> Epoch: 71 Average loss: 0.000989 
Epoch: 72 [0/5999 (0%)]	Loss: 0.089453
Epoch: 72 [2560/5999 (43%)]	Loss: 0.103990
saving model at:72,0.0006989295333623886
Epoch: 72 [5120/5999 (85%)]	Loss: 0.095936
====> Epoch: 72 Average loss: 0.000946 
Epoch: 73 [0/5999 (0%)]	Loss: 0.139223
Epoch: 73 [2560/5999 (43%)]	Loss: 0.124979
Epoch: 73 [5120/5999 (85%)]	Loss: 0.130216
====> Epoch: 73 Average loss: 0.001231 
Epoch: 74 [0/5999 (0%)]	Loss: 0.105006
Epoch: 74 [2560/5999 (43%)]	Loss: 0.127861
Epoch: 74 [5120/5999 (85%)]	Loss: 0.106815
saving model at:74,0.0006838015075773001
====> Epoch: 74 Average loss: 0.000824 
Epoch: 75 [0/5999 (0%)]	Loss: 0.119854
Epoch: 75 [2560/5999 (43%)]	Loss: 0.106860
Epoch: 75 [5120/5999 (85%)]	Loss: 0.139916
====> Epoch: 75 Average loss: 0.001045 
Epoch: 76 [0/5999 (0%)]	Loss: 0.276921
Epoch: 76 [2560/5999 (43%)]	Loss: 0.087483
saving model at:76,0.0006730315103195607
Epoch: 76 [5120/5999 (85%)]	Loss: 0.137713
saving model at:76,0.0006576921860687435
====> Epoch: 76 Average loss: 0.001076 
Epoch: 77 [0/5999 (0%)]	Loss: 0.109921
Epoch: 77 [2560/5999 (43%)]	Loss: 0.092707
saving model at:77,0.0006198720601387321
Epoch: 77 [5120/5999 (85%)]	Loss: 0.178715
====> Epoch: 77 Average loss: 0.000985 
Epoch: 78 [0/5999 (0%)]	Loss: 0.103625
Epoch: 78 [2560/5999 (43%)]	Loss: 0.131879
Epoch: 78 [5120/5999 (85%)]	Loss: 0.203974
====> Epoch: 78 Average loss: 0.001085 
Epoch: 79 [0/5999 (0%)]	Loss: 0.226036
Epoch: 79 [2560/5999 (43%)]	Loss: 0.205439
Epoch: 79 [5120/5999 (85%)]	Loss: 0.082832
====> Epoch: 79 Average loss: 0.000878 
Epoch: 80 [0/5999 (0%)]	Loss: 0.113508
Epoch: 80 [2560/5999 (43%)]	Loss: 0.081824
saving model at:80,0.0006028071343898773
Epoch: 80 [5120/5999 (85%)]	Loss: 0.127105
====> Epoch: 80 Average loss: 0.000818 
Epoch: 81 [0/5999 (0%)]	Loss: 0.180089
Epoch: 81 [2560/5999 (43%)]	Loss: 0.105432
Epoch: 81 [5120/5999 (85%)]	Loss: 0.160261
====> Epoch: 81 Average loss: 0.000887 
Epoch: 82 [0/5999 (0%)]	Loss: 0.076878
Epoch: 82 [2560/5999 (43%)]	Loss: 0.140246
Epoch: 82 [5120/5999 (85%)]	Loss: 0.095944
saving model at:82,0.0005981869553215802
====> Epoch: 82 Average loss: 0.000803 
Epoch: 83 [0/5999 (0%)]	Loss: 0.081927
Epoch: 83 [2560/5999 (43%)]	Loss: 0.129880
Epoch: 83 [5120/5999 (85%)]	Loss: 0.074289
saving model at:83,0.0005768152764067054
====> Epoch: 83 Average loss: 0.000861 
Epoch: 84 [0/5999 (0%)]	Loss: 0.153968
Epoch: 84 [2560/5999 (43%)]	Loss: 0.080658
saving model at:84,0.0005689328210428357
Epoch: 84 [5120/5999 (85%)]	Loss: 0.158715
====> Epoch: 84 Average loss: 0.000947 
Epoch: 85 [0/5999 (0%)]	Loss: 0.166278
Epoch: 85 [2560/5999 (43%)]	Loss: 0.074531
Epoch: 85 [5120/5999 (85%)]	Loss: 0.089416
====> Epoch: 85 Average loss: 0.000887 
Epoch: 86 [0/5999 (0%)]	Loss: 0.077575
Epoch: 86 [2560/5999 (43%)]	Loss: 0.146719
saving model at:86,0.0004990798423532396
Epoch: 86 [5120/5999 (85%)]	Loss: 0.179286
====> Epoch: 86 Average loss: 0.001107 
Epoch: 87 [0/5999 (0%)]	Loss: 0.119184
Epoch: 87 [2560/5999 (43%)]	Loss: 0.093405
Epoch: 87 [5120/5999 (85%)]	Loss: 0.099027
====> Epoch: 87 Average loss: 0.000775 
Epoch: 88 [0/5999 (0%)]	Loss: 0.062098
Epoch: 88 [2560/5999 (43%)]	Loss: 0.102529
Epoch: 88 [5120/5999 (85%)]	Loss: 0.117087
====> Epoch: 88 Average loss: 0.000750 
Epoch: 89 [0/5999 (0%)]	Loss: 0.078072
Epoch: 89 [2560/5999 (43%)]	Loss: 0.070669
Epoch: 89 [5120/5999 (85%)]	Loss: 0.230924
====> Epoch: 89 Average loss: 0.000975 
Epoch: 90 [0/5999 (0%)]	Loss: 0.119685
Epoch: 90 [2560/5999 (43%)]	Loss: 0.074367
Epoch: 90 [5120/5999 (85%)]	Loss: 0.144876
====> Epoch: 90 Average loss: 0.000885 
Epoch: 91 [0/5999 (0%)]	Loss: 0.126644
Epoch: 91 [2560/5999 (43%)]	Loss: 0.106883
Epoch: 91 [5120/5999 (85%)]	Loss: 0.107188
====> Epoch: 91 Average loss: 0.000865 
Epoch: 92 [0/5999 (0%)]	Loss: 0.224573
Epoch: 92 [2560/5999 (43%)]	Loss: 0.083332
Epoch: 92 [5120/5999 (85%)]	Loss: 0.056430
====> Epoch: 92 Average loss: 0.000828 
Epoch: 93 [0/5999 (0%)]	Loss: 0.131906
Epoch: 93 [2560/5999 (43%)]	Loss: 0.078072
Epoch: 93 [5120/5999 (85%)]	Loss: 0.238856
====> Epoch: 93 Average loss: 0.000929 
Epoch: 94 [0/5999 (0%)]	Loss: 0.160918
Epoch: 94 [2560/5999 (43%)]	Loss: 0.111792
Epoch: 94 [5120/5999 (85%)]	Loss: 0.099797
====> Epoch: 94 Average loss: 0.000929 
Epoch: 95 [0/5999 (0%)]	Loss: 0.118884
Epoch: 95 [2560/5999 (43%)]	Loss: 0.112834
Epoch: 95 [5120/5999 (85%)]	Loss: 0.081978
====> Epoch: 95 Average loss: 0.000796 
Epoch: 96 [0/5999 (0%)]	Loss: 0.283159
Epoch: 96 [2560/5999 (43%)]	Loss: 0.136028
Epoch: 96 [5120/5999 (85%)]	Loss: 0.060553
saving model at:96,0.00046193616976961495
====> Epoch: 96 Average loss: 0.000721 
Epoch: 97 [0/5999 (0%)]	Loss: 0.065866
Epoch: 97 [2560/5999 (43%)]	Loss: 0.076901
Epoch: 97 [5120/5999 (85%)]	Loss: 0.064912
saving model at:97,0.00041528454003855584
====> Epoch: 97 Average loss: 0.000702 
Epoch: 98 [0/5999 (0%)]	Loss: 0.081522
Epoch: 98 [2560/5999 (43%)]	Loss: 0.414941
Epoch: 98 [5120/5999 (85%)]	Loss: 0.179222
====> Epoch: 98 Average loss: 0.001248 
Epoch: 99 [0/5999 (0%)]	Loss: 0.107684
Epoch: 99 [2560/5999 (43%)]	Loss: 0.081913
Epoch: 99 [5120/5999 (85%)]	Loss: 0.191504
====> Epoch: 99 Average loss: 0.000966 
Epoch: 100 [0/5999 (0%)]	Loss: 0.091588
Epoch: 100 [2560/5999 (43%)]	Loss: 0.141633
Epoch: 100 [5120/5999 (85%)]	Loss: 0.064014
====> Epoch: 100 Average loss: 0.000781 
Epoch: 101 [0/5999 (0%)]	Loss: 0.071696
Epoch: 101 [2560/5999 (43%)]	Loss: 0.067033
saving model at:101,0.00041120573855005205
Epoch: 101 [5120/5999 (85%)]	Loss: 0.113259
====> Epoch: 101 Average loss: 0.000619 
Epoch: 102 [0/5999 (0%)]	Loss: 0.054438
Epoch: 102 [2560/5999 (43%)]	Loss: 0.114542
Epoch: 102 [5120/5999 (85%)]	Loss: 0.070074
====> Epoch: 102 Average loss: 0.000626 
Epoch: 103 [0/5999 (0%)]	Loss: 0.061922
Epoch: 103 [2560/5999 (43%)]	Loss: 0.138980
Epoch: 103 [5120/5999 (85%)]	Loss: 0.060302
====> Epoch: 103 Average loss: 0.000628 
Epoch: 104 [0/5999 (0%)]	Loss: 0.102019
Epoch: 104 [2560/5999 (43%)]	Loss: 0.091971
Epoch: 104 [5120/5999 (85%)]	Loss: 0.249276
====> Epoch: 104 Average loss: 0.001023 
Epoch: 105 [0/5999 (0%)]	Loss: 0.150382
Epoch: 105 [2560/5999 (43%)]	Loss: 0.183529
Epoch: 105 [5120/5999 (85%)]	Loss: 0.089752
====> Epoch: 105 Average loss: 0.000883 
Epoch: 106 [0/5999 (0%)]	Loss: 0.068867
Epoch: 106 [2560/5999 (43%)]	Loss: 0.077560
saving model at:106,0.00037269786652177573
Epoch: 106 [5120/5999 (85%)]	Loss: 0.190292
====> Epoch: 106 Average loss: 0.000725 
Epoch: 107 [0/5999 (0%)]	Loss: 0.162340
Epoch: 107 [2560/5999 (43%)]	Loss: 0.069859
Epoch: 107 [5120/5999 (85%)]	Loss: 0.063560
====> Epoch: 107 Average loss: 0.000848 
Epoch: 108 [0/5999 (0%)]	Loss: 0.097195
Epoch: 108 [2560/5999 (43%)]	Loss: 0.068742
Epoch: 108 [5120/5999 (85%)]	Loss: 0.214091
====> Epoch: 108 Average loss: 0.000789 
Epoch: 109 [0/5999 (0%)]	Loss: 0.120845
Epoch: 109 [2560/5999 (43%)]	Loss: 0.069359
Epoch: 109 [5120/5999 (85%)]	Loss: 0.099904
====> Epoch: 109 Average loss: 0.000655 
Epoch: 110 [0/5999 (0%)]	Loss: 0.070884
Epoch: 110 [2560/5999 (43%)]	Loss: 0.061200
saving model at:110,0.0003723109404090792
Epoch: 110 [5120/5999 (85%)]	Loss: 0.089296
====> Epoch: 110 Average loss: 0.000699 
Epoch: 111 [0/5999 (0%)]	Loss: 0.098737
Epoch: 111 [2560/5999 (43%)]	Loss: 0.087282
Epoch: 111 [5120/5999 (85%)]	Loss: 0.065609
====> Epoch: 111 Average loss: 0.000858 
Epoch: 112 [0/5999 (0%)]	Loss: 0.080386
Epoch: 112 [2560/5999 (43%)]	Loss: 0.078462
Epoch: 112 [5120/5999 (85%)]	Loss: 0.131614
====> Epoch: 112 Average loss: 0.000660 
Epoch: 113 [0/5999 (0%)]	Loss: 0.166078
Epoch: 113 [2560/5999 (43%)]	Loss: 0.077542
saving model at:113,0.00034122576541267336
Epoch: 113 [5120/5999 (85%)]	Loss: 0.058819
====> Epoch: 113 Average loss: 0.000604 
Epoch: 114 [0/5999 (0%)]	Loss: 0.106044
Epoch: 114 [2560/5999 (43%)]	Loss: 0.063582
Epoch: 114 [5120/5999 (85%)]	Loss: 0.058696
====> Epoch: 114 Average loss: 0.000750 
Epoch: 115 [0/5999 (0%)]	Loss: 0.050262
Epoch: 115 [2560/5999 (43%)]	Loss: 0.140421
Epoch: 115 [5120/5999 (85%)]	Loss: 0.060857
====> Epoch: 115 Average loss: 0.000721 
Epoch: 116 [0/5999 (0%)]	Loss: 0.069519
Epoch: 116 [2560/5999 (43%)]	Loss: 0.075251
Epoch: 116 [5120/5999 (85%)]	Loss: 0.043785
====> Epoch: 116 Average loss: 0.000613 
Epoch: 117 [0/5999 (0%)]	Loss: 0.065582
Epoch: 117 [2560/5999 (43%)]	Loss: 0.067668
Epoch: 117 [5120/5999 (85%)]	Loss: 0.067409
saving model at:117,0.00033865640545263886
====> Epoch: 117 Average loss: 0.000646 
Epoch: 118 [0/5999 (0%)]	Loss: 0.047115
Epoch: 118 [2560/5999 (43%)]	Loss: 0.059622
Epoch: 118 [5120/5999 (85%)]	Loss: 0.050658
====> Epoch: 118 Average loss: 0.000672 
Epoch: 119 [0/5999 (0%)]	Loss: 0.088959
Epoch: 119 [2560/5999 (43%)]	Loss: 0.076009
Epoch: 119 [5120/5999 (85%)]	Loss: 0.079355
====> Epoch: 119 Average loss: 0.000616 
Epoch: 120 [0/5999 (0%)]	Loss: 0.067190
Epoch: 120 [2560/5999 (43%)]	Loss: 0.097808
Epoch: 120 [5120/5999 (85%)]	Loss: 0.226614
====> Epoch: 120 Average loss: 0.000756 
Epoch: 121 [0/5999 (0%)]	Loss: 0.061305
Epoch: 121 [2560/5999 (43%)]	Loss: 0.120393
Epoch: 121 [5120/5999 (85%)]	Loss: 0.107548
====> Epoch: 121 Average loss: 0.000647 
Epoch: 122 [0/5999 (0%)]	Loss: 0.112590
Epoch: 122 [2560/5999 (43%)]	Loss: 0.072236
Epoch: 122 [5120/5999 (85%)]	Loss: 0.065782
====> Epoch: 122 Average loss: 0.000639 
Epoch: 123 [0/5999 (0%)]	Loss: 0.083002
Epoch: 123 [2560/5999 (43%)]	Loss: 0.144324
Epoch: 123 [5120/5999 (85%)]	Loss: 0.050362
saving model at:123,0.000294040753506124
====> Epoch: 123 Average loss: 0.000793 
Epoch: 124 [0/5999 (0%)]	Loss: 0.049406
Epoch: 124 [2560/5999 (43%)]	Loss: 0.053745
Epoch: 124 [5120/5999 (85%)]	Loss: 0.045801
====> Epoch: 124 Average loss: 0.000594 
Epoch: 125 [0/5999 (0%)]	Loss: 0.036894
Epoch: 125 [2560/5999 (43%)]	Loss: 0.060729
saving model at:125,0.0002883171536959708
Epoch: 125 [5120/5999 (85%)]	Loss: 0.059065
====> Epoch: 125 Average loss: 0.000501 
Epoch: 126 [0/5999 (0%)]	Loss: 0.097449
Epoch: 126 [2560/5999 (43%)]	Loss: 0.125391
Epoch: 126 [5120/5999 (85%)]	Loss: 0.074552
====> Epoch: 126 Average loss: 0.000621 
Epoch: 127 [0/5999 (0%)]	Loss: 0.047900
Epoch: 127 [2560/5999 (43%)]	Loss: 0.053772
Epoch: 127 [5120/5999 (85%)]	Loss: 0.064119
====> Epoch: 127 Average loss: 0.000623 
Epoch: 128 [0/5999 (0%)]	Loss: 0.047876
Epoch: 128 [2560/5999 (43%)]	Loss: 0.057370
Epoch: 128 [5120/5999 (85%)]	Loss: 0.063812
====> Epoch: 128 Average loss: 0.000571 
Epoch: 129 [0/5999 (0%)]	Loss: 0.090566
Epoch: 129 [2560/5999 (43%)]	Loss: 0.056928
Epoch: 129 [5120/5999 (85%)]	Loss: 0.053225
====> Epoch: 129 Average loss: 0.000523 
Epoch: 130 [0/5999 (0%)]	Loss: 0.046200
Epoch: 130 [2560/5999 (43%)]	Loss: 0.052270
Epoch: 130 [5120/5999 (85%)]	Loss: 0.041274
====> Epoch: 130 Average loss: 0.000520 
Epoch: 131 [0/5999 (0%)]	Loss: 0.067747
Epoch: 131 [2560/5999 (43%)]	Loss: 0.053914
Epoch: 131 [5120/5999 (85%)]	Loss: 0.112690
====> Epoch: 131 Average loss: 0.000640 
Epoch: 132 [0/5999 (0%)]	Loss: 0.056636
Epoch: 132 [2560/5999 (43%)]	Loss: 0.049872
Epoch: 132 [5120/5999 (85%)]	Loss: 0.163560
====> Epoch: 132 Average loss: 0.000615 
Epoch: 133 [0/5999 (0%)]	Loss: 0.071103
Epoch: 133 [2560/5999 (43%)]	Loss: 0.046183
Epoch: 133 [5120/5999 (85%)]	Loss: 0.087421
====> Epoch: 133 Average loss: 0.000613 
Epoch: 134 [0/5999 (0%)]	Loss: 0.126127
Epoch: 134 [2560/5999 (43%)]	Loss: 0.054870
Epoch: 134 [5120/5999 (85%)]	Loss: 0.056514
saving model at:134,0.0002747349477140233
====> Epoch: 134 Average loss: 0.000513 
Epoch: 135 [0/5999 (0%)]	Loss: 0.055302
Epoch: 135 [2560/5999 (43%)]	Loss: 0.123473
Epoch: 135 [5120/5999 (85%)]	Loss: 0.073938
====> Epoch: 135 Average loss: 0.000689 
Epoch: 136 [0/5999 (0%)]	Loss: 0.053790
Epoch: 136 [2560/5999 (43%)]	Loss: 0.057248
Epoch: 136 [5120/5999 (85%)]	Loss: 0.075862
====> Epoch: 136 Average loss: 0.000518 
Epoch: 137 [0/5999 (0%)]	Loss: 0.057460
Epoch: 137 [2560/5999 (43%)]	Loss: 0.112947
Epoch: 137 [5120/5999 (85%)]	Loss: 0.065638
====> Epoch: 137 Average loss: 0.000565 
Epoch: 138 [0/5999 (0%)]	Loss: 0.115848
Epoch: 138 [2560/5999 (43%)]	Loss: 0.065453
Epoch: 138 [5120/5999 (85%)]	Loss: 0.038278
saving model at:138,0.0002465170742943883
====> Epoch: 138 Average loss: 0.000541 
Epoch: 139 [0/5999 (0%)]	Loss: 0.033537
Epoch: 139 [2560/5999 (43%)]	Loss: 0.038623
saving model at:139,0.0002450510007329285
Epoch: 139 [5120/5999 (85%)]	Loss: 0.037703
====> Epoch: 139 Average loss: 0.000436 
Epoch: 140 [0/5999 (0%)]	Loss: 0.150678
Epoch: 140 [2560/5999 (43%)]	Loss: 0.132065
Epoch: 140 [5120/5999 (85%)]	Loss: 0.041363
====> Epoch: 140 Average loss: 0.000640 
Epoch: 141 [0/5999 (0%)]	Loss: 0.065636
Epoch: 141 [2560/5999 (43%)]	Loss: 0.099384
Epoch: 141 [5120/5999 (85%)]	Loss: 0.048516
====> Epoch: 141 Average loss: 0.000600 
Epoch: 142 [0/5999 (0%)]	Loss: 0.065571
Epoch: 142 [2560/5999 (43%)]	Loss: 0.035225
saving model at:142,0.00022922256076708436
Epoch: 142 [5120/5999 (85%)]	Loss: 0.059107
====> Epoch: 142 Average loss: 0.000418 
Epoch: 143 [0/5999 (0%)]	Loss: 0.082317
Epoch: 143 [2560/5999 (43%)]	Loss: 0.093080
Epoch: 143 [5120/5999 (85%)]	Loss: 0.037733
====> Epoch: 143 Average loss: 0.000550 
Epoch: 144 [0/5999 (0%)]	Loss: 0.083638
Epoch: 144 [2560/5999 (43%)]	Loss: 0.188486
Epoch: 144 [5120/5999 (85%)]	Loss: 0.036093
====> Epoch: 144 Average loss: 0.000610 
Epoch: 145 [0/5999 (0%)]	Loss: 0.068780
Epoch: 145 [2560/5999 (43%)]	Loss: 0.041810
Epoch: 145 [5120/5999 (85%)]	Loss: 0.043244
====> Epoch: 145 Average loss: 0.000514 
Epoch: 146 [0/5999 (0%)]	Loss: 0.043822
Epoch: 146 [2560/5999 (43%)]	Loss: 0.086933
Epoch: 146 [5120/5999 (85%)]	Loss: 0.094568
====> Epoch: 146 Average loss: 0.000541 
Epoch: 147 [0/5999 (0%)]	Loss: 0.113120
Epoch: 147 [2560/5999 (43%)]	Loss: 0.052883
Epoch: 147 [5120/5999 (85%)]	Loss: 0.082508
====> Epoch: 147 Average loss: 0.000560 
Epoch: 148 [0/5999 (0%)]	Loss: 0.042772
Epoch: 148 [2560/5999 (43%)]	Loss: 0.051523
Epoch: 148 [5120/5999 (85%)]	Loss: 0.070326
====> Epoch: 148 Average loss: 0.000466 
Epoch: 149 [0/5999 (0%)]	Loss: 0.032430
Epoch: 149 [2560/5999 (43%)]	Loss: 0.054823
Epoch: 149 [5120/5999 (85%)]	Loss: 0.049518
====> Epoch: 149 Average loss: 0.000462 
Epoch: 150 [0/5999 (0%)]	Loss: 0.072906
Epoch: 150 [2560/5999 (43%)]	Loss: 0.033627
Epoch: 150 [5120/5999 (85%)]	Loss: 0.083718
====> Epoch: 150 Average loss: 0.000484 
Epoch: 151 [0/5999 (0%)]	Loss: 0.034374
Epoch: 151 [2560/5999 (43%)]	Loss: 0.030975
Epoch: 151 [5120/5999 (85%)]	Loss: 0.086397
====> Epoch: 151 Average loss: 0.000467 
Epoch: 152 [0/5999 (0%)]	Loss: 0.069547
Epoch: 152 [2560/5999 (43%)]	Loss: 0.036602
Epoch: 152 [5120/5999 (85%)]	Loss: 0.054657
====> Epoch: 152 Average loss: 0.000533 
Epoch: 153 [0/5999 (0%)]	Loss: 0.046960
Epoch: 153 [2560/5999 (43%)]	Loss: 0.044480
Epoch: 153 [5120/5999 (85%)]	Loss: 0.056799
====> Epoch: 153 Average loss: 0.000410 
Epoch: 154 [0/5999 (0%)]	Loss: 0.044013
Epoch: 154 [2560/5999 (43%)]	Loss: 0.064903
Epoch: 154 [5120/5999 (85%)]	Loss: 0.119074
====> Epoch: 154 Average loss: 0.000674 
Epoch: 155 [0/5999 (0%)]	Loss: 0.094285
Epoch: 155 [2560/5999 (43%)]	Loss: 0.032280
Epoch: 155 [5120/5999 (85%)]	Loss: 0.052425
====> Epoch: 155 Average loss: 0.000485 
Epoch: 156 [0/5999 (0%)]	Loss: 0.058568
Epoch: 156 [2560/5999 (43%)]	Loss: 0.042534
Epoch: 156 [5120/5999 (85%)]	Loss: 0.070610
====> Epoch: 156 Average loss: 0.000474 
Epoch: 157 [0/5999 (0%)]	Loss: 0.033258
Epoch: 157 [2560/5999 (43%)]	Loss: 0.033640
Epoch: 157 [5120/5999 (85%)]	Loss: 0.034056
====> Epoch: 157 Average loss: 0.000416 
Epoch: 158 [0/5999 (0%)]	Loss: 0.062837
Epoch: 158 [2560/5999 (43%)]	Loss: 0.079732
Epoch: 158 [5120/5999 (85%)]	Loss: 0.044771
====> Epoch: 158 Average loss: 0.000689 
Epoch: 159 [0/5999 (0%)]	Loss: 0.054981
Epoch: 159 [2560/5999 (43%)]	Loss: 0.042579
Epoch: 159 [5120/5999 (85%)]	Loss: 0.109635
====> Epoch: 159 Average loss: 0.000788 
Epoch: 160 [0/5999 (0%)]	Loss: 0.120414
Epoch: 160 [2560/5999 (43%)]	Loss: 0.059818
Epoch: 160 [5120/5999 (85%)]	Loss: 0.031153
====> Epoch: 160 Average loss: 0.000541 
Epoch: 161 [0/5999 (0%)]	Loss: 0.041931
Epoch: 161 [2560/5999 (43%)]	Loss: 0.051531
Epoch: 161 [5120/5999 (85%)]	Loss: 0.083576
====> Epoch: 161 Average loss: 0.000410 
Epoch: 162 [0/5999 (0%)]	Loss: 0.045917
Epoch: 162 [2560/5999 (43%)]	Loss: 0.035249
Epoch: 162 [5120/5999 (85%)]	Loss: 0.040279
saving model at:162,0.0002032269531628117
====> Epoch: 162 Average loss: 0.000496 
Epoch: 163 [0/5999 (0%)]	Loss: 0.049328
Epoch: 163 [2560/5999 (43%)]	Loss: 0.034304
Epoch: 163 [5120/5999 (85%)]	Loss: 0.037018
====> Epoch: 163 Average loss: 0.000391 
Epoch: 164 [0/5999 (0%)]	Loss: 0.106869
Epoch: 164 [2560/5999 (43%)]	Loss: 0.069908
Epoch: 164 [5120/5999 (85%)]	Loss: 0.049314
====> Epoch: 164 Average loss: 0.000486 
Epoch: 165 [0/5999 (0%)]	Loss: 0.034883
Epoch: 165 [2560/5999 (43%)]	Loss: 0.032101
saving model at:165,0.0002005093633197248
Epoch: 165 [5120/5999 (85%)]	Loss: 0.063978
====> Epoch: 165 Average loss: 0.000658 
Epoch: 166 [0/5999 (0%)]	Loss: 0.071444
Epoch: 166 [2560/5999 (43%)]	Loss: 0.119888
Epoch: 166 [5120/5999 (85%)]	Loss: 0.041587
====> Epoch: 166 Average loss: 0.000511 
Epoch: 167 [0/5999 (0%)]	Loss: 0.045585
Epoch: 167 [2560/5999 (43%)]	Loss: 0.045584
Epoch: 167 [5120/5999 (85%)]	Loss: 0.132786
====> Epoch: 167 Average loss: 0.000658 
Epoch: 168 [0/5999 (0%)]	Loss: 0.066549
Epoch: 168 [2560/5999 (43%)]	Loss: 0.060868
Epoch: 168 [5120/5999 (85%)]	Loss: 0.065948
====> Epoch: 168 Average loss: 0.000570 
Epoch: 169 [0/5999 (0%)]	Loss: 0.087023
Epoch: 169 [2560/5999 (43%)]	Loss: 0.048663
Epoch: 169 [5120/5999 (85%)]	Loss: 0.097404
====> Epoch: 169 Average loss: 0.000779 
Epoch: 170 [0/5999 (0%)]	Loss: 0.053185
Epoch: 170 [2560/5999 (43%)]	Loss: 0.111289
Epoch: 170 [5120/5999 (85%)]	Loss: 0.122250
====> Epoch: 170 Average loss: 0.000678 
Epoch: 171 [0/5999 (0%)]	Loss: 0.127083
Epoch: 171 [2560/5999 (43%)]	Loss: 0.190710
Epoch: 171 [5120/5999 (85%)]	Loss: 0.046716
====> Epoch: 171 Average loss: 0.000683 
Epoch: 172 [0/5999 (0%)]	Loss: 0.048800
Epoch: 172 [2560/5999 (43%)]	Loss: 0.069266
Epoch: 172 [5120/5999 (85%)]	Loss: 0.059604
====> Epoch: 172 Average loss: 0.000468 
Epoch: 173 [0/5999 (0%)]	Loss: 0.076897
Epoch: 173 [2560/5999 (43%)]	Loss: 0.057092
Epoch: 173 [5120/5999 (85%)]	Loss: 0.111371
====> Epoch: 173 Average loss: 0.000679 
Epoch: 174 [0/5999 (0%)]	Loss: 0.053402
Epoch: 174 [2560/5999 (43%)]	Loss: 0.068193
Epoch: 174 [5120/5999 (85%)]	Loss: 0.059587
====> Epoch: 174 Average loss: 0.000423 
Epoch: 175 [0/5999 (0%)]	Loss: 0.066134
Epoch: 175 [2560/5999 (43%)]	Loss: 0.163169
Epoch: 175 [5120/5999 (85%)]	Loss: 0.054535
====> Epoch: 175 Average loss: 0.000470 
Epoch: 176 [0/5999 (0%)]	Loss: 0.028893
Epoch: 176 [2560/5999 (43%)]	Loss: 0.056765
Epoch: 176 [5120/5999 (85%)]	Loss: 0.045622
====> Epoch: 176 Average loss: 0.000525 
Epoch: 177 [0/5999 (0%)]	Loss: 0.110814
Epoch: 177 [2560/5999 (43%)]	Loss: 0.049618
Epoch: 177 [5120/5999 (85%)]	Loss: 0.106764
====> Epoch: 177 Average loss: 0.000452 
Epoch: 178 [0/5999 (0%)]	Loss: 0.037741
Epoch: 178 [2560/5999 (43%)]	Loss: 0.067052
Epoch: 178 [5120/5999 (85%)]	Loss: 0.061517
====> Epoch: 178 Average loss: 0.000582 
Epoch: 179 [0/5999 (0%)]	Loss: 0.062507
Epoch: 179 [2560/5999 (43%)]	Loss: 0.032275
Epoch: 179 [5120/5999 (85%)]	Loss: 0.103565
====> Epoch: 179 Average loss: 0.000474 
Epoch: 180 [0/5999 (0%)]	Loss: 0.066989
Epoch: 180 [2560/5999 (43%)]	Loss: 0.052528
Epoch: 180 [5120/5999 (85%)]	Loss: 0.103529
====> Epoch: 180 Average loss: 0.000462 
Epoch: 181 [0/5999 (0%)]	Loss: 0.077827
Epoch: 181 [2560/5999 (43%)]	Loss: 0.029043
Epoch: 181 [5120/5999 (85%)]	Loss: 0.045173
====> Epoch: 181 Average loss: 0.000475 
Epoch: 182 [0/5999 (0%)]	Loss: 0.032128
Epoch: 182 [2560/5999 (43%)]	Loss: 0.029052
saving model at:182,0.00019500440265983343
Epoch: 182 [5120/5999 (85%)]	Loss: 0.064097
====> Epoch: 182 Average loss: 0.000447 
Epoch: 183 [0/5999 (0%)]	Loss: 0.047505
Epoch: 183 [2560/5999 (43%)]	Loss: 0.034085
saving model at:183,0.00019088197662495076
Epoch: 183 [5120/5999 (85%)]	Loss: 0.034663
====> Epoch: 183 Average loss: 0.000391 
Epoch: 184 [0/5999 (0%)]	Loss: 0.049364
Epoch: 184 [2560/5999 (43%)]	Loss: 0.040276
Epoch: 184 [5120/5999 (85%)]	Loss: 0.046746
====> Epoch: 184 Average loss: 0.000512 
Epoch: 185 [0/5999 (0%)]	Loss: 0.030049
Epoch: 185 [2560/5999 (43%)]	Loss: 0.075554
Epoch: 185 [5120/5999 (85%)]	Loss: 0.038057
====> Epoch: 185 Average loss: 0.000549 
Epoch: 186 [0/5999 (0%)]	Loss: 0.042213
Epoch: 186 [2560/5999 (43%)]	Loss: 0.118123
Epoch: 186 [5120/5999 (85%)]	Loss: 0.065814
====> Epoch: 186 Average loss: 0.000499 
Epoch: 187 [0/5999 (0%)]	Loss: 0.079326
Epoch: 187 [2560/5999 (43%)]	Loss: 0.037012
Epoch: 187 [5120/5999 (85%)]	Loss: 0.113842
====> Epoch: 187 Average loss: 0.000651 
Epoch: 188 [0/5999 (0%)]	Loss: 0.061028
Epoch: 188 [2560/5999 (43%)]	Loss: 0.058138
Epoch: 188 [5120/5999 (85%)]	Loss: 0.122584
====> Epoch: 188 Average loss: 0.000579 
Epoch: 189 [0/5999 (0%)]	Loss: 0.088231
Epoch: 189 [2560/5999 (43%)]	Loss: 0.033246
Epoch: 189 [5120/5999 (85%)]	Loss: 0.034703
====> Epoch: 189 Average loss: 0.000498 
Epoch: 190 [0/5999 (0%)]	Loss: 0.067960
Epoch: 190 [2560/5999 (43%)]	Loss: 0.033345
Epoch: 190 [5120/5999 (85%)]	Loss: 0.065475
====> Epoch: 190 Average loss: 0.000466 
Epoch: 191 [0/5999 (0%)]	Loss: 0.068225
Epoch: 191 [2560/5999 (43%)]	Loss: 0.095856
Epoch: 191 [5120/5999 (85%)]	Loss: 0.070769
====> Epoch: 191 Average loss: 0.000682 
Epoch: 192 [0/5999 (0%)]	Loss: 0.022638
Epoch: 192 [2560/5999 (43%)]	Loss: 0.030415
Epoch: 192 [5120/5999 (85%)]	Loss: 0.041683
====> Epoch: 192 Average loss: 0.000483 
Epoch: 193 [0/5999 (0%)]	Loss: 0.041238
Epoch: 193 [2560/5999 (43%)]	Loss: 0.034802
saving model at:193,0.00018311790109146387
Epoch: 193 [5120/5999 (85%)]	Loss: 0.029534
saving model at:193,0.00017576254566665738
====> Epoch: 193 Average loss: 0.000370 
Epoch: 194 [0/5999 (0%)]	Loss: 0.034187
Epoch: 194 [2560/5999 (43%)]	Loss: 0.031160
Epoch: 194 [5120/5999 (85%)]	Loss: 0.066858
====> Epoch: 194 Average loss: 0.000519 
Epoch: 195 [0/5999 (0%)]	Loss: 0.025702
Epoch: 195 [2560/5999 (43%)]	Loss: 0.055748
Epoch: 195 [5120/5999 (85%)]	Loss: 0.125543
====> Epoch: 195 Average loss: 0.000488 
Epoch: 196 [0/5999 (0%)]	Loss: 0.045889
Epoch: 196 [2560/5999 (43%)]	Loss: 0.041363
Epoch: 196 [5120/5999 (85%)]	Loss: 0.034996
====> Epoch: 196 Average loss: 0.000390 
Epoch: 197 [0/5999 (0%)]	Loss: 0.026467
Epoch: 197 [2560/5999 (43%)]	Loss: 0.163594
Epoch: 197 [5120/5999 (85%)]	Loss: 0.031912
====> Epoch: 197 Average loss: 0.000427 
Epoch: 198 [0/5999 (0%)]	Loss: 0.037107
Epoch: 198 [2560/5999 (43%)]	Loss: 0.036229
Epoch: 198 [5120/5999 (85%)]	Loss: 0.087506
====> Epoch: 198 Average loss: 0.000661 
Epoch: 199 [0/5999 (0%)]	Loss: 0.070066
Epoch: 199 [2560/5999 (43%)]	Loss: 0.035026
Epoch: 199 [5120/5999 (85%)]	Loss: 0.026118
====> Epoch: 199 Average loss: 0.000389 
Epoch: 200 [0/5999 (0%)]	Loss: 0.065697
Epoch: 200 [2560/5999 (43%)]	Loss: 0.034118
Epoch: 200 [5120/5999 (85%)]	Loss: 0.029064
====> Epoch: 200 Average loss: 0.000419 
Epoch: 201 [0/5999 (0%)]	Loss: 0.032164
Epoch: 201 [2560/5999 (43%)]	Loss: 0.098790
Epoch: 201 [5120/5999 (85%)]	Loss: 0.030404
====> Epoch: 201 Average loss: 0.000400 
Epoch: 202 [0/5999 (0%)]	Loss: 0.024074
Epoch: 202 [2560/5999 (43%)]	Loss: 0.191226
Epoch: 202 [5120/5999 (85%)]	Loss: 0.046417
====> Epoch: 202 Average loss: 0.000538 
Epoch: 203 [0/5999 (0%)]	Loss: 0.042204
Epoch: 203 [2560/5999 (43%)]	Loss: 0.082187
Epoch: 203 [5120/5999 (85%)]	Loss: 0.044848
====> Epoch: 203 Average loss: 0.000523 
Epoch: 204 [0/5999 (0%)]	Loss: 0.033313
Epoch: 204 [2560/5999 (43%)]	Loss: 0.025657
saving model at:204,0.0001652932504657656
Epoch: 204 [5120/5999 (85%)]	Loss: 0.082662
====> Epoch: 204 Average loss: 0.000347 
Epoch: 205 [0/5999 (0%)]	Loss: 0.024040
Epoch: 205 [2560/5999 (43%)]	Loss: 0.036003
Epoch: 205 [5120/5999 (85%)]	Loss: 0.042827
====> Epoch: 205 Average loss: 0.000426 
Epoch: 206 [0/5999 (0%)]	Loss: 0.043465
Epoch: 206 [2560/5999 (43%)]	Loss: 0.028090
saving model at:206,0.0001520730258198455
Epoch: 206 [5120/5999 (85%)]	Loss: 0.025454
====> Epoch: 206 Average loss: 0.000393 
Epoch: 207 [0/5999 (0%)]	Loss: 0.151082
Epoch: 207 [2560/5999 (43%)]	Loss: 0.046111
Epoch: 207 [5120/5999 (85%)]	Loss: 0.029833
====> Epoch: 207 Average loss: 0.000318 
Epoch: 208 [0/5999 (0%)]	Loss: 0.051164
Epoch: 208 [2560/5999 (43%)]	Loss: 0.101194
Epoch: 208 [5120/5999 (85%)]	Loss: 0.053454
====> Epoch: 208 Average loss: 0.000522 
Epoch: 209 [0/5999 (0%)]	Loss: 0.042206
Epoch: 209 [2560/5999 (43%)]	Loss: 0.088337
Epoch: 209 [5120/5999 (85%)]	Loss: 0.041958
====> Epoch: 209 Average loss: 0.000435 
Epoch: 210 [0/5999 (0%)]	Loss: 0.123199
Epoch: 210 [2560/5999 (43%)]	Loss: 0.096557
Epoch: 210 [5120/5999 (85%)]	Loss: 0.028163
====> Epoch: 210 Average loss: 0.000388 
Epoch: 211 [0/5999 (0%)]	Loss: 0.184583
Epoch: 211 [2560/5999 (43%)]	Loss: 0.040764
Epoch: 211 [5120/5999 (85%)]	Loss: 0.031476
====> Epoch: 211 Average loss: 0.000419 
Epoch: 212 [0/5999 (0%)]	Loss: 0.148359
Epoch: 212 [2560/5999 (43%)]	Loss: 0.030884
Epoch: 212 [5120/5999 (85%)]	Loss: 0.026551
====> Epoch: 212 Average loss: 0.000430 
Epoch: 213 [0/5999 (0%)]	Loss: 0.024453
Epoch: 213 [2560/5999 (43%)]	Loss: 0.037295
Epoch: 213 [5120/5999 (85%)]	Loss: 0.071251
====> Epoch: 213 Average loss: 0.000625 
Epoch: 214 [0/5999 (0%)]	Loss: 0.052685
Epoch: 214 [2560/5999 (43%)]	Loss: 0.049597
Epoch: 214 [5120/5999 (85%)]	Loss: 0.087665
====> Epoch: 214 Average loss: 0.000395 
Epoch: 215 [0/5999 (0%)]	Loss: 0.033797
Epoch: 215 [2560/5999 (43%)]	Loss: 0.044890
Epoch: 215 [5120/5999 (85%)]	Loss: 0.032587
====> Epoch: 215 Average loss: 0.000535 
Epoch: 216 [0/5999 (0%)]	Loss: 0.032215
Epoch: 216 [2560/5999 (43%)]	Loss: 0.024170
Epoch: 216 [5120/5999 (85%)]	Loss: 0.043926
====> Epoch: 216 Average loss: 0.000305 
Epoch: 217 [0/5999 (0%)]	Loss: 0.042405
Epoch: 217 [2560/5999 (43%)]	Loss: 0.141282
Epoch: 217 [5120/5999 (85%)]	Loss: 0.031053
====> Epoch: 217 Average loss: 0.000502 
Epoch: 218 [0/5999 (0%)]	Loss: 0.033810
Epoch: 218 [2560/5999 (43%)]	Loss: 0.060224
Epoch: 218 [5120/5999 (85%)]	Loss: 0.040618
====> Epoch: 218 Average loss: 0.000467 
Epoch: 219 [0/5999 (0%)]	Loss: 0.035264
Epoch: 219 [2560/5999 (43%)]	Loss: 0.051529
Epoch: 219 [5120/5999 (85%)]	Loss: 0.032060
====> Epoch: 219 Average loss: 0.000285 
Epoch: 220 [0/5999 (0%)]	Loss: 0.089791
Epoch: 220 [2560/5999 (43%)]	Loss: 0.042813
Epoch: 220 [5120/5999 (85%)]	Loss: 0.038571
====> Epoch: 220 Average loss: 0.000364 
Epoch: 221 [0/5999 (0%)]	Loss: 0.028711
Epoch: 221 [2560/5999 (43%)]	Loss: 0.060905
Epoch: 221 [5120/5999 (85%)]	Loss: 0.035953
====> Epoch: 221 Average loss: 0.000377 
Epoch: 222 [0/5999 (0%)]	Loss: 0.064421
Epoch: 222 [2560/5999 (43%)]	Loss: 0.034021
Epoch: 222 [5120/5999 (85%)]	Loss: 0.049737
====> Epoch: 222 Average loss: 0.000381 
Epoch: 223 [0/5999 (0%)]	Loss: 0.036871
Epoch: 223 [2560/5999 (43%)]	Loss: 0.099999
Epoch: 223 [5120/5999 (85%)]	Loss: 0.026005
====> Epoch: 223 Average loss: 0.000446 
Epoch: 224 [0/5999 (0%)]	Loss: 0.042307
Epoch: 224 [2560/5999 (43%)]	Loss: 0.061280
Epoch: 224 [5120/5999 (85%)]	Loss: 0.038837
====> Epoch: 224 Average loss: 0.000352 
Epoch: 225 [0/5999 (0%)]	Loss: 0.040522
Epoch: 225 [2560/5999 (43%)]	Loss: 0.061197
Epoch: 225 [5120/5999 (85%)]	Loss: 0.034229
====> Epoch: 225 Average loss: 0.000450 
Epoch: 226 [0/5999 (0%)]	Loss: 0.044256
Epoch: 226 [2560/5999 (43%)]	Loss: 0.033795
Epoch: 226 [5120/5999 (85%)]	Loss: 0.081233
====> Epoch: 226 Average loss: 0.000380 
Epoch: 227 [0/5999 (0%)]	Loss: 0.118365
Epoch: 227 [2560/5999 (43%)]	Loss: 0.039432
Epoch: 227 [5120/5999 (85%)]	Loss: 0.031416
====> Epoch: 227 Average loss: 0.000465 
Epoch: 228 [0/5999 (0%)]	Loss: 0.043102
Epoch: 228 [2560/5999 (43%)]	Loss: 0.125023
Epoch: 228 [5120/5999 (85%)]	Loss: 0.030644
====> Epoch: 228 Average loss: 0.000423 
Epoch: 229 [0/5999 (0%)]	Loss: 0.108733
Epoch: 229 [2560/5999 (43%)]	Loss: 0.049997
Epoch: 229 [5120/5999 (85%)]	Loss: 0.044528
====> Epoch: 229 Average loss: 0.000389 
Epoch: 230 [0/5999 (0%)]	Loss: 0.024058
Epoch: 230 [2560/5999 (43%)]	Loss: 0.038020
Epoch: 230 [5120/5999 (85%)]	Loss: 0.054387
====> Epoch: 230 Average loss: 0.000391 
Epoch: 231 [0/5999 (0%)]	Loss: 0.077643
Epoch: 231 [2560/5999 (43%)]	Loss: 0.040254
Epoch: 231 [5120/5999 (85%)]	Loss: 0.089302
====> Epoch: 231 Average loss: 0.000526 
Epoch: 232 [0/5999 (0%)]	Loss: 0.078897
Epoch: 232 [2560/5999 (43%)]	Loss: 0.027501
saving model at:232,0.0001512120981933549
Epoch: 232 [5120/5999 (85%)]	Loss: 0.024778
====> Epoch: 232 Average loss: 0.000358 
Epoch: 233 [0/5999 (0%)]	Loss: 0.023031
Epoch: 233 [2560/5999 (43%)]	Loss: 0.098983
Epoch: 233 [5120/5999 (85%)]	Loss: 0.023527
====> Epoch: 233 Average loss: 0.000394 
Epoch: 234 [0/5999 (0%)]	Loss: 0.042863
Epoch: 234 [2560/5999 (43%)]	Loss: 0.037530
Epoch: 234 [5120/5999 (85%)]	Loss: 0.036585
====> Epoch: 234 Average loss: 0.000489 
Epoch: 235 [0/5999 (0%)]	Loss: 0.046689
Epoch: 235 [2560/5999 (43%)]	Loss: 0.029815
Epoch: 235 [5120/5999 (85%)]	Loss: 0.182072
====> Epoch: 235 Average loss: 0.000463 
Epoch: 236 [0/5999 (0%)]	Loss: 0.036657
Epoch: 236 [2560/5999 (43%)]	Loss: 0.026910
Epoch: 236 [5120/5999 (85%)]	Loss: 0.050058
====> Epoch: 236 Average loss: 0.000329 
Epoch: 237 [0/5999 (0%)]	Loss: 0.051594
Epoch: 237 [2560/5999 (43%)]	Loss: 0.031314
Epoch: 237 [5120/5999 (85%)]	Loss: 0.041232
====> Epoch: 237 Average loss: 0.000318 
Epoch: 238 [0/5999 (0%)]	Loss: 0.036466
Epoch: 238 [2560/5999 (43%)]	Loss: 0.030448
Epoch: 238 [5120/5999 (85%)]	Loss: 0.034086
====> Epoch: 238 Average loss: 0.000425 
Epoch: 239 [0/5999 (0%)]	Loss: 0.062523
Epoch: 239 [2560/5999 (43%)]	Loss: 0.046706
Epoch: 239 [5120/5999 (85%)]	Loss: 0.023961
saving model at:239,0.00012727406341582535
====> Epoch: 239 Average loss: 0.000390 
Epoch: 240 [0/5999 (0%)]	Loss: 0.027051
Epoch: 240 [2560/5999 (43%)]	Loss: 0.033650
Epoch: 240 [5120/5999 (85%)]	Loss: 0.038181
====> Epoch: 240 Average loss: 0.000573 
Epoch: 241 [0/5999 (0%)]	Loss: 0.063237
Epoch: 241 [2560/5999 (43%)]	Loss: 0.039529
Epoch: 241 [5120/5999 (85%)]	Loss: 0.048057
====> Epoch: 241 Average loss: 0.000386 
Epoch: 242 [0/5999 (0%)]	Loss: 0.039700
Epoch: 242 [2560/5999 (43%)]	Loss: 0.019043
Epoch: 242 [5120/5999 (85%)]	Loss: 0.039397
====> Epoch: 242 Average loss: 0.000376 
Epoch: 243 [0/5999 (0%)]	Loss: 0.045254
Epoch: 243 [2560/5999 (43%)]	Loss: 0.026005
Epoch: 243 [5120/5999 (85%)]	Loss: 0.070155
====> Epoch: 243 Average loss: 0.000408 
Epoch: 244 [0/5999 (0%)]	Loss: 0.019303
Epoch: 244 [2560/5999 (43%)]	Loss: 0.028823
Epoch: 244 [5120/5999 (85%)]	Loss: 0.031362
====> Epoch: 244 Average loss: 0.000326 
Epoch: 245 [0/5999 (0%)]	Loss: 0.021783
Epoch: 245 [2560/5999 (43%)]	Loss: 0.068938
Epoch: 245 [5120/5999 (85%)]	Loss: 0.051370
====> Epoch: 245 Average loss: 0.000325 
Epoch: 246 [0/5999 (0%)]	Loss: 0.021658
Epoch: 246 [2560/5999 (43%)]	Loss: 0.018783
Epoch: 246 [5120/5999 (85%)]	Loss: 0.062317
====> Epoch: 246 Average loss: 0.000342 
Epoch: 247 [0/5999 (0%)]	Loss: 0.045152
Epoch: 247 [2560/5999 (43%)]	Loss: 0.030746
Epoch: 247 [5120/5999 (85%)]	Loss: 0.053330
====> Epoch: 247 Average loss: 0.000407 
Epoch: 248 [0/5999 (0%)]	Loss: 0.066426
Epoch: 248 [2560/5999 (43%)]	Loss: 0.087716
Epoch: 248 [5120/5999 (85%)]	Loss: 0.052538
====> Epoch: 248 Average loss: 0.000350 
Epoch: 249 [0/5999 (0%)]	Loss: 0.040610
Epoch: 249 [2560/5999 (43%)]	Loss: 0.042548
Epoch: 249 [5120/5999 (85%)]	Loss: 0.058500
====> Epoch: 249 Average loss: 0.000314 
Epoch: 250 [0/5999 (0%)]	Loss: 0.029947
Epoch: 250 [2560/5999 (43%)]	Loss: 0.020051
Epoch: 250 [5120/5999 (85%)]	Loss: 0.037308
====> Epoch: 250 Average loss: 0.000425 
Epoch: 251 [0/5999 (0%)]	Loss: 0.050145
Epoch: 251 [2560/5999 (43%)]	Loss: 0.022845
saving model at:251,0.0001232267317827791
Epoch: 251 [5120/5999 (85%)]	Loss: 0.028984
====> Epoch: 251 Average loss: 0.000405 
Epoch: 252 [0/5999 (0%)]	Loss: 0.024760
Epoch: 252 [2560/5999 (43%)]	Loss: 0.074172
Epoch: 252 [5120/5999 (85%)]	Loss: 0.081127
====> Epoch: 252 Average loss: 0.000546 
Epoch: 253 [0/5999 (0%)]	Loss: 0.026952
Epoch: 253 [2560/5999 (43%)]	Loss: 0.057193
Epoch: 253 [5120/5999 (85%)]	Loss: 0.041845
====> Epoch: 253 Average loss: 0.000366 
Epoch: 254 [0/5999 (0%)]	Loss: 0.024055
Epoch: 254 [2560/5999 (43%)]	Loss: 0.037062
Epoch: 254 [5120/5999 (85%)]	Loss: 0.046779
====> Epoch: 254 Average loss: 0.000327 
Epoch: 255 [0/5999 (0%)]	Loss: 0.078932
Epoch: 255 [2560/5999 (43%)]	Loss: 0.035455
Epoch: 255 [5120/5999 (85%)]	Loss: 0.082508
====> Epoch: 255 Average loss: 0.000323 
Epoch: 256 [0/5999 (0%)]	Loss: 0.016213
Epoch: 256 [2560/5999 (43%)]	Loss: 0.030393
saving model at:256,0.00011042323720175773
Epoch: 256 [5120/5999 (85%)]	Loss: 0.039978
====> Epoch: 256 Average loss: 0.000284 
Epoch: 257 [0/5999 (0%)]	Loss: 0.017318
Epoch: 257 [2560/5999 (43%)]	Loss: 0.022784
Epoch: 257 [5120/5999 (85%)]	Loss: 0.025522
====> Epoch: 257 Average loss: 0.000308 
Epoch: 258 [0/5999 (0%)]	Loss: 0.048227
Epoch: 258 [2560/5999 (43%)]	Loss: 0.038943
Epoch: 258 [5120/5999 (85%)]	Loss: 0.062972
====> Epoch: 258 Average loss: 0.000497 
Epoch: 259 [0/5999 (0%)]	Loss: 0.031001
Epoch: 259 [2560/5999 (43%)]	Loss: 0.023926
Epoch: 259 [5120/5999 (85%)]	Loss: 0.022351
====> Epoch: 259 Average loss: 0.000372 
Epoch: 260 [0/5999 (0%)]	Loss: 0.037882
Epoch: 260 [2560/5999 (43%)]	Loss: 0.020154
Epoch: 260 [5120/5999 (85%)]	Loss: 0.061019
====> Epoch: 260 Average loss: 0.000371 
Epoch: 261 [0/5999 (0%)]	Loss: 0.046429
Epoch: 261 [2560/5999 (43%)]	Loss: 0.025466
Epoch: 261 [5120/5999 (85%)]	Loss: 0.097778
====> Epoch: 261 Average loss: 0.000387 
Epoch: 262 [0/5999 (0%)]	Loss: 0.024906
Epoch: 262 [2560/5999 (43%)]	Loss: 0.025328
Epoch: 262 [5120/5999 (85%)]	Loss: 0.103204
====> Epoch: 262 Average loss: 0.000332 
Epoch: 263 [0/5999 (0%)]	Loss: 0.037879
Epoch: 263 [2560/5999 (43%)]	Loss: 0.056166
Epoch: 263 [5120/5999 (85%)]	Loss: 0.056570
====> Epoch: 263 Average loss: 0.000376 
Epoch: 264 [0/5999 (0%)]	Loss: 0.037576
Epoch: 264 [2560/5999 (43%)]	Loss: 0.034161
Epoch: 264 [5120/5999 (85%)]	Loss: 0.031528
====> Epoch: 264 Average loss: 0.000322 
Epoch: 265 [0/5999 (0%)]	Loss: 0.039590
Epoch: 265 [2560/5999 (43%)]	Loss: 0.031408
Epoch: 265 [5120/5999 (85%)]	Loss: 0.022895
====> Epoch: 265 Average loss: 0.000278 
Epoch: 266 [0/5999 (0%)]	Loss: 0.032776
Epoch: 266 [2560/5999 (43%)]	Loss: 0.115615
Epoch: 266 [5120/5999 (85%)]	Loss: 0.034365
====> Epoch: 266 Average loss: 0.000359 
Epoch: 267 [0/5999 (0%)]	Loss: 0.045944
Epoch: 267 [2560/5999 (43%)]	Loss: 0.016429
Epoch: 267 [5120/5999 (85%)]	Loss: 0.027234
====> Epoch: 267 Average loss: 0.000505 
Epoch: 268 [0/5999 (0%)]	Loss: 0.052957
Epoch: 268 [2560/5999 (43%)]	Loss: 0.031761
Epoch: 268 [5120/5999 (85%)]	Loss: 0.030303
====> Epoch: 268 Average loss: 0.000406 
Epoch: 269 [0/5999 (0%)]	Loss: 0.046787
Epoch: 269 [2560/5999 (43%)]	Loss: 0.021920
Epoch: 269 [5120/5999 (85%)]	Loss: 0.027111
====> Epoch: 269 Average loss: 0.000282 
Epoch: 270 [0/5999 (0%)]	Loss: 0.017398
Epoch: 270 [2560/5999 (43%)]	Loss: 0.048679
Epoch: 270 [5120/5999 (85%)]	Loss: 0.041865
====> Epoch: 270 Average loss: 0.000262 
Epoch: 271 [0/5999 (0%)]	Loss: 0.024884
Epoch: 271 [2560/5999 (43%)]	Loss: 0.076404
Epoch: 271 [5120/5999 (85%)]	Loss: 0.028436
====> Epoch: 271 Average loss: 0.000330 
Epoch: 272 [0/5999 (0%)]	Loss: 0.048945
Epoch: 272 [2560/5999 (43%)]	Loss: 0.019091
Epoch: 272 [5120/5999 (85%)]	Loss: 0.028093
====> Epoch: 272 Average loss: 0.000306 
Epoch: 273 [0/5999 (0%)]	Loss: 0.032854
Epoch: 273 [2560/5999 (43%)]	Loss: 0.033011
Epoch: 273 [5120/5999 (85%)]	Loss: 0.056582
====> Epoch: 273 Average loss: 0.000425 
Epoch: 274 [0/5999 (0%)]	Loss: 0.026975
Epoch: 274 [2560/5999 (43%)]	Loss: 0.065281
Epoch: 274 [5120/5999 (85%)]	Loss: 0.033497
====> Epoch: 274 Average loss: 0.000538 
Epoch: 275 [0/5999 (0%)]	Loss: 0.115913
Epoch: 275 [2560/5999 (43%)]	Loss: 0.038569
Epoch: 275 [5120/5999 (85%)]	Loss: 0.032229
====> Epoch: 275 Average loss: 0.000364 
Epoch: 276 [0/5999 (0%)]	Loss: 0.024689
Epoch: 276 [2560/5999 (43%)]	Loss: 0.030108
Epoch: 276 [5120/5999 (85%)]	Loss: 0.032286
====> Epoch: 276 Average loss: 0.000412 
Epoch: 277 [0/5999 (0%)]	Loss: 0.035313
Epoch: 277 [2560/5999 (43%)]	Loss: 0.025745
Epoch: 277 [5120/5999 (85%)]	Loss: 0.072915
====> Epoch: 277 Average loss: 0.000351 
Epoch: 278 [0/5999 (0%)]	Loss: 0.123819
Epoch: 278 [2560/5999 (43%)]	Loss: 0.203052
Epoch: 278 [5120/5999 (85%)]	Loss: 0.043866
====> Epoch: 278 Average loss: 0.000419 
Epoch: 279 [0/5999 (0%)]	Loss: 0.035492
Epoch: 279 [2560/5999 (43%)]	Loss: 0.018914
Epoch: 279 [5120/5999 (85%)]	Loss: 0.012371
====> Epoch: 279 Average loss: 0.000276 
Epoch: 280 [0/5999 (0%)]	Loss: 0.051736
Epoch: 280 [2560/5999 (43%)]	Loss: 0.017969
saving model at:280,0.00010822821111651138
Epoch: 280 [5120/5999 (85%)]	Loss: 0.022885
====> Epoch: 280 Average loss: 0.000451 
Epoch: 281 [0/5999 (0%)]	Loss: 0.035691
Epoch: 281 [2560/5999 (43%)]	Loss: 0.017692
Epoch: 281 [5120/5999 (85%)]	Loss: 0.026694
====> Epoch: 281 Average loss: 0.000335 
Epoch: 282 [0/5999 (0%)]	Loss: 0.085580
Epoch: 282 [2560/5999 (43%)]	Loss: 0.040085
Epoch: 282 [5120/5999 (85%)]	Loss: 0.023360
====> Epoch: 282 Average loss: 0.000457 
Epoch: 283 [0/5999 (0%)]	Loss: 0.036336
Epoch: 283 [2560/5999 (43%)]	Loss: 0.037204
Epoch: 283 [5120/5999 (85%)]	Loss: 0.048256
====> Epoch: 283 Average loss: 0.000347 
Epoch: 284 [0/5999 (0%)]	Loss: 0.027598
Epoch: 284 [2560/5999 (43%)]	Loss: 0.038251
Epoch: 284 [5120/5999 (85%)]	Loss: 0.032398
====> Epoch: 284 Average loss: 0.000264 
Epoch: 285 [0/5999 (0%)]	Loss: 0.025577
Epoch: 285 [2560/5999 (43%)]	Loss: 0.036034
Epoch: 285 [5120/5999 (85%)]	Loss: 0.036154
====> Epoch: 285 Average loss: 0.000343 
Epoch: 286 [0/5999 (0%)]	Loss: 0.025807
Epoch: 286 [2560/5999 (43%)]	Loss: 0.098442
Epoch: 286 [5120/5999 (85%)]	Loss: 0.066451
====> Epoch: 286 Average loss: 0.000353 
Epoch: 287 [0/5999 (0%)]	Loss: 0.088462
Epoch: 287 [2560/5999 (43%)]	Loss: 0.019981
Epoch: 287 [5120/5999 (85%)]	Loss: 0.140322
====> Epoch: 287 Average loss: 0.000373 
Epoch: 288 [0/5999 (0%)]	Loss: 0.073297
Epoch: 288 [2560/5999 (43%)]	Loss: 0.051236
Epoch: 288 [5120/5999 (85%)]	Loss: 0.031407
====> Epoch: 288 Average loss: 0.000372 
Epoch: 289 [0/5999 (0%)]	Loss: 0.034856
Epoch: 289 [2560/5999 (43%)]	Loss: 0.034500
Epoch: 289 [5120/5999 (85%)]	Loss: 0.062036
====> Epoch: 289 Average loss: 0.000284 
Epoch: 290 [0/5999 (0%)]	Loss: 0.019414
Epoch: 290 [2560/5999 (43%)]	Loss: 0.045172
Epoch: 290 [5120/5999 (85%)]	Loss: 0.033744
====> Epoch: 290 Average loss: 0.000416 
Epoch: 291 [0/5999 (0%)]	Loss: 0.033065
Epoch: 291 [2560/5999 (43%)]	Loss: 0.076461
Epoch: 291 [5120/5999 (85%)]	Loss: 0.054311
====> Epoch: 291 Average loss: 0.000417 
Epoch: 292 [0/5999 (0%)]	Loss: 0.025618
Epoch: 292 [2560/5999 (43%)]	Loss: 0.054715
Epoch: 292 [5120/5999 (85%)]	Loss: 0.031601
====> Epoch: 292 Average loss: 0.000403 
Epoch: 293 [0/5999 (0%)]	Loss: 0.019914
Epoch: 293 [2560/5999 (43%)]	Loss: 0.047594
Epoch: 293 [5120/5999 (85%)]	Loss: 0.019142
====> Epoch: 293 Average loss: 0.000438 
Epoch: 294 [0/5999 (0%)]	Loss: 0.090360
Epoch: 294 [2560/5999 (43%)]	Loss: 0.032401
Epoch: 294 [5120/5999 (85%)]	Loss: 0.078913
====> Epoch: 294 Average loss: 0.000345 
Epoch: 295 [0/5999 (0%)]	Loss: 0.021649
Epoch: 295 [2560/5999 (43%)]	Loss: 0.044080
Epoch: 295 [5120/5999 (85%)]	Loss: 0.044927
====> Epoch: 295 Average loss: 0.000575 
Epoch: 296 [0/5999 (0%)]	Loss: 0.197603
Epoch: 296 [2560/5999 (43%)]	Loss: 0.026183
Epoch: 296 [5120/5999 (85%)]	Loss: 0.036616
====> Epoch: 296 Average loss: 0.000421 
Epoch: 297 [0/5999 (0%)]	Loss: 0.021844
Epoch: 297 [2560/5999 (43%)]	Loss: 0.026574
Epoch: 297 [5120/5999 (85%)]	Loss: 0.114137
====> Epoch: 297 Average loss: 0.000266 
Epoch: 298 [0/5999 (0%)]	Loss: 0.028691
Epoch: 298 [2560/5999 (43%)]	Loss: 0.025235
Epoch: 298 [5120/5999 (85%)]	Loss: 0.020993
====> Epoch: 298 Average loss: 0.000317 
Epoch: 299 [0/5999 (0%)]	Loss: 0.019666
Epoch: 299 [2560/5999 (43%)]	Loss: 0.027747
Epoch: 299 [5120/5999 (85%)]	Loss: 0.067068
====> Epoch: 299 Average loss: 0.000272 
Epoch: 300 [0/5999 (0%)]	Loss: 0.022906
Epoch: 300 [2560/5999 (43%)]	Loss: 0.025178
Epoch: 300 [5120/5999 (85%)]	Loss: 0.080393
====> Epoch: 300 Average loss: 0.000393 
Epoch: 301 [0/5999 (0%)]	Loss: 0.025989
Epoch: 301 [2560/5999 (43%)]	Loss: 0.034074
Epoch: 301 [5120/5999 (85%)]	Loss: 0.048770
====> Epoch: 301 Average loss: 0.000388 
Epoch: 302 [0/5999 (0%)]	Loss: 0.093901
Epoch: 302 [2560/5999 (43%)]	Loss: 0.041054
Epoch: 302 [5120/5999 (85%)]	Loss: 0.030853
====> Epoch: 302 Average loss: 0.000432 
Epoch: 303 [0/5999 (0%)]	Loss: 0.027941
Epoch: 303 [2560/5999 (43%)]	Loss: 0.024336
Epoch: 303 [5120/5999 (85%)]	Loss: 0.027650
====> Epoch: 303 Average loss: 0.000444 
Epoch: 304 [0/5999 (0%)]	Loss: 0.024188
Epoch: 304 [2560/5999 (43%)]	Loss: 0.015938
saving model at:304,0.0001026349674211815
Epoch: 304 [5120/5999 (85%)]	Loss: 0.067659
====> Epoch: 304 Average loss: 0.000363 
Epoch: 305 [0/5999 (0%)]	Loss: 0.055248
Epoch: 305 [2560/5999 (43%)]	Loss: 0.026898
Epoch: 305 [5120/5999 (85%)]	Loss: 0.017267
====> Epoch: 305 Average loss: 0.000301 
Epoch: 306 [0/5999 (0%)]	Loss: 0.049198
Epoch: 306 [2560/5999 (43%)]	Loss: 0.019751
Epoch: 306 [5120/5999 (85%)]	Loss: 0.023657
====> Epoch: 306 Average loss: 0.000258 
Epoch: 307 [0/5999 (0%)]	Loss: 0.023812
Epoch: 307 [2560/5999 (43%)]	Loss: 0.141010
Epoch: 307 [5120/5999 (85%)]	Loss: 0.057899
====> Epoch: 307 Average loss: 0.000420 
Epoch: 308 [0/5999 (0%)]	Loss: 0.058601
Epoch: 308 [2560/5999 (43%)]	Loss: 0.025555
Epoch: 308 [5120/5999 (85%)]	Loss: 0.024792
====> Epoch: 308 Average loss: 0.000344 
Epoch: 309 [0/5999 (0%)]	Loss: 0.031401
Epoch: 309 [2560/5999 (43%)]	Loss: 0.030695
Epoch: 309 [5120/5999 (85%)]	Loss: 0.090817
====> Epoch: 309 Average loss: 0.000268 
Epoch: 310 [0/5999 (0%)]	Loss: 0.024880
Epoch: 310 [2560/5999 (43%)]	Loss: 0.051622
Epoch: 310 [5120/5999 (85%)]	Loss: 0.027537
====> Epoch: 310 Average loss: 0.000326 
Epoch: 311 [0/5999 (0%)]	Loss: 0.101624
Epoch: 311 [2560/5999 (43%)]	Loss: 0.083336
Epoch: 311 [5120/5999 (85%)]	Loss: 0.059819
====> Epoch: 311 Average loss: 0.000388 
Epoch: 312 [0/5999 (0%)]	Loss: 0.025976
Epoch: 312 [2560/5999 (43%)]	Loss: 0.029697
Epoch: 312 [5120/5999 (85%)]	Loss: 0.031855
====> Epoch: 312 Average loss: 0.000296 
Epoch: 313 [0/5999 (0%)]	Loss: 0.016110
Epoch: 313 [2560/5999 (43%)]	Loss: 0.019426
Epoch: 313 [5120/5999 (85%)]	Loss: 0.048524
====> Epoch: 313 Average loss: 0.000320 
Epoch: 314 [0/5999 (0%)]	Loss: 0.024011
Epoch: 314 [2560/5999 (43%)]	Loss: 0.046192
Epoch: 314 [5120/5999 (85%)]	Loss: 0.044178
====> Epoch: 314 Average loss: 0.000296 
Epoch: 315 [0/5999 (0%)]	Loss: 0.031731
Epoch: 315 [2560/5999 (43%)]	Loss: 0.016902
Epoch: 315 [5120/5999 (85%)]	Loss: 0.075160
====> Epoch: 315 Average loss: 0.000291 
Epoch: 316 [0/5999 (0%)]	Loss: 0.035640
Epoch: 316 [2560/5999 (43%)]	Loss: 0.016348
saving model at:316,9.734883590135724e-05
Epoch: 316 [5120/5999 (85%)]	Loss: 0.021860
====> Epoch: 316 Average loss: 0.000263 
Epoch: 317 [0/5999 (0%)]	Loss: 0.036785
Epoch: 317 [2560/5999 (43%)]	Loss: 0.042647
Epoch: 317 [5120/5999 (85%)]	Loss: 0.029451
====> Epoch: 317 Average loss: 0.000539 
Epoch: 318 [0/5999 (0%)]	Loss: 0.040011
Epoch: 318 [2560/5999 (43%)]	Loss: 0.015867
Epoch: 318 [5120/5999 (85%)]	Loss: 0.020262
====> Epoch: 318 Average loss: 0.000306 
Epoch: 319 [0/5999 (0%)]	Loss: 0.102369
Epoch: 319 [2560/5999 (43%)]	Loss: 0.023510
Epoch: 319 [5120/5999 (85%)]	Loss: 0.068991
====> Epoch: 319 Average loss: 0.000328 
Epoch: 320 [0/5999 (0%)]	Loss: 0.036162
Epoch: 320 [2560/5999 (43%)]	Loss: 0.028957
Epoch: 320 [5120/5999 (85%)]	Loss: 0.034488
====> Epoch: 320 Average loss: 0.000259 
Epoch: 321 [0/5999 (0%)]	Loss: 0.032933
Epoch: 321 [2560/5999 (43%)]	Loss: 0.026016
Epoch: 321 [5120/5999 (85%)]	Loss: 0.017607
====> Epoch: 321 Average loss: 0.000254 
Epoch: 322 [0/5999 (0%)]	Loss: 0.017895
Epoch: 322 [2560/5999 (43%)]	Loss: 0.024499
Epoch: 322 [5120/5999 (85%)]	Loss: 0.037698
====> Epoch: 322 Average loss: 0.000372 
Epoch: 323 [0/5999 (0%)]	Loss: 0.046660
Epoch: 323 [2560/5999 (43%)]	Loss: 0.039736
Epoch: 323 [5120/5999 (85%)]	Loss: 0.036175
====> Epoch: 323 Average loss: 0.000320 
Epoch: 324 [0/5999 (0%)]	Loss: 0.017473
Epoch: 324 [2560/5999 (43%)]	Loss: 0.036708
Epoch: 324 [5120/5999 (85%)]	Loss: 0.033577
====> Epoch: 324 Average loss: 0.000313 
Epoch: 325 [0/5999 (0%)]	Loss: 0.024357
Epoch: 325 [2560/5999 (43%)]	Loss: 0.020299
Epoch: 325 [5120/5999 (85%)]	Loss: 0.026408
====> Epoch: 325 Average loss: 0.000355 
Epoch: 326 [0/5999 (0%)]	Loss: 0.034259
Epoch: 326 [2560/5999 (43%)]	Loss: 0.049914
Epoch: 326 [5120/5999 (85%)]	Loss: 0.073402
====> Epoch: 326 Average loss: 0.000547 
Epoch: 327 [0/5999 (0%)]	Loss: 0.088918
Epoch: 327 [2560/5999 (43%)]	Loss: 0.030258
Epoch: 327 [5120/5999 (85%)]	Loss: 0.031185
====> Epoch: 327 Average loss: 0.000312 
Epoch: 328 [0/5999 (0%)]	Loss: 0.030887
Epoch: 328 [2560/5999 (43%)]	Loss: 0.018988
Epoch: 328 [5120/5999 (85%)]	Loss: 0.029298
====> Epoch: 328 Average loss: 0.000317 
Epoch: 329 [0/5999 (0%)]	Loss: 0.131372
Epoch: 329 [2560/5999 (43%)]	Loss: 0.104630
Epoch: 329 [5120/5999 (85%)]	Loss: 0.034843
====> Epoch: 329 Average loss: 0.000488 
Epoch: 330 [0/5999 (0%)]	Loss: 0.047615
Epoch: 330 [2560/5999 (43%)]	Loss: 0.017973
Epoch: 330 [5120/5999 (85%)]	Loss: 0.078925
====> Epoch: 330 Average loss: 0.000370 
Epoch: 331 [0/5999 (0%)]	Loss: 0.034616
Epoch: 331 [2560/5999 (43%)]	Loss: 0.021732
Epoch: 331 [5120/5999 (85%)]	Loss: 0.080248
====> Epoch: 331 Average loss: 0.000255 
Epoch: 332 [0/5999 (0%)]	Loss: 0.019401
Epoch: 332 [2560/5999 (43%)]	Loss: 0.020464
Epoch: 332 [5120/5999 (85%)]	Loss: 0.108336
====> Epoch: 332 Average loss: 0.000414 
Epoch: 333 [0/5999 (0%)]	Loss: 0.079766
Epoch: 333 [2560/5999 (43%)]	Loss: 0.031149
Epoch: 333 [5120/5999 (85%)]	Loss: 0.043888
====> Epoch: 333 Average loss: 0.000280 
Epoch: 334 [0/5999 (0%)]	Loss: 0.096145
Epoch: 334 [2560/5999 (43%)]	Loss: 0.022844
Epoch: 334 [5120/5999 (85%)]	Loss: 0.082952
====> Epoch: 334 Average loss: 0.000293 
Epoch: 335 [0/5999 (0%)]	Loss: 0.121608
Epoch: 335 [2560/5999 (43%)]	Loss: 0.074527
Epoch: 335 [5120/5999 (85%)]	Loss: 0.044849
====> Epoch: 335 Average loss: 0.000581 
Epoch: 336 [0/5999 (0%)]	Loss: 0.023300
Epoch: 336 [2560/5999 (43%)]	Loss: 0.016114
Epoch: 336 [5120/5999 (85%)]	Loss: 0.017471
====> Epoch: 336 Average loss: 0.000281 
Epoch: 337 [0/5999 (0%)]	Loss: 0.018875
Epoch: 337 [2560/5999 (43%)]	Loss: 0.032660
Epoch: 337 [5120/5999 (85%)]	Loss: 0.036849
====> Epoch: 337 Average loss: 0.000283 
Epoch: 338 [0/5999 (0%)]	Loss: 0.020355
Epoch: 338 [2560/5999 (43%)]	Loss: 0.024900
Epoch: 338 [5120/5999 (85%)]	Loss: 0.040630
====> Epoch: 338 Average loss: 0.000307 
Epoch: 339 [0/5999 (0%)]	Loss: 0.068797
Epoch: 339 [2560/5999 (43%)]	Loss: 0.038473
Epoch: 339 [5120/5999 (85%)]	Loss: 0.085178
====> Epoch: 339 Average loss: 0.000373 
Epoch: 340 [0/5999 (0%)]	Loss: 0.036095
Epoch: 340 [2560/5999 (43%)]	Loss: 0.043746
Epoch: 340 [5120/5999 (85%)]	Loss: 0.044932
====> Epoch: 340 Average loss: 0.000295 
Epoch: 341 [0/5999 (0%)]	Loss: 0.063348
Epoch: 341 [2560/5999 (43%)]	Loss: 0.050411
Epoch: 341 [5120/5999 (85%)]	Loss: 0.026952
====> Epoch: 341 Average loss: 0.000282 
Epoch: 342 [0/5999 (0%)]	Loss: 0.111034
Epoch: 342 [2560/5999 (43%)]	Loss: 0.016880
saving model at:342,9.633518994087353e-05
Epoch: 342 [5120/5999 (85%)]	Loss: 0.014889
====> Epoch: 342 Average loss: 0.000294 
Epoch: 343 [0/5999 (0%)]	Loss: 0.071898
Epoch: 343 [2560/5999 (43%)]	Loss: 0.022567
Epoch: 343 [5120/5999 (85%)]	Loss: 0.025562
====> Epoch: 343 Average loss: 0.000442 
Epoch: 344 [0/5999 (0%)]	Loss: 0.050178
Epoch: 344 [2560/5999 (43%)]	Loss: 0.029895
Epoch: 344 [5120/5999 (85%)]	Loss: 0.020988
====> Epoch: 344 Average loss: 0.000272 
Epoch: 345 [0/5999 (0%)]	Loss: 0.018364
Epoch: 345 [2560/5999 (43%)]	Loss: 0.021256
Epoch: 345 [5120/5999 (85%)]	Loss: 0.069170
====> Epoch: 345 Average loss: 0.000274 
Epoch: 346 [0/5999 (0%)]	Loss: 0.072001
Epoch: 346 [2560/5999 (43%)]	Loss: 0.078012
Epoch: 346 [5120/5999 (85%)]	Loss: 0.035059
====> Epoch: 346 Average loss: 0.000292 
Epoch: 347 [0/5999 (0%)]	Loss: 0.037219
Epoch: 347 [2560/5999 (43%)]	Loss: 0.222588
Epoch: 347 [5120/5999 (85%)]	Loss: 0.027427
====> Epoch: 347 Average loss: 0.000401 
Epoch: 348 [0/5999 (0%)]	Loss: 0.046428
Epoch: 348 [2560/5999 (43%)]	Loss: 0.036745
Epoch: 348 [5120/5999 (85%)]	Loss: 0.030385
====> Epoch: 348 Average loss: 0.000368 
Epoch: 349 [0/5999 (0%)]	Loss: 0.026759
Epoch: 349 [2560/5999 (43%)]	Loss: 0.088702
Epoch: 349 [5120/5999 (85%)]	Loss: 0.051812
====> Epoch: 349 Average loss: 0.000301 
Epoch: 350 [0/5999 (0%)]	Loss: 0.017470
Epoch: 350 [2560/5999 (43%)]	Loss: 0.010147
Epoch: 350 [5120/5999 (85%)]	Loss: 0.024664
====> Epoch: 350 Average loss: 0.000331 
Epoch: 351 [0/5999 (0%)]	Loss: 0.103177
Epoch: 351 [2560/5999 (43%)]	Loss: 0.013829
saving model at:351,8.150556049076841e-05
Epoch: 351 [5120/5999 (85%)]	Loss: 0.041132
====> Epoch: 351 Average loss: 0.000251 
Epoch: 352 [0/5999 (0%)]	Loss: 0.023596
Epoch: 352 [2560/5999 (43%)]	Loss: 0.031196
Epoch: 352 [5120/5999 (85%)]	Loss: 0.078323
====> Epoch: 352 Average loss: 0.000304 
Epoch: 353 [0/5999 (0%)]	Loss: 0.038548
Epoch: 353 [2560/5999 (43%)]	Loss: 0.038222
Epoch: 353 [5120/5999 (85%)]	Loss: 0.040711
====> Epoch: 353 Average loss: 0.000275 
Epoch: 354 [0/5999 (0%)]	Loss: 0.033883
Epoch: 354 [2560/5999 (43%)]	Loss: 0.021286
Epoch: 354 [5120/5999 (85%)]	Loss: 0.021844
====> Epoch: 354 Average loss: 0.000258 
Epoch: 355 [0/5999 (0%)]	Loss: 0.077394
Epoch: 355 [2560/5999 (43%)]	Loss: 0.053555
Epoch: 355 [5120/5999 (85%)]	Loss: 0.106579
====> Epoch: 355 Average loss: 0.000322 
Epoch: 356 [0/5999 (0%)]	Loss: 0.020223
Epoch: 356 [2560/5999 (43%)]	Loss: 0.064100
Epoch: 356 [5120/5999 (85%)]	Loss: 0.030588
====> Epoch: 356 Average loss: 0.000319 
Epoch: 357 [0/5999 (0%)]	Loss: 0.016734
Epoch: 357 [2560/5999 (43%)]	Loss: 0.028977
Epoch: 357 [5120/5999 (85%)]	Loss: 0.093487
====> Epoch: 357 Average loss: 0.000384 
Epoch: 358 [0/5999 (0%)]	Loss: 0.032868
Epoch: 358 [2560/5999 (43%)]	Loss: 0.037650
Epoch: 358 [5120/5999 (85%)]	Loss: 0.015735
====> Epoch: 358 Average loss: 0.000243 
Epoch: 359 [0/5999 (0%)]	Loss: 0.031109
Epoch: 359 [2560/5999 (43%)]	Loss: 0.016711
Epoch: 359 [5120/5999 (85%)]	Loss: 0.025990
====> Epoch: 359 Average loss: 0.000288 
Epoch: 360 [0/5999 (0%)]	Loss: 0.073944
Epoch: 360 [2560/5999 (43%)]	Loss: 0.125016
Epoch: 360 [5120/5999 (85%)]	Loss: 0.039043
====> Epoch: 360 Average loss: 0.000376 
Epoch: 361 [0/5999 (0%)]	Loss: 0.030420
Epoch: 361 [2560/5999 (43%)]	Loss: 0.043524
Epoch: 361 [5120/5999 (85%)]	Loss: 0.086190
====> Epoch: 361 Average loss: 0.000283 
Epoch: 362 [0/5999 (0%)]	Loss: 0.126087
Epoch: 362 [2560/5999 (43%)]	Loss: 0.019058
Epoch: 362 [5120/5999 (85%)]	Loss: 0.111510
====> Epoch: 362 Average loss: 0.000339 
Epoch: 363 [0/5999 (0%)]	Loss: 0.017672
Epoch: 363 [2560/5999 (43%)]	Loss: 0.024368
Epoch: 363 [5120/5999 (85%)]	Loss: 0.013792
====> Epoch: 363 Average loss: 0.000311 
Epoch: 364 [0/5999 (0%)]	Loss: 0.022588
Epoch: 364 [2560/5999 (43%)]	Loss: 0.017734
Epoch: 364 [5120/5999 (85%)]	Loss: 0.033159
====> Epoch: 364 Average loss: 0.000289 
Epoch: 365 [0/5999 (0%)]	Loss: 0.053943
Epoch: 365 [2560/5999 (43%)]	Loss: 0.049766
Epoch: 365 [5120/5999 (85%)]	Loss: 0.022216
====> Epoch: 365 Average loss: 0.000349 
Epoch: 366 [0/5999 (0%)]	Loss: 0.030091
Epoch: 366 [2560/5999 (43%)]	Loss: 0.021370
Epoch: 366 [5120/5999 (85%)]	Loss: 0.022554
====> Epoch: 366 Average loss: 0.000280 
Epoch: 367 [0/5999 (0%)]	Loss: 0.045244
Epoch: 367 [2560/5999 (43%)]	Loss: 0.031972
Epoch: 367 [5120/5999 (85%)]	Loss: 0.020955
====> Epoch: 367 Average loss: 0.000356 
Epoch: 368 [0/5999 (0%)]	Loss: 0.160439
Epoch: 368 [2560/5999 (43%)]	Loss: 0.081240
Epoch: 368 [5120/5999 (85%)]	Loss: 0.077813
====> Epoch: 368 Average loss: 0.000477 
Epoch: 369 [0/5999 (0%)]	Loss: 0.030375
Epoch: 369 [2560/5999 (43%)]	Loss: 0.031731
Epoch: 369 [5120/5999 (85%)]	Loss: 0.020576
====> Epoch: 369 Average loss: 0.000290 
Epoch: 370 [0/5999 (0%)]	Loss: 0.018249
Epoch: 370 [2560/5999 (43%)]	Loss: 0.068372
Epoch: 370 [5120/5999 (85%)]	Loss: 0.059647
====> Epoch: 370 Average loss: 0.000345 
Epoch: 371 [0/5999 (0%)]	Loss: 0.033420
Epoch: 371 [2560/5999 (43%)]	Loss: 0.108125
Epoch: 371 [5120/5999 (85%)]	Loss: 0.067201
====> Epoch: 371 Average loss: 0.000440 
Epoch: 372 [0/5999 (0%)]	Loss: 0.037665
Epoch: 372 [2560/5999 (43%)]	Loss: 0.033301
Epoch: 372 [5120/5999 (85%)]	Loss: 0.017031
saving model at:372,8.031331165693701e-05
====> Epoch: 372 Average loss: 0.000278 
Epoch: 373 [0/5999 (0%)]	Loss: 0.030263
Epoch: 373 [2560/5999 (43%)]	Loss: 0.023834
Epoch: 373 [5120/5999 (85%)]	Loss: 0.029806
====> Epoch: 373 Average loss: 0.000312 
Epoch: 374 [0/5999 (0%)]	Loss: 0.016805
Epoch: 374 [2560/5999 (43%)]	Loss: 0.048197
Epoch: 374 [5120/5999 (85%)]	Loss: 0.041250
====> Epoch: 374 Average loss: 0.000386 
Epoch: 375 [0/5999 (0%)]	Loss: 0.049030
Epoch: 375 [2560/5999 (43%)]	Loss: 0.040430
Epoch: 375 [5120/5999 (85%)]	Loss: 0.015131
====> Epoch: 375 Average loss: 0.000283 
Epoch: 376 [0/5999 (0%)]	Loss: 0.031501
Epoch: 376 [2560/5999 (43%)]	Loss: 0.034092
Epoch: 376 [5120/5999 (85%)]	Loss: 0.151476
====> Epoch: 376 Average loss: 0.000277 
Epoch: 377 [0/5999 (0%)]	Loss: 0.023742
Epoch: 377 [2560/5999 (43%)]	Loss: 0.014475
Epoch: 377 [5120/5999 (85%)]	Loss: 0.021853
====> Epoch: 377 Average loss: 0.000343 
Epoch: 378 [0/5999 (0%)]	Loss: 0.071958
Epoch: 378 [2560/5999 (43%)]	Loss: 0.022066
Epoch: 378 [5120/5999 (85%)]	Loss: 0.037042
====> Epoch: 378 Average loss: 0.000265 
Epoch: 379 [0/5999 (0%)]	Loss: 0.015745
Epoch: 379 [2560/5999 (43%)]	Loss: 0.015617
Epoch: 379 [5120/5999 (85%)]	Loss: 0.032632
====> Epoch: 379 Average loss: 0.000233 
Epoch: 380 [0/5999 (0%)]	Loss: 0.016368
Epoch: 380 [2560/5999 (43%)]	Loss: 0.011996
saving model at:380,7.39667501184158e-05
Epoch: 380 [5120/5999 (85%)]	Loss: 0.030364
====> Epoch: 380 Average loss: 0.000273 
Epoch: 381 [0/5999 (0%)]	Loss: 0.060462
Epoch: 381 [2560/5999 (43%)]	Loss: 0.052236
Epoch: 381 [5120/5999 (85%)]	Loss: 0.019235
====> Epoch: 381 Average loss: 0.000302 
Epoch: 382 [0/5999 (0%)]	Loss: 0.043946
Epoch: 382 [2560/5999 (43%)]	Loss: 0.033593
Epoch: 382 [5120/5999 (85%)]	Loss: 0.037548
====> Epoch: 382 Average loss: 0.000301 
Epoch: 383 [0/5999 (0%)]	Loss: 0.038524
Epoch: 383 [2560/5999 (43%)]	Loss: 0.155544
Epoch: 383 [5120/5999 (85%)]	Loss: 0.030660
====> Epoch: 383 Average loss: 0.000294 
Epoch: 384 [0/5999 (0%)]	Loss: 0.064282
Epoch: 384 [2560/5999 (43%)]	Loss: 0.032436
Epoch: 384 [5120/5999 (85%)]	Loss: 0.026082
====> Epoch: 384 Average loss: 0.000371 
Epoch: 385 [0/5999 (0%)]	Loss: 0.018180
Epoch: 385 [2560/5999 (43%)]	Loss: 0.023196
Epoch: 385 [5120/5999 (85%)]	Loss: 0.059267
====> Epoch: 385 Average loss: 0.000259 
Epoch: 386 [0/5999 (0%)]	Loss: 0.025920
Epoch: 386 [2560/5999 (43%)]	Loss: 0.035126
Epoch: 386 [5120/5999 (85%)]	Loss: 0.021942
====> Epoch: 386 Average loss: 0.000284 
Epoch: 387 [0/5999 (0%)]	Loss: 0.018073
Epoch: 387 [2560/5999 (43%)]	Loss: 0.035416
Epoch: 387 [5120/5999 (85%)]	Loss: 0.029635
====> Epoch: 387 Average loss: 0.000439 
Epoch: 388 [0/5999 (0%)]	Loss: 0.069142
Epoch: 388 [2560/5999 (43%)]	Loss: 0.022978
Epoch: 388 [5120/5999 (85%)]	Loss: 0.052360
====> Epoch: 388 Average loss: 0.000321 
Epoch: 389 [0/5999 (0%)]	Loss: 0.042852
Epoch: 389 [2560/5999 (43%)]	Loss: 0.024203
Epoch: 389 [5120/5999 (85%)]	Loss: 0.023580
====> Epoch: 389 Average loss: 0.000289 
Epoch: 390 [0/5999 (0%)]	Loss: 0.016019
Epoch: 390 [2560/5999 (43%)]	Loss: 0.111981
Epoch: 390 [5120/5999 (85%)]	Loss: 0.026682
====> Epoch: 390 Average loss: 0.000338 
Epoch: 391 [0/5999 (0%)]	Loss: 0.023890
Epoch: 391 [2560/5999 (43%)]	Loss: 0.063585
Epoch: 391 [5120/5999 (85%)]	Loss: 0.042870
====> Epoch: 391 Average loss: 0.000347 
Epoch: 392 [0/5999 (0%)]	Loss: 0.042334
Epoch: 392 [2560/5999 (43%)]	Loss: 0.029429
Epoch: 392 [5120/5999 (85%)]	Loss: 0.026433
====> Epoch: 392 Average loss: 0.000263 
Epoch: 393 [0/5999 (0%)]	Loss: 0.023979
Epoch: 393 [2560/5999 (43%)]	Loss: 0.016051
Epoch: 393 [5120/5999 (85%)]	Loss: 0.056817
====> Epoch: 393 Average loss: 0.000255 
Epoch: 394 [0/5999 (0%)]	Loss: 0.031827
Epoch: 394 [2560/5999 (43%)]	Loss: 0.022166
Epoch: 394 [5120/5999 (85%)]	Loss: 0.035120
====> Epoch: 394 Average loss: 0.000338 
Epoch: 395 [0/5999 (0%)]	Loss: 0.015061
Epoch: 395 [2560/5999 (43%)]	Loss: 0.034959
Epoch: 395 [5120/5999 (85%)]	Loss: 0.056221
====> Epoch: 395 Average loss: 0.000265 
Epoch: 396 [0/5999 (0%)]	Loss: 0.029724
Epoch: 396 [2560/5999 (43%)]	Loss: 0.023539
Epoch: 396 [5120/5999 (85%)]	Loss: 0.018613
====> Epoch: 396 Average loss: 0.000297 
Epoch: 397 [0/5999 (0%)]	Loss: 0.016019
Epoch: 397 [2560/5999 (43%)]	Loss: 0.050881
Epoch: 397 [5120/5999 (85%)]	Loss: 0.035420
====> Epoch: 397 Average loss: 0.000318 
Epoch: 398 [0/5999 (0%)]	Loss: 0.083879
Epoch: 398 [2560/5999 (43%)]	Loss: 0.033011
Epoch: 398 [5120/5999 (85%)]	Loss: 0.024321
====> Epoch: 398 Average loss: 0.000314 
Epoch: 399 [0/5999 (0%)]	Loss: 0.020701
Epoch: 399 [2560/5999 (43%)]	Loss: 0.024618
Epoch: 399 [5120/5999 (85%)]	Loss: 0.031447
====> Epoch: 399 Average loss: 0.000298 
Epoch: 400 [0/5999 (0%)]	Loss: 0.052029
Epoch: 400 [2560/5999 (43%)]	Loss: 0.015853
Epoch: 400 [5120/5999 (85%)]	Loss: 0.017490
====> Epoch: 400 Average loss: 0.000232 
Reconstruction Loss 7.396675314521417e-05
per_obj_mse: ['0.00011391112639103085', '3.402238144190051e-05']
Reconstruction Loss 7.428315261770468e-05
per_obj_mse: ['0.00011507963063195348', '3.348668178659864e-05']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_2_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_2_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_2_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_2_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0003657373283058405
per_obj_mse: ['0.0003677054774016142', '0.00036376918433234096']
Reconstruction Loss 0.000366826696573417
per_obj_mse: ['0.000362730905180797', '0.0003709225566126406']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_2_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=5, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_2_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7999, 2, 11)
(1999, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.0003657373283058405
per_obj_mse: ['0.0003677054774016142', '0.00036376918433234096']
Reconstruction Loss 0.000366826696573417
per_obj_mse: ['0.000362730905180797', '0.0003709225566126406']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_2_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=20, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_2_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7997, 2, 11)
(1997, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.001921044277958572
per_obj_mse: ['0.002413479844108224', '0.0014286088990047574']
Reconstruction Loss 0.0019872778568194847
per_obj_mse: ['0.0025079050101339817', '0.0014666509814560413']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_2_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=60, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_2_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7993, 2, 11)
(1993, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.006044861348322581
per_obj_mse: ['0.007823922671377659', '0.00426580011844635']
Reconstruction Loss 0.006252509963544944
per_obj_mse: ['0.00809923280030489', '0.004405786283314228']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_2_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=100, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_2_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7989, 2, 11)
(1989, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.010171219700009556
per_obj_mse: ['0.013244221918284893', '0.007098216097801924']
Reconstruction Loss 0.01051710928004102
per_obj_mse: ['0.013688899576663971', '0.007345319259911776']
cuda
Namespace(action_dim=15, baseline=True, batch_size=128, copy_action=False, cuda=True, dataset='narma_2d_2_n0.csv', dec_epochs=0, decoder=False, decoder_gl=1.0, decoder_l1=1.0, dropout=0.0, embedding_dim=40, epochs=0, forecasting_M5=False, forecasting_cl=False, full=True, futureControlOnly=False, gl=1.0, hard_decoder=False, hidden_dim=512, hierarchical_ls=False, horizon=140, ignore_action=False, isControl=False, l1=1.0, layer_gl=False, layer_l1=False, learning_rate=0.001, length=11, log_interval=20, message_l1=1.0, message_pass=False, name='B', no_cuda=False, nodes=2, noise=False, normalize=False, num_cont=2, num_objects=2, onlyReLU=False, ood=True, pastControlOnly=False, pastStateOnly=False, pastinfo=False, path='/data/pankaj/CLTS//data/narma/combgen/6/test_ood_alpha_0.0_0.1/', per_node_MLP=False, pert=None, recurrent=False, save_embeddings=False, save_folder='/data/pankaj/CLTS/output/CombGen/5Aug2020_NARMA_2D_S1_4_data6_spur_corr_400epochs_train_alpha_0.4_0.7_B_4L/narma_2d_2_n0.csv', save_interval=20, save_predictions=False, seed=42, sepCTRL=False, setup=1, shift=10, shuffle=True, sigma=0.5, soft_decoder_gl=False, soft_decoder_l1=False, split=0.75, steps=1, stride=1, use_condenser=False, window_size=11)
(7985, 2, 11)
(1985, 2, 11)
Data loaded!
MVTS(
  (control_CNN): CNN_Extractor_cont(
    (dropoutlayer): Dropout(p=0.0)
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_CNN): CNN_Extractor(
    (CNN): Sequential(
      (0): Conv1d(2, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (1): Chomp1d()
      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))
      (5): Chomp1d()
      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.01)
      (8): Conv1d(8, 2, kernel_size=(5,), stride=(1,), padding=(4,))
      (9): Chomp1d()
      (10): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): Tanh()
    )
  )
  (state_LSTM): LSTM(11, 40)
  (control_LSTM): LSTM(11, 40)
  (extract_state): Sequential(
    (0): Linear(in_features=40, out_features=11, bias=True)
    (1): Tanh()
  )
  (transition_model): Sequential(
    (0): Linear(in_features=160, out_features=160, bias=True)
    (1): Tanh()
    (2): Linear(in_features=160, out_features=80, bias=True)
    (3): Tanh()
  )
  (decoder): Linear(in_features=80, out_features=2, bias=True)
  (MSE): MSELoss()
)
Starting model training...
Reconstruction Loss 0.014261955711999922
per_obj_mse: ['0.018584562465548515', '0.009939349256455898']
Reconstruction Loss 0.014788542221997037
per_obj_mse: ['0.01928693987429142', '0.010290143080055714']
